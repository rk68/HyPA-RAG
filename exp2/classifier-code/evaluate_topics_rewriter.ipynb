{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "import openai\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env file\n",
    "import pandas as pd\n",
    "from llama_index.core.evaluation import (\n",
    "    RetrieverEvaluator,\n",
    "    get_retrieval_results_df,\n",
    ")\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from datetime import datetime\n",
    "from llama_index.core import (\n",
    "    StorageContext, VectorStoreIndex, SimpleDirectoryReader, \n",
    "    get_response_synthesizer, Settings\n",
    ")\n",
    "import traceback\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "from llama_index.core.evaluation import generate_question_context_pairs, QueryResponseDataset\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from typing import List\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from llama_index.core import PromptTemplate\n",
    "import time\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import json\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "\n",
    "import giskard\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from giskard.llm import set_llm_model, set_llm_api\n",
    "from giskard.llm.client import get_default_client\n",
    "from giskard.llm.client import set_llm_api, set_llm_model\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import os\n",
    "from giskard.rag import KnowledgeBase, generate_testset, QATestset\n",
    "#from giskard.rag.question_generators import complex_questions, complex_situational_questions, compare_questions, simple_questions, rule_conclusion_questions, distracting_questions, na_questions, vague_questions, oos_questions, situational_questions, double_questions, conversational_questions\n",
    "from giskard.rag import AgentAnswer\n",
    "from giskard.rag import evaluate, RAGReport\n",
    "from giskard.rag.metrics.ragas_metrics import ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# Fetch API keys from environment variables\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "AZURE_API_KEY = os.getenv('AZURE_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")\n",
    "\n",
    "Settings.embed_model=embed_model\n",
    "\n",
    "# setup LLM\n",
    "\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt4o\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=\"gpt35\",#AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "set_llm_api(\"azure\")\n",
    "set_llm_model('gpt4o')\n",
    "# You'll need to provide the name of the model that you've deployed\n",
    "# Beware, the model provided must be capable of using function calls\n",
    "client = get_default_client()\n",
    "assert client._client._base_url == f'{os.environ[\"AZURE_OPENAI_ENDPOINT\"]}/openai/'\n",
    "assert client._client.api_key == os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "assert client._client._api_version == os.environ[\"OPENAI_API_VERSION\"]\n",
    "assert client.model == \"gpt4o\"#os.environ[\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Giskard\n",
    "Settings.llm=llm_gpt35\n",
    "loader = PyMuPDFReader()\n",
    "#file_extractor = {\".pdf\": loader}\n",
    "documents1 = loader.load(file_path=\"../legal_data/LL144/LL144.pdf\")\n",
    "documents2 = loader.load(file_path=\"../legal_data/LL144/LL144_Definitions.pdf\")\n",
    "documents = documents1 + documents2\n",
    "\n",
    "chunk_size = 512\n",
    "splitter = SentenceSplitter(chunk_size=chunk_size)\n",
    "index = VectorStoreIndex.from_documents(documents, llm=llm_gpt35, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine()\n",
    "\n",
    "# Load nodes W/ METADATA REFERENCES INSTEAD\n",
    "base_nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "extractors = [\n",
    "    SummaryExtractor(summaries=[\"self\"], show_progress=True),\n",
    "    QuestionsAnsweredExtractor(questions=5, show_progress=True),\n",
    "]\n",
    "\n",
    "# run metadata extractor across base nodes, get back dictionaries\n",
    "node_to_metadata = {}\n",
    "\n",
    "for extractor in extractors:\n",
    "    metadata_dicts = extractor.extract(base_nodes)\n",
    "    for node, metadata in zip(base_nodes, metadata_dicts):\n",
    "        if node.node_id not in node_to_metadata:\n",
    "            node_to_metadata[node.node_id] = metadata\n",
    "        else:\n",
    "            node_to_metadata[node.node_id].update(metadata)\n",
    "\n",
    "# cache metadata dicts\n",
    "def save_metadata_dicts(path, data):\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(data, fp)\n",
    "\n",
    "\n",
    "def load_metadata_dicts(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    return data\n",
    "\n",
    "#save_metadata_dicts(\"metadata_dicts.json\", node_to_metadata)\n",
    "metadata_dicts = load_metadata_dicts(\"metadata_dicts.json\")\n",
    "metadata = metadata_dicts\n",
    "\n",
    "# all nodes consists of source nodes, along with metadata\n",
    "import copy\n",
    "\n",
    "all_nodes = copy.deepcopy(base_nodes)\n",
    "for node_id, metadata in node_to_metadata.items():\n",
    "    for val in metadata.values():\n",
    "        all_nodes.append(IndexNode(text=val, index_id=node_id))\n",
    "\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "\n",
    "## Load index into vector index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "index = VectorStoreIndex(all_nodes)\n",
    "chat_engine = index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_nodes[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "\n",
    "topics_in_each_node = \"\"\"\n",
    "- New York City Department of Consumer and Worker Protection (DCWP)\n",
    "- Commissioner of the DCWP\n",
    "- New York City Charter\n",
    "\n",
    "Bias Audit Requirements\n",
    "- Clarifications and Definitions\n",
    "- Public Feedback and Revisions\n",
    "- Impact Ratio Calculation\n",
    "- Data for Bias Audits\n",
    "- Shared Bias Audits\n",
    "\n",
    "Automated Employment Decision Tool (AEDT)\n",
    "- Bias Audit\n",
    "- Candidate for Employment\n",
    "- Employers\n",
    "- Employment Agencies\n",
    "- EEOC\n",
    "- Independent Auditors\n",
    "- Bias Audit\n",
    "- Impact Ratio\n",
    "- Machine Learning, Statistical Modeling, Data Analytics, or Artificial Intelligence\n",
    "\n",
    "Scoring Rate\n",
    "- Screen\n",
    "- Selection Rate\n",
    "- Definition of Test Data\n",
    "- Bias Audit Requirement\n",
    "- Audit Calculations\n",
    "- Group Classification\n",
    "- Exclusion of Unknown Categories\n",
    "- Example Scenario\n",
    "\n",
    "Employers\n",
    "- Employment Agencies\n",
    "- AEDTs\n",
    "- Vendors Conducting Bias Audits\n",
    "- Use of AEDT in Hiring Processes\n",
    "- Requirement for Bias Audits\n",
    "- Example of Bias Audit Process\n",
    "- Metrics for Evaluating Bias (sex categories, number of applicants, number selected, selection rate, impact ratio)\n",
    "\n",
    "Employer\n",
    "- AEDT (Automated Employment Decision Tool)\n",
    "- Vendor\n",
    "- Independent Auditor\n",
    "- Applicants (Male and Female)\n",
    "\n",
    "Race/Ethnicity Categories\n",
    "- Hispanic or Latino\n",
    "- White (Not Hispanic or Latino)\n",
    "- Black or African American (Not Hispanic or Latino)\n",
    "\n",
    "Intersectional Categories\n",
    "- Hispanic or Latino\n",
    "- Non-Hispanic or Latino\n",
    "- Male\n",
    "\n",
    "Automated Employment Decision Tool (AEDT)\n",
    "- Candidates for Employment\n",
    "- Employees Being Considered for Promotion\n",
    "\n",
    "Employers\n",
    "- Independent Auditors\n",
    "- Demographic Groups (sex, race/ethnicity, intersectional categories)\n",
    "- Independent Auditor\n",
    "- Employer\n",
    "- Automated Employment Decision Tools (AEDTs)\n",
    "\n",
    "Sex Categories\n",
    "- Male\n",
    "- Female\n",
    "\n",
    "Race/Ethnicity Categories\n",
    "- Hispanic or Latino\n",
    "- White (Not Hispanic or Latino)\n",
    "- Black or African American (Not Hispanic or Latino)\n",
    "- Native Hawaiian or Pacific Islander (Not Hispanic or Latino)\n",
    "- Asian (Not Hispanic or Latino)\n",
    "- Native American or Alaska Native (Not Hispanic or Latino)\n",
    "- Two or More Races (Not Hispanic or Latino)\n",
    "\n",
    "Intersectional Categories\n",
    "- Data Exclusion\n",
    "- Historical Data Requirements\n",
    "- Requirements for Conducting a Bias Audit\n",
    "\n",
    "Automated Employment Decision Tool (AEDT)\n",
    "- Use of Historical Data and Test Data\n",
    "- Employment Agencies\n",
    "- Employers\n",
    "- AEDTs (Automated Employment Decision Tools)\n",
    "\n",
    "Bias Audit Restrictions\n",
    "- Published Results (§ 5-303)\n",
    "- Notice to Candidates and Employees (§ 5-304)\n",
    "- Automated Employment Decision Tool (AEDT)\n",
    "- Employers and Employment Agencies\n",
    "- Candidates for Employment\n",
    "\n",
    "Employers\n",
    "- Employment Agencies\n",
    "- Candidates for Employment\n",
    "- New York City Council\n",
    "- Council Members\n",
    "- Administrative Code of the City of New York\n",
    "\n",
    "Bias Audit Requirement\n",
    "- Notification Requirements\n",
    "- Legal References\n",
    "- Disclosure Requirements\n",
    "- Penalties\n",
    "- Enforcement\n",
    "\n",
    "Office of Administrative Trials and Hearings\n",
    "- City Agencies\n",
    "- Corporation Counsel\n",
    "- File #: Int 1894-2020\n",
    "- New York City Commission on Human Rights\n",
    "- Title 8\n",
    "\"\"\"\n",
    "\n",
    "query_gen_prompt_str = (\n",
    "\n",
    "    \"You are an expert legal assistant that generates multiple search queries (e.g. keywords or key phrases) based on a \"\n",
    "    \"single input query about a particular set of regulation. Your aim is to gather information\"\n",
    "    \"that would contribute to a complete answer to the single query.\"\n",
    "\n",
    "    \"You have been provided with a list of specific keywords and topics that are discussed in the knowledge \"\n",
    "    \"base that the search queries will be applied to. Use these keywords to guide your generation of specific search queries and fill in any gaps.\"\n",
    "\n",
    "    \"{topics_in_each_node}\"\n",
    "\n",
    "    \"Generate {num_queries} search queries, one on each line, \"\n",
    "   \"related to the following input query and using the provided keywords appropriately:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "\n",
    "num_queries=6\n",
    "query_str = query_gen_prompt_str\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "fmt_prompt = query_gen_prompt.format(\n",
    "    num_queries=num_queries - 1, query=query_str, topics_in_each_node=topics_in_each_node\n",
    ")\n",
    "print(fmt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "import asyncio\n",
    "\n",
    "# Reciprocal rank fusion\n",
    "from typing import List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# Vector Search on Each QUery\n",
    "\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# Query Gen/Rewriting stage\n",
    "from llama_index.core import PromptTemplate\n",
    "import time\n",
    "\n",
    "\n",
    "import os\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "load_dotenv()\n",
    "AZURE_API_KEY = os.getenv('AZURE_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# setup LLM\n",
    "import giskard\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from giskard.llm import set_llm_model, set_llm_api\n",
    "from giskard.llm.client import get_default_client\n",
    "from giskard.llm.client import set_llm_api, set_llm_model\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import os\n",
    "from giskard.rag import KnowledgeBase, generate_testset, QATestset\n",
    "\n",
    "\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt4o\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents2 = loader.load(file_path=\"../legal_data//LL144/LL144_Definitions.pdf\")\n",
    "documents1 = loader.load(file_path=\"../legal_data//LL144/LL144.pdf\")\n",
    "documents = documents1 + documents2\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "#index = VectorStoreIndex.from_documents(\n",
    "#    documents, transformations=[splitter], embed_model=embed_model, llm=llm_gpt35\n",
    "#)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "\n",
    "query_str = \"What is a bias audit and why do I need to do one?\"\n",
    "\n",
    "\n",
    "legal_prompt_str = (\n",
    "    \"You are an expert legal assistant that generates multiple search queries based on a \"\n",
    "    \"single input query about a particular set of regulation. Your aim is to gather information\"\n",
    "    \"that would contribute to a complete answer to the single query, tailored to legal clients.\"\n",
    "    \"Consider (if applicable) the following:\"\n",
    "    \"- definitions\"\n",
    "    \"- requirements\"\n",
    "    \"- scope\"\n",
    "    \"- jurisdiction\"\n",
    "    \"- penalties\"\n",
    "    \"Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following iput query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "\n",
    "legal_prompt_str = \"\"\"\n",
    "You are an expert legal assistant that generates multiple search queries based on a \n",
    "single input query about a particular set of regulation. Your aim is to gather information\n",
    "that would contribute to a complete answer to the single query, tailored to legal clients.\n",
    "\n",
    "Use any relevant keywords from the following list to improve the specificity of your queries:\n",
    "\n",
    "- New York City Department of Consumer and Worker Protection (DCWP)\n",
    "- Commissioner of the DCWP\n",
    "- New York City Charter\n",
    "\n",
    "Bias Audit Requirements\n",
    "- Clarifications and Definitions\n",
    "- Public Feedback and Revisions\n",
    "- Impact Ratio Calculation\n",
    "- Data for Bias Audits\n",
    "- Shared Bias Audits\n",
    "\n",
    "Automated Employment Decision Tool (AEDT)\n",
    "- Bias Audit\n",
    "- Candidate for Employment\n",
    "- Employers\n",
    "- Employment Agencies\n",
    "- EEOC\n",
    "- Independent Auditors\n",
    "- Bias Audit\n",
    "- Impact Ratio\n",
    "- Machine Learning, Statistical Modeling, Data Analytics, or Artificial Intelligence\n",
    "\n",
    "Scoring Rate\n",
    "- Screen\n",
    "- Selection Rate\n",
    "- Definition of Test Data\n",
    "- Bias Audit Requirement\n",
    "- Audit Calculations\n",
    "- Group Classification\n",
    "- Exclusion of Unknown Categories\n",
    "- Example Scenario\n",
    "\n",
    "Employers\n",
    "- Employment Agencies\n",
    "- AEDTs\n",
    "- Vendors Conducting Bias Audits\n",
    "- Use of AEDT in Hiring Processes\n",
    "- Requirement for Bias Audits\n",
    "- Example of Bias Audit Process\n",
    "- Metrics for Evaluating Bias (sex categories, number of applicants, number selected, selection rate, impact ratio)\n",
    "\n",
    "Employer\n",
    "- AEDT (Automated Employment Decision Tool)\n",
    "- Vendor\n",
    "- Independent Auditor\n",
    "- Applicants (Male and Female)\n",
    "\n",
    "Race/Ethnicity Categories\n",
    "- Hispanic or Latino\n",
    "- White (Not Hispanic or Latino)\n",
    "- Black or African American (Not Hispanic or Latino)\n",
    "\n",
    "Intersectional Categories\n",
    "- Hispanic or Latino\n",
    "- Non-Hispanic or Latino\n",
    "- Male\n",
    "\n",
    "Automated Employment Decision Tool (AEDT)\n",
    "- Candidates for Employment\n",
    "- Employees Being Considered for Promotion\n",
    "\n",
    "Employers\n",
    "- Independent Auditors\n",
    "- Demographic Groups (sex, race/ethnicity, intersectional categories)\n",
    "- Independent Auditor\n",
    "- Employer\n",
    "- Automated Employment Decision Tools (AEDTs)\n",
    "\n",
    "Sex Categories\n",
    "- Male\n",
    "- Female\n",
    "\n",
    "Race/Ethnicity Categories\n",
    "- Hispanic or Latino\n",
    "- White (Not Hispanic or Latino)\n",
    "- Black or African American (Not Hispanic or Latino)\n",
    "- Native Hawaiian or Pacific Islander (Not Hispanic or Latino)\n",
    "- Asian (Not Hispanic or Latino)\n",
    "- Native American or Alaska Native (Not Hispanic or Latino)\n",
    "- Two or More Races (Not Hispanic or Latino)\n",
    "\n",
    "Intersectional Categories\n",
    "- Data Exclusion\n",
    "- Historical Data Requirements\n",
    "- Requirements for Conducting a Bias Audit\n",
    "\n",
    "Automated Employment Decision Tool (AEDT)\n",
    "- Use of Historical Data and Test Data\n",
    "- Employment Agencies\n",
    "- Employers\n",
    "- AEDTs (Automated Employment Decision Tools)\n",
    "\n",
    "Bias Audit Restrictions\n",
    "- Published Results (§ 5-303)\n",
    "- Notice to Candidates and Employees (§ 5-304)\n",
    "- Automated Employment Decision Tool (AEDT)\n",
    "- Employers and Employment Agencies\n",
    "- Candidates for Employment\n",
    "\n",
    "Employers\n",
    "- Employment Agencies\n",
    "- Candidates for Employment\n",
    "- New York City Council\n",
    "- Council Members\n",
    "- Administrative Code of the City of New York\n",
    "\n",
    "Bias Audit Requirement\n",
    "- Notification Requirements\n",
    "- Legal References\n",
    "- Disclosure Requirements\n",
    "- Penalties\n",
    "- Enforcement\n",
    "\n",
    "Office of Administrative Trials and Hearings\n",
    "- City Agencies\n",
    "- Corporation Counsel\n",
    "- File #: Int 1894-2020\n",
    "- New York City Commission on Human Rights\n",
    "- Title 8\n",
    "\n",
    "\n",
    "Generate {num_queries} search queries, one on each line, \n",
    "related to the following input query:\\n\n",
    "Query: {query}\\n\n",
    "Queries:\\n\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_gen_prompt_str = legal_prompt_str\n",
    "\n",
    "results_path = 'rewriter_with_topics4'\n",
    "\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 6):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    #print(queries)\n",
    "    return queries\n",
    "\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "\n",
    "# get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=5\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=6\n",
    "        )\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "    \n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"context\")\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=5)\n",
    "\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    #print(answer)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_275_New.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "#report.to_html(html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'rewriter_with_topics_n7'\n",
    "\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 6):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    #print(queries)\n",
    "    return queries\n",
    "\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=5\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=8\n",
    "        )\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "    \n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "#index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine()\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=5)\n",
    "\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    #print(answer)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_275_New.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "report.to_html(html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'rewriter_with_topics_n10'\n",
    "\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 6):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    #print(queries)\n",
    "    return queries\n",
    "\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "\n",
    "# get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=5\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=11\n",
    "        )\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "    \n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "#index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine()\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=5)\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    #encoding = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "    #print(answer)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "\n",
    "    #docs_tokens = encoding.encode(documents)\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_275_New.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "report.to_html(html_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Rewriter, Just Vary K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "from giskard.rag import KnowledgeBase, QATestset, generate_testset\n",
    "from giskard import evaluate\n",
    "from giskard.llm.client import set_llm_model, set_llm_api, get_default_client\n",
    "from giskard.llm import set_llm_model\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "AZURE_API_KEY = os.getenv('AZURE_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# Setup LLM\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt4o\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Load documents\n",
    "loader = PyMuPDFReader()\n",
    "documents2 = loader.load(file_path=\"../legal_data//LL144/LL144_Definitions.pdf\")\n",
    "documents1 = loader.load(file_path=\"../legal_data//LL144/LL144.pdf\")\n",
    "documents = documents1 + documents2\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "# Define the query\n",
    "query_str = \"What is a bias audit and why do I need to do one?\"\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# Get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=5\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = [query_bundle.query_str]  # Pass through the original query directly\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "#index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine()\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=5)\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    print(answer)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_275_New.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "results_path = 'results_no_rewriter'\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "#report.to_html(html_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "\n",
    "    \"You are an expert legal assistant that generates multiple search queries (e.g. keywords or key phrases) based on a \"\n",
    "    \"single input query. Your aim is to generate search queries to gather information\"\n",
    "    \"that would contribute to a complete answer to the original single query.\"\n",
    "\n",
    "    \"Generate {num_queries} search queries, one on each line, \"\n",
    "   \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "\n",
    "topics_in_each_node=None\n",
    "\n",
    "num_queries=6\n",
    "query_str = query_gen_prompt_str\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "fmt_prompt = query_gen_prompt.format(\n",
    "    num_queries=num_queries - 1, query=query_str, topics_in_each_node=topics_in_each_node\n",
    ")\n",
    "print(fmt_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Rewriter w/ topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "from giskard.rag import KnowledgeBase, QATestset, generate_testset\n",
    "from giskard.llm.client import set_llm_model, set_llm_api, get_default_client\n",
    "from giskard.llm import set_llm_model\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "AZURE_API_KEY = os.getenv('AZURE_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# Setup LLM\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt4o\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Load documents\n",
    "loader = PyMuPDFReader()\n",
    "documents2 = loader.load(file_path=\"../legal_data//LL144/LL144_Definitions.pdf\")\n",
    "documents1 = loader.load(file_path=\"../legal_data//LL144/LL144.pdf\")\n",
    "documents = documents1 + documents2\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "# Define the query\n",
    "query_str = \"What is a bias audit and why do I need to do one?\"\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# Get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = [query_bundle.query_str]  # Pass through the original query directly\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "#index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine()\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=5)\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    #print(answer)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_275_New.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "results_path = 'results_no_rewriter_k5'\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "#report.to_html(html_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "from giskard.rag import KnowledgeBase, QATestset, generate_testset\n",
    "from giskard.llm.client import set_llm_model, set_llm_api, get_default_client\n",
    "from giskard.llm import set_llm_model\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "AZURE_API_KEY = os.getenv('AZURE_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# Setup LLM\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt4o\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Load documents\n",
    "loader = PyMuPDFReader()\n",
    "documents2 = loader.load(file_path=\"../legal_data//LL144/LL144_Definitions.pdf\")\n",
    "documents1 = loader.load(file_path=\"../legal_data//LL144/LL144.pdf\")\n",
    "documents = documents1 + documents2\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "# Define the query\n",
    "query_str = \"What is a bias audit and why do I need to do one?\"\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# Get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = [query_bundle.query_str]  # Pass through the original query directly\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "#index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine()\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=7)\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    #print(answer)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_275_New.jsonl\")\n",
    "\"\"\"report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "results_path = 'results_no_rewriter_k7'\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "#report.to_html(html_path)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_nodes = text_nodes\n",
    "\n",
    "for idx, node in enumerate(run_nodes):\n",
    "    node.id_ = f\"node_{idx}\"\n",
    "\n",
    "vector_index = VectorStoreIndex(run_nodes)\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=vector_index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "\n",
    "retriever = FusionRetriever(llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=10)\n",
    "\n",
    "retrieved_nodes = retriever.retrieve(\"What is a bias audit?\")\n",
    "\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] load\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"../retriever_eval_sets/retriever_eval_dataset.json\")\n",
    "metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever\n",
    ")\n",
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[0]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents using PyMuPDFReader\n",
    "loader = PyMuPDFReader()\n",
    "docs1 = loader.load(file_path=\"../legal_data/LL144/LL144.pdf\")\n",
    "docs2 = loader.load(file_path=\"../legal_data/LL144/LL144_Definitions.pdf\")\n",
    "\n",
    "# Combine the documents\n",
    "docs = docs1 + docs2\n",
    "\n",
    "# Initialize an empty string to store all text\n",
    "all_text = \"\"\n",
    "\n",
    "# Iterate through the documents and concatenate the text\n",
    "for doc in docs:\n",
    "    all_text += doc.text  # Assuming 'text' attribute contains the document text\n",
    "\n",
    "# Define the token to split on\n",
    "token = \"\\n§\"\n",
    "\n",
    "# Create a regular expression pattern with a lookahead assertion to split before the token\n",
    "pattern = re.compile(r\"(?=\" + re.escape(token) + \")\")\n",
    "\n",
    "# Split the text based on the pattern\n",
    "split_text = pattern.split(all_text)\n",
    "\n",
    "# Create TextNode objects from the split text\n",
    "text_nodes = []\n",
    "for idx, part in enumerate(split_text):\n",
    "    node = TextNode(text=part)\n",
    "    text_nodes.append(node)\n",
    "\n",
    "# Print the number of TextNode objects created to verify\n",
    "print(f\"Number of TextNode objects created: {len(text_nodes)}\")\n",
    "\n",
    "# Create the VectorStoreIndex with your own TextNode objects\n",
    "index = VectorStoreIndex(text_nodes)\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"context\")\n",
    "\n",
    "response = chat_engine.chat(\"what a bias audit\")\n",
    "print(f\"Chat Engine Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "AZURE_API_KEY = os.getenv('AZURE_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# Setup LLM\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt4o\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "# Define the query\n",
    "query_str = \"What is a bias audit and why do I need to do one?\"\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# Get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = [query_bundle.query_str]  # Pass through the original query directly\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=5)\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    #print(answer)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_275_New.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "results_path = 'char_split_results2'\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "#report.to_html(html_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from llama_index.core import PromptTemplate\n",
    "import time\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "\n",
    "#query_str = \"What is a bias audit and why do I need to do one?\"\n",
    "\n",
    "#query_gen_prompt_str = (\n",
    "#    \"You are an expert legal assistant that generates multiple search queries based on a \"\n",
    "#    \"single input query about a particular set of regulation. Your aim is to gather information\"\n",
    "#    \"that would contribute to a complete answer to the single query, tailored to legal clients.\"\n",
    "#    \"Consider (if applicable) the following:\"\n",
    "#    \"- definitions\"\n",
    "#    \"- requirements\"\n",
    "#    \"- scope\"\n",
    "#    \"- jurisdiction\"\n",
    "#    \"- penalties\"\n",
    "#    \"Generate {num_queries} search queries, one on each line, \"\n",
    "#    \"related to the following iput query:\\n\"\n",
    "#    \"Query: {query}\\n\"\n",
    "#    \"Queries:\\n\"\n",
    "#)\n",
    "import time\n",
    "\n",
    "\n",
    "Settings.llm=llm_gpt35\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 6, topics_in_each_node=topics_in_each_node):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str, topics_in_each_node=topics_in_each_node\n",
    "    )\n",
    "    #print(fmt_prompt)\n",
    "    #time.sleep(2)\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    #time.sleep(2)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    print(queries)\n",
    "    return queries\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "    #print(results_dict)\n",
    "    return results_dict\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=6, topics_in_each_node=topics_in_each_node\n",
    "        )\n",
    "\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "        #print(f\"final_results {final_results}\")\n",
    "\n",
    "        return final_results\n",
    "    \n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "#fusion_retriever = FusionRetriever(\n",
    " #   llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=3\n",
    "#)\n",
    "\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in all_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=5)#index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "#retriever = create_adaptive_retriever(index=index, llm=llm_gpt35, similarity_top_k=10)\n",
    "\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    # Format appropriately the history for your RAG agent\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    # Get the answer and the documents\n",
    "    #agent_output = get_answer_from_agent(messages)\n",
    "    answer = answer_fn(question, history)\n",
    "    print(f\"answer: {answer}\")\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    #print(retrieved_nodes)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    #print(documents)\n",
    "\n",
    "    # Following llama_index syntax, you can get the answer and the retrieved documents\n",
    "    #answer = agent_output.text\n",
    "    #documents = agent_output.source_nodes\n",
    "\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_Sample.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "# Adaptive Method\n",
    "#report.to_html(\"simple_rewriter_results.html\")\n",
    "results = report.to_pandas()\n",
    "results.to_csv('rewriter_with_topics4.csv', index=False)\n",
    "report.to_html(\"rewriter_with_topics4.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate optimal n queries: n=3, n=5, n=7 || k=10 for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET RESULTS FILENAME:\n",
    "results_path = 'rewriter_n3_k10'\n",
    "\n",
    "\n",
    "Settings.llm=llm_gpt35\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 6, topics_in_each_node=topics_in_each_node):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str, topics_in_each_node=topics_in_each_node\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "    #print(results_dict)\n",
    "    return results_dict\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=4, topics_in_each_node=topics_in_each_node\n",
    "        )\n",
    "\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "        #print(f\"final_results {final_results}\")\n",
    "\n",
    "        return final_results\n",
    "    \n",
    "\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in all_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=10)\n",
    "\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_Sample.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "report.to_html(html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET RESULTS FILENAME:\n",
    "results_path = 'rewriter_n5_k10'\n",
    "\n",
    "\n",
    "Settings.llm=llm_gpt35\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 6, topics_in_each_node=topics_in_each_node):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str, topics_in_each_node=topics_in_each_node\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "    #print(results_dict)\n",
    "    return results_dict\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=6, topics_in_each_node=topics_in_each_node\n",
    "        )\n",
    "\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "        #print(f\"final_results {final_results}\")\n",
    "\n",
    "        return final_results\n",
    "    \n",
    "\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in all_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=10)\n",
    "\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_Sample.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "report.to_html(html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET RESULTS FILENAME:\n",
    "results_path = 'rewriter_n7_k10'\n",
    "\n",
    "\n",
    "Settings.llm=llm_gpt35\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 6, topics_in_each_node=topics_in_each_node):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str, topics_in_each_node=topics_in_each_node\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "    #print(results_dict)\n",
    "    return results_dict\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=8, topics_in_each_node=topics_in_each_node\n",
    "        )\n",
    "\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "        #print(f\"final_results {final_results}\")\n",
    "\n",
    "        return final_results\n",
    "    \n",
    "\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in all_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=10)\n",
    "\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    \n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_Sample.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "report.to_html(html_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate top-k, No rewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET RESULTS FILENAME BASE:\n",
    "results_base_path = 'hybrid_search_k5'\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "async def run_queries(query, retrievers):\n",
    "    \"\"\"Run query against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for i, retriever in enumerate(retrievers):\n",
    "        tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await asyncio.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, query_result in enumerate(task_results):\n",
    "        results_dict[i] = query_result\n",
    "    return results_dict\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        results = asyncio.run(run_queries(query_bundle.query_str, self._retrievers))\n",
    "\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine()\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in all_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] == \"user\" else MessageRole.ASSISTANT, content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    print(f\"question: {question}\")\n",
    "    print(f\"answer: {answer}\")\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_275_New.jsonl\")\n",
    "\n",
    "for k in [3, 5, 7, 10]:\n",
    "    results_path = f'{results_base_path}{k}'\n",
    "    \n",
    "    retriever = FusionRetriever([vector_retriever, bm25_retriever], similarity_top_k=k)\n",
    "    \n",
    "    report = evaluate(get_answer_fn,\n",
    "                    testset=testset,\n",
    "                    knowledge_base=knowledge_base,\n",
    "                    metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "    results = report.to_pandas()\n",
    "    csv_path = results_path + \".csv\"\n",
    "    html_path = results_path + \".html\"\n",
    "\n",
    "    results.to_csv(csv_path, index=False)\n",
    "    report.to_html(html_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "import asyncio\n",
    "\n",
    "# Reciprocal rank fusion\n",
    "from typing import List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# Vector Search on Each QUery\n",
    "\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# Query Gen/Rewriting stage\n",
    "from llama_index.core import PromptTemplate\n",
    "import time\n",
    "\n",
    "\n",
    "import os\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "load_dotenv()\n",
    "AZURE_API_KEY = os.getenv('AZURE_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# setup LLM\n",
    "import giskard\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from giskard.llm import set_llm_model, set_llm_api\n",
    "from giskard.llm.client import get_default_client\n",
    "from giskard.llm.client import set_llm_api, set_llm_model\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import os\n",
    "from giskard.rag import KnowledgeBase, generate_testset, QATestset\n",
    "\n",
    "\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt4o\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents2 = loader.load(file_path=\"../legal_data//LL144/LL144_Definitions.pdf\")\n",
    "documents1 = loader.load(file_path=\"../legal_data//LL144/LL144.pdf\")\n",
    "documents = documents1 + documents2\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "#index = VectorStoreIndex.from_documents(\n",
    "#    documents, transformations=[splitter], embed_model=embed_model, llm=llm_gpt35\n",
    "#)\n",
    "\n",
    "\n",
    "\n",
    "query_str = \"What is a bias audit and why do I need to do one?\"\n",
    "\n",
    "\n",
    "legal_prompt_str = (\n",
    "    \"You are an expert legal assistant that generates multiple search queries based on a \"\n",
    "    \"single input query about a particular set of regulation. Your aim is to gather information\"\n",
    "    \"that would contribute to a complete answer to the single query, tailored to legal clients.\"\n",
    "    \"Consider (if applicable) the following:\"\n",
    "    \"- definitions\"\n",
    "    \"- requirements\"\n",
    "    \"- scope\"\n",
    "    \"- jurisdiction\"\n",
    "    \"- penalties\"\n",
    "    \"Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following iput query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "\n",
    "legal_prompt_str = \"\"\"\n",
    "You are an expert legal assistant that generates multiple search queries based on a \n",
    "single input query about a particular set of regulation. Your aim is to gather information\n",
    "that would contribute to a complete answer to the single query, tailored to legal clients.\n",
    "\n",
    "Use any relevant keywords from the following list to improve the specificity of your queries:\n",
    "\n",
    "- New York City Department of Consumer and Worker Protection (DCWP)\n",
    "- Commissioner of the DCWP\n",
    "- New York City Charter\n",
    "\n",
    "Bias Audit Requirements\n",
    "- Clarifications and Definitions\n",
    "- Public Feedback and Revisions\n",
    "- Impact Ratio Calculation\n",
    "- Data for Bias Audits\n",
    "- Shared Bias Audits\n",
    "\n",
    "Automated Employment Decision Tool (AEDT)\n",
    "- Bias Audit\n",
    "- Candidate for Employment\n",
    "- Employers\n",
    "- Employment Agencies\n",
    "- EEOC\n",
    "- Independent Auditors\n",
    "- Bias Audit\n",
    "- Impact Ratio\n",
    "- Machine Learning, Statistical Modeling, Data Analytics, or Artificial Intelligence\n",
    "\n",
    "Scoring Rate\n",
    "- Screen\n",
    "- Selection Rate\n",
    "- Definition of Test Data\n",
    "- Bias Audit Requirement\n",
    "- Audit Calculations\n",
    "- Group Classification\n",
    "- Exclusion of Unknown Categories\n",
    "- Example Scenario\n",
    "\n",
    "Employers\n",
    "- Employment Agencies\n",
    "- AEDTs\n",
    "- Vendors Conducting Bias Audits\n",
    "- Use of AEDT in Hiring Processes\n",
    "- Requirement for Bias Audits\n",
    "- Example of Bias Audit Process\n",
    "- Metrics for Evaluating Bias (sex categories, number of applicants, number selected, selection rate, impact ratio)\n",
    "\n",
    "Employer\n",
    "- AEDT (Automated Employment Decision Tool)\n",
    "- Vendor\n",
    "- Independent Auditor\n",
    "- Applicants (Male and Female)\n",
    "\n",
    "Race/Ethnicity Categories\n",
    "- Hispanic or Latino\n",
    "- White (Not Hispanic or Latino)\n",
    "- Black or African American (Not Hispanic or Latino)\n",
    "\n",
    "Intersectional Categories\n",
    "- Hispanic or Latino\n",
    "- Non-Hispanic or Latino\n",
    "- Male\n",
    "\n",
    "Automated Employment Decision Tool (AEDT)\n",
    "- Candidates for Employment\n",
    "- Employees Being Considered for Promotion\n",
    "\n",
    "Employers\n",
    "- Independent Auditors\n",
    "- Demographic Groups (sex, race/ethnicity, intersectional categories)\n",
    "- Independent Auditor\n",
    "- Employer\n",
    "- Automated Employment Decision Tools (AEDTs)\n",
    "\n",
    "Sex Categories\n",
    "- Male\n",
    "- Female\n",
    "\n",
    "Race/Ethnicity Categories\n",
    "- Hispanic or Latino\n",
    "- White (Not Hispanic or Latino)\n",
    "- Black or African American (Not Hispanic or Latino)\n",
    "- Native Hawaiian or Pacific Islander (Not Hispanic or Latino)\n",
    "- Asian (Not Hispanic or Latino)\n",
    "- Native American or Alaska Native (Not Hispanic or Latino)\n",
    "- Two or More Races (Not Hispanic or Latino)\n",
    "\n",
    "Intersectional Categories\n",
    "- Data Exclusion\n",
    "- Historical Data Requirements\n",
    "- Requirements for Conducting a Bias Audit\n",
    "\n",
    "Automated Employment Decision Tool (AEDT)\n",
    "- Use of Historical Data and Test Data\n",
    "- Employment Agencies\n",
    "- Employers\n",
    "- AEDTs (Automated Employment Decision Tools)\n",
    "\n",
    "Bias Audit Restrictions\n",
    "- Published Results (§ 5-303)\n",
    "- Notice to Candidates and Employees (§ 5-304)\n",
    "- Automated Employment Decision Tool (AEDT)\n",
    "- Employers and Employment Agencies\n",
    "- Candidates for Employment\n",
    "\n",
    "Employers\n",
    "- Employment Agencies\n",
    "- Candidates for Employment\n",
    "- New York City Council\n",
    "- Council Members\n",
    "- Administrative Code of the City of New York\n",
    "\n",
    "Bias Audit Requirement\n",
    "- Notification Requirements\n",
    "- Legal References\n",
    "- Disclosure Requirements\n",
    "- Penalties\n",
    "- Enforcement\n",
    "\n",
    "Office of Administrative Trials and Hearings\n",
    "- City Agencies\n",
    "- Corporation Counsel\n",
    "- File #: Int 1894-2020\n",
    "- New York City Commission on Human Rights\n",
    "- Title 8\n",
    "\n",
    "\n",
    "Generate {num_queries} search queries, one on each line, \n",
    "related to the following input query:\\n\n",
    "Query: {query}\\n\n",
    "Queries:\\n\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_gen_prompt_str = legal_prompt_str\n",
    "\n",
    "\n",
    "\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 6):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    print(queries)\n",
    "    return queries\n",
    "\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "\n",
    "# get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=5\n",
    ")\n",
    "\n",
    "def fuse_results(results_dict, similarity_top_k: int = 5):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._llm = llm\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(\n",
    "            self._llm, query_bundle.query_str, num_queries=4\n",
    "        )\n",
    "        results = asyncio.run(run_queries(queries, self._retrievers))\n",
    "        final_results = fuse_results(\n",
    "            results, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results\n",
    "    \n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "#index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine()\n",
    "# create vector index\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in all_nodes], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=5)\n",
    "\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "def answer_fn(question, history=None):\n",
    "    if history:\n",
    "        answer = chat_engine.chat(question, chat_history=[ChatMessage(role=MessageRole.USER if msg[\"role\"] ==\"user\" else MessageRole.ASSISTANT,\n",
    "                                                          content=msg[\"content\"]) for msg in history])\n",
    "    else:\n",
    "        answer = chat_engine.chat(question, chat_history=[])\n",
    "    return str(answer)\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    messages = history if history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "    answer = answer_fn(question, history)\n",
    "    print(answer)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    # Instead of returning a simple string, we return the AgentAnswer object which\n",
    "    # allows us to specify the retrieved context which is used by RAGAS metrics\n",
    "    return AgentAnswer(\n",
    "        message=answer,\n",
    "        documents=documents\n",
    "    )\n",
    "\n",
    "testset = QATestset.load(\"../giskard_test_sets/LL144_Sample.jsonl\")\n",
    "report = evaluate(get_answer_fn,\n",
    "                testset=testset,\n",
    "                knowledge_base=knowledge_base,\n",
    "                metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "\n",
    "\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + \".csv\"\n",
    "html_path = results_path + \".html\"\n",
    "\n",
    "results.to_csv(csv_path, index=False)\n",
    "report.to_html(html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = FusionRetriever(\n",
    "    llm_gpt35, [vector_retriever, bm25_retriever], similarity_top_k=10)\n",
    "\n",
    "\n",
    "retrieved_nodes = retriever.retrieve(\"what is a bias audit?\")\n",
    "\n",
    "print(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in retrieved_nodes:\n",
    "    print(node.get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate query router method w/ adaptive k value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "import openai\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env file\n",
    "import pandas as pd\n",
    "from llama_index.core.evaluation import (\n",
    "    RetrieverEvaluator,\n",
    "    get_retrieval_results_df,\n",
    ")\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from datetime import datetime\n",
    "from llama_index.core import (\n",
    "    StorageContext, VectorStoreIndex, SimpleDirectoryReader, \n",
    "    get_response_synthesizer, Settings\n",
    ")\n",
    "import traceback\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "from llama_index.core.evaluation import generate_question_context_pairs, QueryResponseDataset\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from typing import List\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from llama_index.core import PromptTemplate\n",
    "import time\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import json\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "\n",
    "import giskard\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from giskard.llm import set_llm_model, set_llm_api\n",
    "from giskard.llm.client import get_default_client\n",
    "from giskard.llm.client import set_llm_api, set_llm_model\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import os\n",
    "from giskard.rag import KnowledgeBase, generate_testset, QATestset\n",
    "#from giskard.rag.question_generators import complex_questions, complex_situational_questions, compare_questions, simple_questions, rule_conclusion_questions, distracting_questions, na_questions, vague_questions, oos_questions, situational_questions, double_questions, conversational_questions\n",
    "from giskard.rag import AgentAnswer\n",
    "from giskard.rag import evaluate, RAGReport\n",
    "from giskard.rag.metrics.ragas_metrics import ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from huggingface_hub import InferenceApi\n",
    "from transformers import pipeline\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "\n",
    "# LLamaIndex Imports\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.vector_stores.neo4jvector import Neo4jVectorStore\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext, get_response_synthesizer,PropertyGraphIndex, Document, KnowledgeGraphIndex\n",
    "from llama_index.core.evaluation import (DatasetGenerator,FaithfulnessEvaluator,RelevancyEvaluator)\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from llama_index.core.schema import IndexNode, NodeWithScore, Document, QueryBundle\n",
    "from llama_index.core.extractors import (SummaryExtractor,QuestionsAnsweredExtractor)\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine, KnowledgeGraphQueryEngine, CitationQueryEngine\n",
    "from llama_index.core.prompts.base import PromptTemplate, PromptType\n",
    "from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "# Giskard imports\n",
    "import giskard\n",
    "from giskard.rag import AgentAnswer, evaluate, RAGReport, KnowledgeBase, generate_testset, QATestset\n",
    "from giskard.rag.metrics.ragas_metrics import ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision\n",
    "from giskard.llm import set_llm_model, set_llm_api\n",
    "from giskard.llm.client import get_default_client\n",
    "from giskard.llm.embeddings import set_default_embedding, get_default_embedding\n",
    "\n",
    "def remove_openai_api_key():\n",
    "    if \"OPENAI_API_KEY\" in os.environ:\n",
    "        del os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Need to specify this here otherwise it doesn't work - Giskard Problem (?)\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"GSK_AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"GSK_AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_API_VERSION\"] = os.getenv(\"AZURE_API_VERSION\")\n",
    "os.environ[\"GSK_LLM_API\"] = \"azure\"\n",
    "os.environ[\"GSK_LLM_MODEL\"] = \"gpt-4o-mini\"\n",
    "set_llm_api(\"azure\")\n",
    "set_llm_model('gpt-4o-mini')\n",
    "\n",
    "AZURE_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Setup LLM\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt-4o-mini\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=\"gpt35\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "# Setup LLM\n",
    "llm_gpt4o_ = AzureOpenAI(\n",
    "    deployment_name=\"gpt4o\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"GPT4O_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"GPT4O_AZURE_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"GPT4O_API_VERSION\")\n",
    ")\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "# Verify LLM setup\n",
    "client = get_default_client()\n",
    "\n",
    "#print(\"Client base URL:\", client._client._base_url)\n",
    "#print(\"Client API key:\", client._client.api_key)\n",
    "#print(\"Client API version:\", client._client._api_version)\n",
    "#print(\"Client model:\", client.model)\n",
    "\n",
    "assert client._client._base_url == f'{os.environ[\"AZURE_OPENAI_ENDPOINT\"]}/openai/'\n",
    "assert client._client.api_key == os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "assert client._client._api_version == os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "url = os.getenv(\"NEO4J_URI\")\n",
    "username = os.getenv(\"NEO4J_USERNAME\")\n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "database = os.getenv(\"NEO4J_DATABASE\")\n",
    "\n",
    "# Load documents\n",
    "loader = PyMuPDFReader()\n",
    "documents2 = loader.load(file_path='../legal_data//LL144/LL144_Definitions.pdf')\n",
    "documents1 = loader.load(file_path='../legal_data//LL144/LL144.pdf')\n",
    "documents = documents1 + documents2\n",
    "\n",
    "# Create vector index\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine(chat_mode='context')\n",
    "\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=['text'])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "# Set results filename base\n",
    "results_base_path = 'hybrid_search2_k'\n",
    "\n",
    "# Define async run queries\n",
    "async def run_queries(query, retrievers):\n",
    "    tasks = [retriever.aretrieve(query) for retriever in retrievers]\n",
    "    task_results = await asyncio.gather(*tasks)\n",
    "    return {i: result for i, result in enumerate(task_results)}\n",
    "\n",
    "# Define fusion retriever\n",
    "def fuse_results(results_dict, similarity_top_k=5):\n",
    "    k = 60.0\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "    reranked_results = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    reranked_nodes = [text_to_node[text] for text in reranked_results.keys()]\n",
    "    for text, score in reranked_results.items():\n",
    "        text_to_node[text].score = score\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    def __init__(self, retrievers: List[BaseRetriever], similarity_top_k=5):\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        results = asyncio.run(run_queries(query_bundle.query_str, self._retrievers))\n",
    "        return fuse_results(results, similarity_top_k=self._similarity_top_k)\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    chat_history = [ChatMessage(role=MessageRole.USER if msg['role'] == 'user' else MessageRole.ASSISTANT, content=msg['content']) for msg in history] if history else []\n",
    "    return str(chat_engine.chat(question, chat_history=chat_history))\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    messages = history if history else []\n",
    "    messages.append({'role': 'user', 'content': question})\n",
    "    answer = answer_fn(question, history)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    return AgentAnswer(message=answer, documents=documents)\n",
    "\n",
    "testset = QATestset.load('../giskard_test_sets/LL144_275_New.jsonl')\n",
    "\n",
    "for k in [3, 5, 7, 10]:\n",
    "    results_path = f'{results_base_path}{k}'\n",
    "    retriever = FusionRetriever([vector_retriever, bm25_retriever], similarity_top_k=k)\n",
    "    report = evaluate(get_answer_fn, testset=testset, knowledge_base=knowledge_base, metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "    results = report.to_pandas()\n",
    "    csv_path = results_path + '.csv'\n",
    "    html_path = results_path + '.html'\n",
    "    results.to_csv(csv_path, index=False)\n",
    "    # report.to_html(html_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Fetch API keys from environment variables\n",
    "\n",
    "AZURE_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "#AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "print(AZURE_API_KEY)\n",
    "#print(AZURE_DEPLOYMENT_NAME)\n",
    "print(AZURE_API_VERSION)\n",
    "print(AZURE_OPENAI_ENDPOINT)\n",
    "\n",
    "\n",
    "from huggingface_hub import InferenceApi\n",
    "from transformers import pipeline\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")\n",
    "\n",
    "Settings.embed_model=embed_model\n",
    "\n",
    "# setup LLM\n",
    "\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt-4o-mini\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=\"gpt35\",#AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "set_llm_api(\"azure\")\n",
    "set_llm_model('gpt-4o-mini')\n",
    "# You'll need to provide the name of the model that you've deployed\n",
    "# Beware, the model provided must be capable of using function calls\n",
    "client = get_default_client()\n",
    "assert client._client._base_url == f'{os.environ[\"AZURE_OPENAI_ENDPOINT\"]}/openai/'\n",
    "assert client._client.api_key == os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "assert client._client._api_version == os.environ[\"OPENAI_API_VERSION\"]\n",
    "assert client.model == \"gpt-4o-mini\"#os.environ[\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\"]\n",
    "\n",
    "\n",
    "# Load documents\n",
    "loader = PyMuPDFReader()\n",
    "documents2 = loader.load(file_path='../legal_data//LL144/LL144_Definitions.pdf')\n",
    "documents1 = loader.load(file_path='../legal_data//LL144/LL144.pdf')\n",
    "documents = documents1 + documents2\n",
    "\n",
    "# Create vector index\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine(chat_mode='context')\n",
    "\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=['text'])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "# Set results filename base\n",
    "results_base_path = 'adaptive_k_ft_classifier_3'\n",
    "\n",
    "# Load classifier\n",
    "classifier = pipeline(\"text-classification\", model=\"rk68/distilbert-q-classifier-3\", device='mps')\n",
    "\n",
    "# Function to classify query and determine top-k value\n",
    "def classify_query_and_get_topk(query):\n",
    "    classification = classifier(query)[0]\n",
    "    label = int(classification['label'].split('_')[-1])\n",
    "    if label == 0:\n",
    "        return 3\n",
    "    elif label == 1:\n",
    "        return 5\n",
    "    else:\n",
    "        return 7\n",
    "\n",
    "# Async run queries\n",
    "async def run_queries(query, retrievers):\n",
    "    tasks = [retriever.aretrieve(query) for retriever in retrievers]\n",
    "    task_results = await asyncio.gather(*tasks)\n",
    "    return {i: result for i, result in enumerate(task_results)}\n",
    "\n",
    "# Define fusion retriever\n",
    "def fuse_results(results_dict, similarity_top_k):\n",
    "    k = 60.0\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "    reranked_results = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    reranked_nodes = [text_to_node[text] for text in reranked_results.keys()]\n",
    "    for text, score in reranked_results.items():\n",
    "        text_to_node[text].score = score\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    def __init__(self, retrievers: List[BaseRetriever]):\n",
    "        self._retrievers = retrievers\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        top_k = classify_query_and_get_topk(query_bundle.query_str)\n",
    "        print(f\"top-k: {top_k}\")\n",
    "        results = asyncio.run(run_queries(query_bundle.query_str, self._retrievers))\n",
    "        return fuse_results(results, similarity_top_k=top_k)\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    chat_history = [ChatMessage(role=MessageRole.USER if msg['role'] == 'user' else MessageRole.ASSISTANT, content=msg['content']) for msg in history] if history else []\n",
    "    return str(chat_engine.chat(question, chat_history=chat_history))\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    messages = history if history else []\n",
    "    messages.append({'role': 'user', 'content': question})\n",
    "    answer = answer_fn(question, history)\n",
    "    print(f\"answer: {answer}\")\n",
    "    retriever = FusionRetriever([vector_retriever, bm25_retriever])\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    print(f\"retrieved nodes {retrieved_nodes}\")\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    return AgentAnswer(message=answer, documents=documents)\n",
    "\n",
    "# Load test set\n",
    "testset = QATestset.load('../giskard_test_sets/LL144_Sample.jsonl')\n",
    "\n",
    "results_path = f'{results_base_path}'\n",
    "report = evaluate(get_answer_fn, testset=testset, knowledge_base=knowledge_base, metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + '.csv'\n",
    "html_path = results_path + '.html'\n",
    "results.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load documents\n",
    "loader = PyMuPDFReader()\n",
    "documents2 = loader.load(file_path='../legal_data//LL144/LL144_Definitions.pdf')\n",
    "documents1 = loader.load(file_path='../legal_data//LL144/LL144.pdf')\n",
    "documents = documents1 + documents2\n",
    "\n",
    "# Create vector index\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[splitter])\n",
    "chat_engine = index.as_chat_engine(chat_mode='context')\n",
    "\n",
    "text_nodes = splitter(documents)\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=['text'])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "\n",
    "# Vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "# BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=10)\n",
    "\n",
    "# Set results filename base\n",
    "results_base_path = 'adaptive_k_ft_classifier_2'\n",
    "\n",
    "# Load classifier\n",
    "classifier = pipeline(\"text-classification\", model=\"rk68/distilbert-q-classifier-2\", device='mps')\n",
    "\n",
    "# Function to classify query and determine top-k value\n",
    "def classify_query_and_get_topk(query):\n",
    "    classification = classifier(query)[0]\n",
    "    label = int(classification['label'].split('_')[-1])\n",
    "    if label == 0:\n",
    "        return 5\n",
    "    elif label == 1:\n",
    "        return 7\n",
    "    #else:\n",
    "    #    return 7\n",
    "\n",
    "# Async run queries\n",
    "async def run_queries(query, retrievers):\n",
    "    tasks = [retriever.aretrieve(query) for retriever in retrievers]\n",
    "    task_results = await asyncio.gather(*tasks)\n",
    "    return {i: result for i, result in enumerate(task_results)}\n",
    "\n",
    "# Define fusion retriever\n",
    "def fuse_results(results_dict, similarity_top_k):\n",
    "    k = 60.0\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "    reranked_results = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    reranked_nodes = [text_to_node[text] for text in reranked_results.keys()]\n",
    "    for text, score in reranked_results.items():\n",
    "        text_to_node[text].score = score\n",
    "    return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    def __init__(self, retrievers: List[BaseRetriever]):\n",
    "        self._retrievers = retrievers\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        top_k = classify_query_and_get_topk(query_bundle.query_str)\n",
    "        #print(f\"top-k: {top_k}\")\n",
    "        results = asyncio.run(run_queries(query_bundle.query_str, self._retrievers))\n",
    "        return fuse_results(results, similarity_top_k=top_k)\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    chat_history = [ChatMessage(role=MessageRole.USER if msg['role'] == 'user' else MessageRole.ASSISTANT, content=msg['content']) for msg in history] if history else []\n",
    "    return str(chat_engine.chat(question, chat_history=chat_history))\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    messages = history if history else []\n",
    "    messages.append({'role': 'user', 'content': question})\n",
    "    answer = answer_fn(question, history)\n",
    "    #print(f\"answer: {answer}\")\n",
    "    retriever = FusionRetriever([vector_retriever, bm25_retriever])\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    #print(f\"retrieved nodes {retrieved_nodes}\")\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    return AgentAnswer(message=answer, documents=documents)\n",
    "\n",
    "# Load test set\n",
    "testset = QATestset.load('../giskard_test_sets/LL144_275_New.jsonl')\n",
    "\n",
    "results_path = f'{results_base_path}'\n",
    "report = evaluate(get_answer_fn, testset=testset, knowledge_base=knowledge_base, metrics=[ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision])\n",
    "results = report.to_pandas()\n",
    "csv_path = results_path + '.csv'\n",
    "html_path = results_path + '.html'\n",
    "results.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# Load the classifier\n",
    "classifier = pipeline(\"text-classification\", model=\"rk68/distilbert-q-classifier-3\", device='mps')\n",
    "\n",
    "# Path to the input JSONL file and output CSV file\n",
    "input_jsonl_path = '../giskard_test_sets/LL144_275_New.jsonl'\n",
    "output_csv_path = 'output_file.csv'\n",
    "\n",
    "# Initialize a list to store classification results\n",
    "classification_results = []\n",
    "\n",
    "# Read and classify each query in the JSONL file\n",
    "with open(input_jsonl_path, 'r') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        data = json.loads(line)\n",
    "        query = data['question']  # Use the 'question' field\n",
    "        classification = classifier(query)[0]\n",
    "        classification_results.append({'question': query, 'label': classification['label']})\n",
    "\n",
    "# Write the classification results to a CSV file\n",
    "with open(output_csv_path, 'w', newline='') as csv_file:\n",
    "    fieldnames = ['question', 'label']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for result in classification_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "# Calculate and print the percentage of each class label\n",
    "labels = [result['label'] for result in classification_results]\n",
    "label_counts = Counter(labels)\n",
    "total_labels = len(labels)\n",
    "\n",
    "print(\"Percentage of each class label:\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / total_labels) * 100\n",
    "    print(f\"{label}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# Load the classifier\n",
    "classifier = pipeline(\"text-classification\", model=\"rk68/distilbert-q-classifier-2\", device='mps')\n",
    "\n",
    "# Path to the input JSONL file and output CSV file\n",
    "input_jsonl_path = '../giskard_test_sets/LL144_275_New.jsonl'\n",
    "output_csv_path = 'output_file.csv'\n",
    "\n",
    "# Initialize a list to store classification results\n",
    "classification_results = []\n",
    "\n",
    "# Read and classify each query in the JSONL file\n",
    "with open(input_jsonl_path, 'r') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        data = json.loads(line)\n",
    "        query = data['question']  # Use the 'question' field\n",
    "        classification = classifier(query)[0]\n",
    "        classification_results.append({'question': query, 'label': classification['label']})\n",
    "\n",
    "# Write the classification results to a CSV file\n",
    "with open(output_csv_path, 'w', newline='') as csv_file:\n",
    "    fieldnames = ['question', 'label']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for result in classification_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "# Calculate and print the percentage of each class label\n",
    "labels = [result['label'] for result in classification_results]\n",
    "label_counts = Counter(labels)\n",
    "total_labels = len(labels)\n",
    "\n",
    "print(\"Percentage of each class label:\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / total_labels) * 100\n",
    "    print(f\"{label}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
