{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers shap torch -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unknown type passed as data object: <class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/rishi/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m probabilities\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Use the selected subset for SHAP's KernelExplainer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mKernelExplainer(predict_proba, subset_tokenized)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Generate SHAP values for the subset of the explanation dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39mshap_values(subset_tokenized)\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/shap/explainers/_kernel.py:96\u001b[0m, in \u001b[0;36mKernelExplainer.__init__\u001b[0;34m(self, model, data, feature_names, link, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_index_ordered \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mkeep_index_ordered\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m convert_to_model(model, keep_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_index)\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m convert_to_data(data, keep_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkeep_index)\n\u001b[1;32m     97\u001b[0m model_null \u001b[39m=\u001b[39m match_model_to_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[1;32m     99\u001b[0m \u001b[39m# enforce our current input type limitations\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/shap/utils/_legacy.py:230\u001b[0m, in \u001b[0;36mconvert_to_data\u001b[0;34m(val, keep_index)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m SparseData(val)\n\u001b[1;32m    229\u001b[0m emsg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown type passed as data object: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(val)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 230\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(emsg)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unknown type passed as data object: <class 'dict'>"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rk68/distilbert-q-classifier-3\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"rk68/distilbert-q-classifier-3\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the explanation dataset (JSON file)\n",
    "file_path = \"../giskard_test_sets/LL144_275_New.jsonl\"\n",
    "explanation_questions = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        explanation_questions.append(data['question'])\n",
    "\n",
    "# Tokenize the explanation questions\n",
    "explanation_tokenized = tokenizer(explanation_questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Select a representative subset of the explanation questions for SHAP analysis\n",
    "subset_indices = np.random.choice(len(explanation_questions), size=50, replace=False)  # For example, 50 samples\n",
    "subset_tokenized = {key: value[subset_indices] for key, value in explanation_tokenized.items()}\n",
    "\n",
    "# Define the prediction function for SHAP\n",
    "def predict_proba(inputs):\n",
    "    # Move inputs to the appropriate device\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Get logits from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "# Use the selected subset for SHAP's KernelExplainer\n",
    "explainer = shap.KernelExplainer(predict_proba, subset_tokenized)\n",
    "\n",
    "# Generate SHAP values for the subset of the explanation dataset\n",
    "shap_values = explainer.shap_values(subset_tokenized)\n",
    "\n",
    "# Visualize SHAP values for the first few samples\n",
    "shap.summary_plot(shap_values, feature_names=tokenizer.convert_ids_to_tokens(subset_tokenized['input_ids'][0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/rp6l70s568gc8_g2c6pn2lnc0000gn/T/ipykernel_1601/2239563485.py:8: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\") if torch.has_mps else torch.device(\"cpu\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb95c78ee012437e902cf1c5a272568b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 16.17 GB, other allocations: 98.67 MB, max allowed: 18.13 GB). Tried to allocate 3.02 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/rishi/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mKernelExplainer(predict_proba, combined_inputs[:\u001b[39m10\u001b[39m])  \u001b[39m# Use only 10 samples\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Generate SHAP values for the explanation dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mshap_values(combined_inputs[:\u001b[39m10\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Visualize SHAP values for the first few samples\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m shap\u001b[39m.\u001b[39msummary_plot(shap_values, feature_names\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(input_ids[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()))\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/shap/explainers/_kernel.py:271\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_index:\n\u001b[1;32m    270\u001b[0m     data \u001b[39m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m], index_name)\n\u001b[0;32m--> 271\u001b[0m explanations\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain(data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mgc_collect\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    273\u001b[0m     gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/shap/explainers/_kernel.py:476\u001b[0m, in \u001b[0;36mKernelExplainer.explain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m weight_left \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[39m.\u001b[39msum()\n\u001b[1;32m    475\u001b[0m \u001b[39m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    478\u001b[0m \u001b[39m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[1;32m    479\u001b[0m phi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mgroups_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD))\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/shap/explainers/_kernel.py:615\u001b[0m, in \u001b[0;36mKernelExplainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_index_ordered:\n\u001b[1;32m    614\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39msort_index()\n\u001b[0;32m--> 615\u001b[0m modelOut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mf(data)\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(modelOut, (pd\u001b[39m.\u001b[39mDataFrame, pd\u001b[39m.\u001b[39mSeries)):\n\u001b[1;32m    617\u001b[0m     modelOut \u001b[39m=\u001b[39m modelOut\u001b[39m.\u001b[39mvalues\n",
      "\u001b[1;32m/Users/rishi/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Get logits from the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rishi/Documents/MSc%20DSML/MSc%20Project/ProjectFiles/RAG_Files/code/experiment_notebooks/classifier_shap.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Convert logits to probabilities using softmax and move back to CPU for SHAP processing\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:883\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    881\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 883\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    884\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    885\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    886\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    887\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    888\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    889\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    890\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    891\u001b[0m )\n\u001b[1;32m    892\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    893\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:703\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m         attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(input_shape, device\u001b[39m=\u001b[39mdevice)  \u001b[39m# (bs, seq_length)\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    704\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[1;32m    705\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    706\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    707\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    708\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    709\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    710\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:464\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    456\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    457\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    458\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m         output_attentions,\n\u001b[1;32m    462\u001b[0m     )\n\u001b[1;32m    463\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    465\u001b[0m         hidden_state,\n\u001b[1;32m    466\u001b[0m         attn_mask,\n\u001b[1;32m    467\u001b[0m         head_mask[i],\n\u001b[1;32m    468\u001b[0m         output_attentions,\n\u001b[1;32m    469\u001b[0m     )\n\u001b[1;32m    471\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:390\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    391\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    392\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    393\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    394\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    395\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    399\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MSc DSML/MSc Project/ProjectFiles/RAG_Files/rag1/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:211\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    208\u001b[0m k \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_lin(key))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m v \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin(value))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    212\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    213\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 16.17 GB, other allocations: 98.67 MB, max allowed: 18.13 GB). Tried to allocate 3.02 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Ensure that `mps` is available\n",
    "device = torch.device(\"mps\") if torch.has_mps else torch.device(\"cpu\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rk68/distilbert-q-classifier-3\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"rk68/distilbert-q-classifier-3\")\n",
    "model.to(device)  # Move the model to the MPS device\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load only 10 samples from the explanation dataset (JSON file)\n",
    "file_path = \"../giskard_test_sets/LL144_275_New.jsonl\"\n",
    "explanation_questions = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 10:  # Stop after 10 samples\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        explanation_questions.append(data['question'])\n",
    "\n",
    "# Tokenize the explanation questions\n",
    "explanation_tokenized = tokenizer(explanation_questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Define the prediction function for SHAP\n",
    "def predict_proba(combined_inputs):\n",
    "    # Determine the length of input_ids and attention_mask from the original data\n",
    "    half = combined_inputs.shape[1] // 2\n",
    "    input_ids = combined_inputs[:, :half]\n",
    "    attention_mask = combined_inputs[:, half:]\n",
    "\n",
    "    # Convert back to tensors and move to the `mps` device\n",
    "    input_ids = torch.tensor(input_ids).to(device)\n",
    "    attention_mask = torch.tensor(attention_mask).to(device)\n",
    "    \n",
    "    # Get logits from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities using softmax and move back to CPU for SHAP processing\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "# Use the 10 samples from your combined inputs for SHAP's KernelExplainer\n",
    "input_ids = explanation_tokenized['input_ids'].numpy()\n",
    "attention_mask = explanation_tokenized['attention_mask'].numpy()\n",
    "\n",
    "# Combine into a single array\n",
    "combined_inputs = np.concatenate([input_ids, attention_mask], axis=1)\n",
    "\n",
    "# Initialize KernelExplainer\n",
    "explainer = shap.KernelExplainer(predict_proba, combined_inputs[:10])  # Use only 10 samples\n",
    "\n",
    "# Generate SHAP values for the explanation dataset\n",
    "shap_values = explainer.shap_values(combined_inputs[:10])\n",
    "\n",
    "# Visualize SHAP values for the first few samples\n",
    "shap.summary_plot(shap_values, feature_names=tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
