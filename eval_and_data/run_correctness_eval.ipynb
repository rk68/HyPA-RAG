{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate, PromptTemplate\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API DETAILS\n",
    "AZURE_API_KEY = os.getenv('AZURE_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt-4o-mini\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this as our example to verify against\n",
    "df = pd.read_csv('results_no_rewriter_k3_correctness.csv')\n",
    "\n",
    "# Extract the 'question' and 'reference answer' columns into lists\n",
    "questions = df['question'].tolist()\n",
    "reference_answers = df['reference_answer'].tolist()\n",
    "agent_answers = df['agent_answer'].tolist()\n",
    "\n",
    "print(agent_answers[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLamaIndex Correctness Evaluator\n",
    "\n",
    "Evaluate the default LLamaIndex CorrectnessEvaluator and save the results for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "evaluator = CorrectnessEvaluator(llm=llm_gpt4o)\n",
    "results_default = []\n",
    "\n",
    "for question,ref_answer, agent_answer in zip(questions, reference_answers, agent_answers):\n",
    "    result = evaluator.evaluate(\n",
    "        query=question,\n",
    "        response=agent_answer,\n",
    "        reference=ref_answer\n",
    "    )\n",
    "\n",
    "    results_default.append(result)\n",
    "\n",
    "# Extract the scores from results_default\n",
    "correctness_scores = [result.score for result in results_default]\n",
    "\n",
    "# Add the correctness_default column to the original DataFrame\n",
    "df['correctness_default'] = correctness_scores\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv('results_no_rewriter_k3_correctness.csv', index=False)\n",
    "\n",
    "# Calculate the average score\n",
    "total_score = sum(result.score for result in results_default)\n",
    "average_score = total_score / len(results_default)\n",
    "print(average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the scores from results_default\n",
    "correctness_reasons1 = [result.feedback for result in results_default]\n",
    "\n",
    "# Add the correctness_default column to the original DataFrame\n",
    "df['reason_default'] = correctness_reasons1\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv('results_no_rewriter_k3_correctness.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Correctness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECTNESS_SYS_TMPL = \"\"\"\n",
    "You are an expert evaluation system for a question answering chatbot.\n",
    "\n",
    "You are given the following information:\n",
    "- a user query,\n",
    "- a reference answer, and\n",
    "- a generated answer.\n",
    "\n",
    "Your job is to judge the correctness of the generated answer.\n",
    "Output a single score that represents a holistic evaluation.\n",
    "You must return your response in a line with only the score.\n",
    "Do not return answers in any other format.\n",
    "On a separate line provide your reasoning for the score as well.\n",
    "\n",
    "Follow these guidelines for scoring:\n",
    "- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\n",
    "- Use the following criteria for scoring correctness:\n",
    "\n",
    "1. Score of 1:\n",
    "    - The generated answer is completely incorrect.\n",
    "    - The generated answer contains major factual errors or misconceptions.\n",
    "    - The generated answer does not address any components of the user query correctly.\n",
    "\n",
    "2. Score of 2:\n",
    "    - The generated answer has significant mistakes.\n",
    "    - The generated answer addresses at least one component of the user query correctly but has major errors in other parts.\n",
    "\n",
    "3. Score of 3:\n",
    "    - The generated answer is partially correct.\n",
    "    - The generated answer addresses multiple components of the user query correctly but includes some incorrect information.\n",
    "    - Minor factual errors are present.\n",
    "\n",
    "4. Score of 4:\n",
    "    - The generated answer is mostly correct.\n",
    "    - The generated answer correctly addresses all components of the user query with minimal errors.\n",
    "    - The errors do not substantially affect the overall correctness of the answer.\n",
    "\n",
    "5. Score of 5:\n",
    "    - The generated answer is completely correct.\n",
    "    - The generated answer addresses all components of the user query correctly without any errors.\n",
    "    - The answer is factually accurate and aligns perfectly with the reference answer.\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_USER_TMPL = \"\"\"\n",
    "## User Query\n",
    "{query}\n",
    "\n",
    "## Reference Answer\n",
    "{reference_answer}\n",
    "\n",
    "## Generated Answer\n",
    "{generated_answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chat_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=CORRECTNESS_SYS_TMPL),\n",
    "        ChatMessage(role=MessageRole.USER, content=CORRECTNESS_USER_TMPL),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_correctness_eval(\n",
    "    query_str: str,\n",
    "    reference_answer: str,\n",
    "    generated_answer: str,\n",
    "    llm: AzureOpenAI,\n",
    "    threshold: float = 4.0,\n",
    ") -> Dict:\n",
    "    \"\"\"Run correctness eval.\"\"\"\n",
    "    fmt_messages = eval_chat_template.format_messages(\n",
    "        llm=llm,\n",
    "        query=query_str,\n",
    "        reference_answer=reference_answer,\n",
    "        generated_answer=generated_answer,\n",
    "    )\n",
    "    chat_response = llm.chat(fmt_messages)\n",
    "    raw_output = chat_response.message.content\n",
    "\n",
    "    # Extract from response\n",
    "    score_str, reasoning_str = raw_output.split(\"\\n\", 1)\n",
    "    score = float(score_str)\n",
    "    reasoning = reasoning_str.lstrip(\"\\n\")\n",
    "\n",
    "    return {\"passing\": score >= threshold, \"score\": score, \"reason\": reasoning}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_method1 = []\n",
    "\n",
    "for question,ref_answer, agent_answer in zip(questions, reference_answers, agent_answers):\n",
    "    result = run_correctness_eval(question, ref_answer, agent_answer, llm=llm_gpt4o, threshold=4.0)\n",
    "    results_method1.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the scores from results_default\n",
    "correctness_scores = [result['score']for result in results_method1]\n",
    "\n",
    "# Add the correctness_default column to the original DataFrame\n",
    "df['correctness_method1'] = correctness_scores\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv('results_no_rewriter_k3_correctness.csv', index=False)\n",
    "\n",
    "# Calculate the average score\n",
    "total_score = sum(result['score'] for result in results_method1)\n",
    "average_score = total_score / len(results_default)\n",
    "print(average_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECTNESS_SYS_TMPL = \"\"\"\n",
    "You are an expert evaluation system for a question answering chatbot.\n",
    "\n",
    "You are given the following information:\n",
    "- a user query,\n",
    "- a reference answer, and\n",
    "- a generated answer.\n",
    "\n",
    "Your job is to judge the correctness of the generated answer.\n",
    "Output a single score that represents a holistic evaluation.\n",
    "You must return your response in a line with only the score.\n",
    "Do not return answers in any other format.\n",
    "On a separate line provide your reasoning for the score as well.\n",
    "The reasoning MUST NOT UNDER ANY CIRCUMSTANCES BE LONGER THAN 1 SENTENCE.\n",
    "\n",
    "Follow these guidelines for scoring:\n",
    "- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\n",
    "- Use the following criteria for scoring correctness:\n",
    "\n",
    "1. Score of 1:\n",
    "    - The generated answer is completely incorrect.\n",
    "    - Contains major factual errors or misconceptions.\n",
    "    - Does not address any components of the user query correctly.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France?\"\n",
    "      - Generated Answer: \"The capital of France is Berlin.\"\n",
    "\n",
    "2. Score of 2:\n",
    "    - Significant mistakes are present.\n",
    "    - Addresses at least one component of the user query correctly but has major errors in other parts.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France and its population?\"\n",
    "      - Generated Answer: \"The capital of France is Paris, and its population is 100 million.\"\n",
    "\n",
    "3. Score of 3:\n",
    "    - Partially correct with some incorrect information.\n",
    "    - Addresses multiple components of the user query correctly.\n",
    "    - Minor factual errors are present.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France and its population?\"\n",
    "      - Generated Answer: \"The capital of France is Paris, and its population is around 3 million.\"\n",
    "\n",
    "4. Score of 4:\n",
    "    - Mostly correct with minimal errors.\n",
    "    - Correctly addresses all components of the user query.\n",
    "    - Errors do not substantially affect the overall correctness.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France and its population?\"\n",
    "      - Generated Answer: \"The capital of France is Paris, and its population is approximately 2.1 million.\"\n",
    "\n",
    "5. Score of 5:\n",
    "    - Completely correct.\n",
    "    - Addresses all components of the user query correctly without any errors.\n",
    "    - Providing more information than necessary should not be penalized as long as all provided information is correct.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France and its population?\"\n",
    "      - Generated Answer: \"The capital of France is Paris, and its population is approximately 2.1 million. Paris is known for its rich history and iconic landmarks such as the Eiffel Tower and Notre-Dame Cathedral.\"\n",
    "\n",
    "Checklist for Evaluation:\n",
    "  - Component Coverage: Does the answer cover all parts of the query?\n",
    "  - Factual Accuracy: Are the facts presented in the answer correct?\n",
    "  - Error Severity: How severe are any errors present in the answer?\n",
    "  - Comparison to Reference: How closely does the answer align with the reference answer?\n",
    "\n",
    "Edge Cases:\n",
    "  - If the answer includes both correct and completely irrelevant information, focus only on the relevant portions for scoring.\n",
    "  - If the answer is correct but incomplete, score based on the completeness criteria within the relevant score range.\n",
    "  - If the answer provides more information than necessary, it should not be penalized as long as all information is correct.\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_USER_TMPL = \"\"\"\n",
    "## User Query\n",
    "{query}\n",
    "\n",
    "## Reference Answer\n",
    "{reference_answer}\n",
    "\n",
    "## Generated Answer\n",
    "{generated_answer}\n",
    "\"\"\"\n",
    "\n",
    "eval_chat_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=CORRECTNESS_SYS_TMPL),\n",
    "        ChatMessage(role=MessageRole.USER, content=CORRECTNESS_USER_TMPL),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def run_correctness_eval(\n",
    "    query_str: str,\n",
    "    reference_answer: str,\n",
    "    generated_answer: str,\n",
    "    llm: AzureOpenAI,\n",
    "    threshold: float = 4.0,\n",
    ") -> Dict:\n",
    "    \"\"\"Run correctness eval.\"\"\"\n",
    "    fmt_messages = eval_chat_template.format_messages(\n",
    "        llm=llm,\n",
    "        query=query_str,\n",
    "        reference_answer=reference_answer,\n",
    "        generated_answer=generated_answer,\n",
    "    )\n",
    "    chat_response = llm.chat(fmt_messages)\n",
    "    raw_output = chat_response.message.content\n",
    "\n",
    "    # Extract from response\n",
    "    score_str, reasoning_str = raw_output.split(\"\\n\", 1)\n",
    "    score = float(score_str)\n",
    "    reasoning = reasoning_str.lstrip(\"\\n\")\n",
    "\n",
    "    return {\"passing\": score >= threshold, \"score\": score, \"reason\": reasoning}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def process_correctness_scores(file_path, llm, threshold=4.0, num_rows=None):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract the 'question', 'reference answer', and 'agent answer' columns into lists\n",
    "    questions = df['question'].tolist()\n",
    "    reference_answers = df['reference_answer'].tolist()\n",
    "    agent_answers = df['agent_answer'].tolist()\n",
    "\n",
    "    # If num_rows is None, process all rows\n",
    "    if num_rows is None:\n",
    "        num_rows = len(df)\n",
    "\n",
    "    # Initialize the results list\n",
    "    results_method2 = []\n",
    "\n",
    "    # Use tqdm for the loading bar\n",
    "    for question, ref_answer, agent_answer in tqdm(zip(questions[:num_rows], reference_answers[:num_rows], agent_answers[:num_rows]), total=num_rows, desc=\"Processing\"):\n",
    "        result = run_correctness_eval(question, ref_answer, agent_answer, llm=llm, threshold=threshold)\n",
    "        results_method2.append(result)\n",
    "\n",
    "    # Extract the scores from results_method2\n",
    "    correctness_scores2 = [result['score'] for result in results_method2]\n",
    "    correctness_reasons2 = [result['reason'] for result in results_method2]\n",
    "\n",
    "    # Add correctness scores and reasons to the DataFrame\n",
    "    df.loc[:num_rows-1, 'correctness_method2'] = correctness_scores2\n",
    "    df.loc[:num_rows-1, 'reason_method2'] = correctness_reasons2\n",
    "\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    # Calculate the average score\n",
    "    total_score = sum(result['score'] for result in results_method2)\n",
    "    average_score = total_score / len(results_method2)\n",
    "    print(f\"Average Score: {average_score}\")\n",
    "\n",
    "    return average_score\n",
    "\n",
    "\n",
    "score = process_correctness_scores(file_path='../exp3/class3_HyPA2_k_Q_eu.csv', llm=llm_gpt4o, threshold=4.0, num_rows=None)\n",
    "#score = process_correctness_scores(file_path='rewriter_with_topics_n5.csv', llm=llm_gpt4o, threshold=4.0, num_rows=None)\n",
    "#score = process_correctness_scores(file_path='rewriter_with_topics_n7.csv', llm=llm_gpt4o, threshold=4.0, num_rows=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install tqdm\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "!jupyter nbextension install --py widgetsnbextension --sys-prefix\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average Scores:\n",
    "- Default: 4.213754646840148\n",
    "- Method 1: 3.8513011152416357\n",
    "- Method 2: 4.074349442379182\n",
    "\n",
    "\n",
    "##### Runtime for each:\n",
    "\n",
    "- Default: 14m 23.1s\n",
    "- Method 1: 19m 04.6s\n",
    "- Method 2: 45m 27.3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results_no_rewriter_k3_correctness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map correctness_human to True/False\n",
    "df1['correctness_human_mapped'] = df1['correctness_human'] == 1\n",
    "\n",
    "# Function to map the correctness methods to True/False\n",
    "def map_correctness(col):\n",
    "    return df1[col] >= 4\n",
    "\n",
    "# Apply the mapping to the correctness columns\n",
    "df1['correctness_default_mapped'] = map_correctness('correctness_default')\n",
    "df1['correctness_method1_mapped'] = map_correctness('correctness_method1')\n",
    "df1['correctness_method2_mapped'] = map_correctness('correctness_method2')\n",
    "\n",
    "# Calculate the alignment scores\n",
    "alignment_default = (df1['correctness_human_mapped'] == df1['correctness_default_mapped']).mean()\n",
    "alignment_method1 = (df1['correctness_human_mapped'] == df1['correctness_method1_mapped']).mean()\n",
    "alignment_method2 = (df1['correctness_human_mapped'] == df1['correctness_method2_mapped']).mean()\n",
    "\n",
    "# Compute the average correctness by mapping 4+ to 1 and below 4 to 0\n",
    "average_correctness_human = df1['correctness_human'].mean()\n",
    "average_correctness_default_mapped = df1['correctness_default_mapped'].mean()\n",
    "average_correctness_method1_mapped = df1['correctness_method1_mapped'].mean()\n",
    "average_correctness_method2_mapped = df1['correctness_method2_mapped'].mean()\n",
    "\n",
    "# Print results to 5 decimal places\n",
    "\n",
    "# Alignment Scores\n",
    "print(f\"Alignment Score for correctness_default: {alignment_default:.5f}\")\n",
    "print(f\"Alignment Score for correctness_method1: {alignment_method1:.5f}\")\n",
    "print(f\"Alignment Score for correctness_method2: {alignment_method2:.5f}\")\n",
    "\n",
    "# Average Correctness Scores (Mapped 4+ to 1, below 4 to 0)\n",
    "print(f\"Average Correctness Score for correctness_human: {average_correctness_human:.5f}\")\n",
    "print(f\"Average Correctness Score for correctness_default (Mapped): {average_correctness_default_mapped:.5f}\")\n",
    "print(f\"Average Correctness Score for correctness_method1 (Mapped): {average_correctness_method1_mapped:.5f}\")\n",
    "print(f\"Average Correctness Score for correctness_method2 (Mapped): {average_correctness_method2_mapped:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics\n",
    "def calculate_metrics(group):\n",
    "    alignment_default = (group['correctness_human_mapped'] == group['correctness_default_mapped']).mean()\n",
    "    alignment_method1 = (group['correctness_human_mapped'] == group['correctness_method1_mapped']).mean()\n",
    "    alignment_method2 = (group['correctness_human_mapped'] == group['correctness_method2_mapped']).mean()\n",
    "    alignment_giskard = (group['correctness_human_mapped'] == group['correctness_mapped']).mean()\n",
    "    \n",
    "    average_correctness_human = group['correctness_human'].mean()\n",
    "    average_correctness_default_mapped = group['correctness_default_mapped'].mean()\n",
    "    average_correctness_method1_mapped = group['correctness_method1_mapped'].mean()\n",
    "    average_correctness_method2_mapped = group['correctness_method2_mapped'].mean()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'alignment_default': alignment_default,\n",
    "        'alignment_method1': alignment_method1,\n",
    "        'alignment_method2': alignment_method2,\n",
    "        'alignment_giskard': alignment_giskard,\n",
    "        'average_correctness_human': average_correctness_human,\n",
    "        'average_correctness_default_mapped': average_correctness_default_mapped,\n",
    "        'average_correctness_method1_mapped': average_correctness_method1_mapped,\n",
    "        'average_correctness_method2_mapped': average_correctness_method2_mapped\n",
    "    })\n",
    "\n",
    "# Apply the function to each group\n",
    "metrics_by_question_type = grouped.apply(calculate_metrics)\n",
    "\n",
    "# Calculate overall scores across all question types\n",
    "overall_metrics = calculate_metrics(df)\n",
    "overall_metrics.name = 'Overall'\n",
    "\n",
    "# Append overall metrics to the dataframe using concat\n",
    "metrics_by_question_type = pd.concat([metrics_by_question_type, overall_metrics.to_frame().T])\n",
    "\n",
    "metrics_by_question_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map correctness_human to True/False\n",
    "from scipy.stats import spearmanr\n",
    "df1['correctness_human_mapped'] = df1['correctness_human'] == 1\n",
    "\n",
    "# Function to map the correctness methods to True/False\n",
    "def map_correctness(col):\n",
    "    return df1[col] >= 4\n",
    "\n",
    "# Apply the mapping to the correctness columns\n",
    "df1['correctness_default_mapped'] = map_correctness('correctness_default')\n",
    "df1['correctness_method1_mapped'] = map_correctness('correctness_method1')\n",
    "df1['correctness_method2_mapped'] = map_correctness('correctness_method2')\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(group):\n",
    "    alignment_default = (group['correctness_human_mapped'] == group['correctness_default_mapped']).mean()\n",
    "    alignment_method1 = (group['correctness_human_mapped'] == group['correctness_method1_mapped']).mean()\n",
    "    alignment_method2 = (group['correctness_human_mapped'] == group['correctness_method2_mapped']).mean()\n",
    "    alignment_giskard = (group['correctness_human_mapped'] == group['correctness']).mean()\n",
    "    \n",
    "    average_correctness_human = group['correctness_human'].mean()\n",
    "    average_correctness_default_mapped = group['correctness_default_mapped'].mean()\n",
    "    average_correctness_method1_mapped = group['correctness_method1_mapped'].mean()\n",
    "    average_correctness_method2_mapped = group['correctness_method2_mapped'].mean()\n",
    "    \n",
    "    # Compute Spearman correlations\n",
    "    spearman_default = spearmanr(group['correctness_human_mapped'], group['correctness_default_mapped']).correlation\n",
    "    spearman_method1 = spearmanr(group['correctness_human_mapped'], group['correctness_method1_mapped']).correlation\n",
    "    spearman_method2 = spearmanr(group['correctness_human_mapped'], group['correctness_method2_mapped']).correlation\n",
    "    \n",
    "    return pd.Series({\n",
    "        'alignment_default': alignment_default,\n",
    "        'alignment_method1': alignment_method1,\n",
    "        'alignment_method2': alignment_method2,\n",
    "        'alignment_giskard': alignment_giskard,\n",
    "        'average_correctness_human': average_correctness_human,\n",
    "        'average_correctness_default_mapped': average_correctness_default_mapped,\n",
    "        'average_correctness_method1_mapped': average_correctness_method1_mapped,\n",
    "        'average_correctness_method2_mapped': average_correctness_method2_mapped,\n",
    "        'spearman_default': spearman_default,\n",
    "        'spearman_method1': spearman_method1,\n",
    "        'spearman_method2': spearman_method2\n",
    "    })\n",
    "\n",
    "# Apply the function to the whole dataset\n",
    "overall_metrics = calculate_metrics(df1)\n",
    "\n",
    "# Apply the function by question type\n",
    "df1['question_type'] = df1['metadata'].apply(lambda x: eval(x)['question_type'])\n",
    "grouped_metrics = df1.groupby('question_type').apply(calculate_metrics)\n",
    "overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Assuming df1 is already defined and loaded with the required data\n",
    "# df1 = pd.read_csv('results_no_rewriter_k3_correctness.csv')  # Example loading data\n",
    "\n",
    "# Map correctness_human to True/False\n",
    "df1['correctness_human_mapped'] = df1['correctness_human'] == 1\n",
    "\n",
    "# Function to map the correctness methods to True/False\n",
    "def map_correctness(col):\n",
    "    return df1[col] >= 4\n",
    "\n",
    "# Apply the mapping to the correctness columns\n",
    "df1['correctness_default_mapped'] = map_correctness('correctness_default')\n",
    "df1['correctness_method1_mapped'] = map_correctness('correctness_method1')\n",
    "df1['correctness_method2_mapped'] = map_correctness('correctness_method2')\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(group):\n",
    "    alignment_default = (group['correctness_human_mapped'] == group['correctness_default_mapped']).mean()\n",
    "    alignment_method1 = (group['correctness_human_mapped'] == group['correctness_method1_mapped']).mean()\n",
    "    alignment_method2 = (group['correctness_human_mapped'] == group['correctness_method2_mapped']).mean()\n",
    "    alignment_giskard = (group['correctness_human_mapped'] == group['correctness']).mean()\n",
    "    \n",
    "    average_correctness_human = group['correctness_human'].mean()\n",
    "    average_correctness_default_mapped = group['correctness_default_mapped'].mean()\n",
    "    average_correctness_method1_mapped = group['correctness_method1_mapped'].mean()\n",
    "    average_correctness_method2_mapped = group['correctness_method2_mapped'].mean()\n",
    "    \n",
    "    # Compute Spearman correlations\n",
    "    spearman_default = spearmanr(group['correctness_human_mapped'], group['correctness_default_mapped']).correlation\n",
    "    spearman_method1 = spearmanr(group['correctness_human_mapped'], group['correctness_method1_mapped']).correlation\n",
    "    spearman_method2 = spearmanr(group['correctness_human_mapped'], group['correctness_method2_mapped']).correlation\n",
    "    \n",
    "    return pd.Series({\n",
    "        'alignment_default': alignment_default,\n",
    "        'alignment_method1': alignment_method1,\n",
    "        'alignment_method2': alignment_method2,\n",
    "        'alignment_giskard': alignment_giskard,\n",
    "        'average_correctness_human': average_correctness_human,\n",
    "        'average_correctness_default_mapped': average_correctness_default_mapped,\n",
    "        'average_correctness_method1_mapped': average_correctness_method1_mapped,\n",
    "        'average_correctness_method2_mapped': average_correctness_method2_mapped,\n",
    "        'spearman_default': spearman_default,\n",
    "        'spearman_method1': spearman_method1,\n",
    "        'spearman_method2': spearman_method2\n",
    "    })\n",
    "\n",
    "# Apply the function to the whole dataset\n",
    "overall_metrics = calculate_metrics(df1).to_frame('Overall').T\n",
    "\n",
    "# Apply the function by question type\n",
    "df1['question_type'] = df1['metadata'].apply(lambda x: eval(x)['question_type'])\n",
    "grouped_metrics = df1.groupby('question_type').apply(calculate_metrics)\n",
    "\n",
    "# Combine overall metrics with grouped metrics\n",
    "combined_metrics = pd.concat([grouped_metrics, overall_metrics])\n",
    "\n",
    "# Extract the alignment metrics\n",
    "alignment_metrics = combined_metrics[['alignment_default', 'alignment_method1', 'alignment_method2', 'alignment_giskard']]\n",
    "\n",
    "# Plotting the alignment metrics with enhanced styling\n",
    "fig, ax = plt.subplots(figsize=(20, 14))  # Make the figure wider\n",
    "\n",
    "alignment_metrics.plot(kind='bar', ax=ax, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.8)  # Adjust width for larger bars\n",
    "ax.set_title('Percentage Agreement by Question Type', fontsize=40, weight='bold')\n",
    "ax.set_xlabel('Question Type', fontsize=35)\n",
    "ax.set_ylabel('Percentage Agreement', fontsize=35)\n",
    "ax.tick_params(axis='x', labelsize=30)\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(10))  # Set smaller y-axis tick intervals\n",
    "ax.legend(['Baseline', 'Prompt 1', 'Prompt 2', 'Giskard'], fontsize=24, title_fontsize=30, frameon=True, edgecolor='black', loc='upper center', bbox_to_anchor=(0.5, -0.27), ncol=4, framealpha=1, borderpad=0.7, fancybox=True, shadow=True, facecolor='white')  # Position the legend below and centered\n",
    "ax.legend().remove()  # Remove the legend\n",
    "# Setting thicker border around the plot\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(4)\n",
    "\n",
    "plt.xticks(rotation=65)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Map the columns according to the instructions\n",
    "df['correctness'] = df['correctness'].map({True: 1, False: 0})\n",
    "df['correctness_default'] = df['correctness_default'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method1'] = df['correctness_method1'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method2'] = df['correctness_method2'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_human'] = df['correctness_human'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Calculate the Spearman correlation coefficients\n",
    "spearman_default = spearmanr(df['correctness_default'], df['correctness_human'])\n",
    "spearman_method1 = spearmanr(df['correctness_method1'], df['correctness_human'])\n",
    "spearman_method2 = spearmanr(df['correctness_method2'], df['correctness_human'])\n",
    "\n",
    "spearman_default, spearman_method1, spearman_method2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file again\n",
    "file_path = 'results_no_rewriter_k3_correctness.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Re-map the columns according to the instructions\n",
    "df['correctness'] = df['correctness'].map({True: 1, False: 0})\n",
    "df['correctness_default'] = df['correctness_default'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method1'] = df['correctness_method1'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method2'] = df['correctness_method2'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_human'] = df['correctness_human'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Extract question types from metadata\n",
    "df['question_type'] = df['metadata'].apply(lambda x: eval(x)['question_type'])\n",
    "\n",
    "# Recompute the Spearman correlation coefficients\n",
    "spearman_default = spearmanr(df['correctness_default'], df['correctness_human']).correlation\n",
    "spearman_method1 = spearmanr(df['correctness_method1'], df['correctness_human']).correlation\n",
    "spearman_method2 = spearmanr(df['correctness_method2'], df['correctness_human']).correlation\n",
    "spearman_giskard = spearmanr(df['correctness'], df['correctness_human']).correlation\n",
    "\n",
    "# Compute Spearman coefficients by question type\n",
    "spearman_by_question_type = df.groupby('question_type').apply(lambda x: pd.Series({\n",
    "    'Baseline': spearmanr(x['correctness_default'], x['correctness_human']).correlation,\n",
    "    'Prompt 1': spearmanr(x['correctness_method1'], x['correctness_human']).correlation,\n",
    "    'Prompt 2': spearmanr(x['correctness_method2'], x['correctness_human']).correlation,\n",
    "    'Giskard': spearmanr(x['correctness'], x['correctness_human']).correlation\n",
    "}))\n",
    "\n",
    "# Add overall Spearman scores\n",
    "spearman_by_question_type.loc['Overall'] = {\n",
    "    'Baseline': spearman_default,\n",
    "    'Prompt 1': spearman_method1,\n",
    "    'Prompt 2': spearman_method2,\n",
    "    'Giskard': spearman_giskard\n",
    "}\n",
    "\n",
    "# Plotting the scores with enhanced styling\n",
    "fig, ax = plt.subplots(figsize=(20, 14))  # Make the figure wider\n",
    "\n",
    "# Plot Spearman's Coefficient\n",
    "spearman_by_question_type.plot(kind='bar', ax=ax, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.8)\n",
    "ax.set_title(\"Spearman's Coefficient by Question Type\", fontsize=33, weight='bold')\n",
    "ax.set_xlabel('Question Type', fontsize=30)\n",
    "ax.set_ylabel(\"Spearman's Coefficient\", fontsize=30)\n",
    "ax.tick_params(axis='x', labelsize=26)\n",
    "ax.tick_params(axis='y', labelsize=26)\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.legend(['Baseline', 'Prompt 1', 'Prompt 2', 'Giskard'], fontsize=24, title_fontsize=30, frameon=True, edgecolor='black', loc='upper center', bbox_to_anchor=(0.5, -0.27), ncol=4, framealpha=1, borderpad=0.7, fancybox=True, shadow=True, facecolor='white')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(3)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Load the CSV file again\n",
    "file_path = 'results_no_rewriter_k3_correctness.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Re-map the columns according to the instructions\n",
    "df['correctness'] = df['correctness'].map({True: 1, False: 0})\n",
    "df['correctness_default'] = df['correctness_default'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method1'] = df['correctness_method1'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method2'] = df['correctness_method2'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_human'] = df['correctness_human'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Extract question types from metadata\n",
    "df['question_type'] = df['metadata'].apply(lambda x: eval(x)['question_type'])\n",
    "\n",
    "# Compute the p-values for Spearman coefficients by question type\n",
    "spearman_pvalues_by_question_type = df.groupby('question_type').apply(lambda x: pd.Series({\n",
    "    'Baseline': spearmanr(x['correctness_default'], x['correctness_human']).pvalue,\n",
    "    'Prompt 1': spearmanr(x['correctness_method1'], x['correctness_human']).pvalue,\n",
    "    'Prompt 2': spearmanr(x['correctness_method2'], x['correctness_human']).pvalue,\n",
    "    'Giskard': spearmanr(x['correctness'], x['correctness_human']).pvalue\n",
    "}))\n",
    "\n",
    "# Add overall Spearman p-values\n",
    "spearman_pvalues_by_question_type.loc['Overall'] = {\n",
    "    'Baseline': spearmanr(df['correctness_default'], df['correctness_human']).pvalue,\n",
    "    'Prompt 1': spearmanr(df['correctness_method1'], df['correctness_human']).pvalue,\n",
    "    'Prompt 2': spearmanr(df['correctness_method2'], df['correctness_human']).pvalue,\n",
    "    'Giskard': spearmanr(df['correctness'], df['correctness_human']).pvalue\n",
    "}\n",
    "\n",
    "# Plotting the p-values for Spearman coefficients with enhanced styling\n",
    "fig, ax = plt.subplots(figsize=(20, 14))  # Make the figure wider\n",
    "\n",
    "# Plot Spearman p-values by question type\n",
    "spearman_pvalues_by_question_type.plot(kind='bar', ax=ax, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.85)\n",
    "ax.set_title(\"P-Values for Spearman Coefficient by Question Type\", fontsize=40, weight='bold')\n",
    "ax.set_xlabel('Question Type', fontsize=35)\n",
    "ax.set_ylabel(\"P-Value\", fontsize=35)\n",
    "ax.tick_params(axis='x', labelsize=30)\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(4)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65)\n",
    "ax.get_legend().remove()  # Remove the legend\n",
    "\n",
    "# Add dashed horizontal lines for specified p-values with different colors and labels\n",
    "thresholds = [0.05, 0.10, 0.20, 0.50]\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "x_lim = ax.get_xlim()[1] - 0.25\n",
    "for threshold, color in zip(thresholds, colors):\n",
    "    ax.axhline(y=threshold, color=color, linestyle='--', linewidth=2)\n",
    "    ax.text(x_lim, threshold, f'p = {threshold}', color=color, \n",
    "            ha='right', va='bottom', fontsize=25, weight='bold', backgroundcolor='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the p-values\n",
    "spearman_pvalues_by_question_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Load the CSV file again \n",
    "file_path = '../exp2/results/no_rewriter_rag_results/results_no_rewriter_k3_correctness.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Re-map the columns according to the instructions\n",
    "df['correctness'] = df['correctness'].map({True: 1, False: 0})\n",
    "df['correctness_default'] = df['correctness_default'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method1'] = df['correctness_method1'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method2'] = df['correctness_method2'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_human'] = df['correctness_human'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Extract question types from metadata\n",
    "df['question_type'] = df['metadata'].apply(lambda x: eval(x)['question_type'])\n",
    "\n",
    "# Recompute the Spearman correlation coefficients\n",
    "spearman_default = spearmanr(df['correctness_default'], df['correctness_human']).correlation\n",
    "spearman_method1 = spearmanr(df['correctness_method1'], df['correctness_human']).correlation\n",
    "spearman_method2 = spearmanr(df['correctness_method2'], df['correctness_human']).correlation\n",
    "spearman_giskard = spearmanr(df['correctness'], df['correctness_human']).correlation\n",
    "\n",
    "# Compute Spearman coefficients and p-values by question type\n",
    "spearman_results = df.groupby('question_type').apply(lambda x: pd.Series({\n",
    "    'Baseline': spearmanr(x['correctness_default'], x['correctness_human']),\n",
    "    'Prompt 1': spearmanr(x['correctness_method1'], x['correctness_human']),\n",
    "    'Prompt 2': spearmanr(x['correctness_method2'], x['correctness_human']),\n",
    "    'Giskard': spearmanr(x['correctness'], x['correctness_human'])\n",
    "}))\n",
    "\n",
    "# Extract correlations and p-values into separate dataframes\n",
    "spearman_by_question_type = spearman_results.map(lambda x: x.correlation)\n",
    "spearman_pvalues_by_question_type = spearman_results.map(lambda x: x.pvalue)\n",
    "\n",
    "# Add overall Spearman scores\n",
    "spearman_by_question_type.loc['Overall'] = {\n",
    "    'Baseline': spearman_default,\n",
    "    'Prompt 1': spearman_method1,\n",
    "    'Prompt 2': spearman_method2,\n",
    "    'Giskard': spearman_giskard\n",
    "}\n",
    "\n",
    "# Function to calculate overall Spearman scores excluding values based on p-value threshold\n",
    "def calculate_overall_spearman_excluding(df, pvalue_threshold):\n",
    "    filtered_df = df.map(lambda x: x if x.pvalue <= pvalue_threshold else None).dropna(how='any')\n",
    "    overall_spearman = filtered_df.map(lambda x: x.correlation).mean()\n",
    "    return overall_spearman\n",
    "\n",
    "# Calculate overall Spearman scores excluding high p-values\n",
    "spearman_exclude_0_5 = calculate_overall_spearman_excluding(spearman_results, 0.5)\n",
    "spearman_exclude_0_2 = calculate_overall_spearman_excluding(spearman_results, 0.2)\n",
    "spearman_exclude_0_1 = calculate_overall_spearman_excluding(spearman_results, 0.1)\n",
    "spearman_exclude_0_05 = calculate_overall_spearman_excluding(spearman_results, 0.05)\n",
    "\n",
    "# Add these new scores to the dataframe for plotting\n",
    "spearman_by_question_type.loc['Overall Excluding p > 0.5'] = spearman_exclude_0_5\n",
    "spearman_by_question_type.loc['Overall Excluding p > 0.2'] = spearman_exclude_0_2\n",
    "spearman_by_question_type.loc['Overall Excluding p > 0.1'] = spearman_exclude_0_1\n",
    "spearman_by_question_type.loc['Overall Excluding p > 0.05'] = spearman_exclude_0_05\n",
    "\n",
    "# Plotting the scores by question type with enhanced styling\n",
    "fig, ax = plt.subplots(figsize=(20, 14))  # Make the figure wider\n",
    "\n",
    "# Plot Spearman's Coefficient by question type\n",
    "spearman_by_question_type.drop(['Overall', 'Overall Excluding p > 0.5', 'Overall Excluding p > 0.2', 'Overall Excluding p > 0.1', 'Overall Excluding p > 0.05']).plot(kind='bar', ax=ax, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.8)\n",
    "ax.set_title(\"Spearman's Coefficient by Question Type\", fontsize=40, weight='bold')\n",
    "ax.set_xlabel('Question Type', fontsize=35)\n",
    "ax.set_ylabel(\"Spearman's Coefficient\", fontsize=35)\n",
    "ax.tick_params(axis='x', labelsize=30)\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.get_legend().remove()  # Remove the legend\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(4)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the overall scores with high p-values excluded\n",
    "fig, ax = plt.subplots(figsize=(20, 16))  # Make the figure wider\n",
    "\n",
    "# Prepare the dataframe for overall scores only\n",
    "overall_scores = spearman_by_question_type.loc[['Overall', 'Overall Excluding p > 0.5', 'Overall Excluding p > 0.2', 'Overall Excluding p > 0.1', 'Overall Excluding p > 0.05']]\n",
    "\n",
    "# Update the x labels\n",
    "overall_scores.index = ['Overall', 'Ex. p > 0.5', 'Ex. p > 0.2', 'Ex. p > 0.1', 'Ex. p > 0.05']\n",
    "\n",
    "# Plot overall Spearman's Coefficient\n",
    "overall_scores.plot(kind='bar', ax=ax, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.8)\n",
    "ax.set_title(\"Overall Spearman's Coefficient Excluding High P-Values\", fontsize=40, weight='bold')\n",
    "ax.set_xlabel('P-Value Exclusion Criteria', fontsize=35)\n",
    "ax.set_ylabel(\"Spearman's Coefficient\", fontsize=35)\n",
    "ax.tick_params(axis='x', labelsize=30)\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax.legend(['Baseline', 'Prompt 1', 'Prompt 2', 'Giskard'], fontsize=30, title_fontsize=35, frameon=True, edgecolor='black', loc='upper center', bbox_to_anchor=(0.5, -0.55), ncol=4, framealpha=1., borderpad=.85, fancybox=True, shadow=False, facecolor='white')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(4)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the p-values for Spearman coefficients with enhanced styling\n",
    "fig, ax = plt.subplots(figsize=(20, 14))  # Make the figure wider\n",
    "\n",
    "# Plot Spearman p-values by question type\n",
    "spearman_pvalues_by_question_type.plot(kind='bar', ax=ax, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.85)\n",
    "ax.set_title(\"P-Values for Spearman Coefficient by Question Type\", fontsize=40, weight='bold')\n",
    "ax.set_xlabel('Question Type', fontsize=35)\n",
    "ax.set_ylabel(\"P-Value\", fontsize=35)\n",
    "ax.tick_params(axis='x', labelsize=30)\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(4)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=65)\n",
    "ax.get_legend().remove()  # Remove the legend\n",
    "\n",
    "# Add dashed horizontal lines for specified p-values with different colors and labels\n",
    "thresholds = [0.05, 0.10, 0.20, 0.50]\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "x_lim = ax.get_xlim()[1] - 0.5  # Adjust to move the labels a bit to the left\n",
    "for threshold, color in zip(thresholds, colors):\n",
    "    ax.axhline(y=threshold, color=color, linestyle='--', linewidth=2)\n",
    "    ax.text(x_lim, threshold, f'p = {threshold}', color=color, \n",
    "            ha='right', va='bottom', fontsize=25, weight='bold', backgroundcolor='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Load the CSV file again \n",
    "file_path = '../exp2/results/no_rewriter_rag_results/results_no_rewriter_k3_correctness.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Re-map the columns according to the instructions\n",
    "df['correctness'] = df['correctness'].map({True: 1, False: 0})\n",
    "df['correctness_default'] = df['correctness_default'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method1'] = df['correctness_method1'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_method2'] = df['correctness_method2'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "df['correctness_human'] = df['correctness_human'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Extract question types from metadata\n",
    "df['question_type'] = df['metadata'].apply(lambda x: eval(x)['question_type'])\n",
    "\n",
    "# Recompute the Spearman correlation coefficients\n",
    "spearman_default = spearmanr(df['correctness_default'], df['correctness_human']).correlation\n",
    "spearman_method1 = spearmanr(df['correctness_method1'], df['correctness_human']).correlation\n",
    "spearman_method2 = spearmanr(df['correctness_method2'], df['correctness_human']).correlation\n",
    "spearman_giskard = spearmanr(df['correctness'], df['correctness_human']).correlation\n",
    "\n",
    "# Compute Spearman coefficients and p-values by question type\n",
    "spearman_results = df.groupby('question_type').apply(lambda x: pd.Series({\n",
    "    'Baseline': spearmanr(x['correctness_default'], x['correctness_human']),\n",
    "    'Prompt 1': spearmanr(x['correctness_method1'], x['correctness_human']),\n",
    "    'Prompt 2': spearmanr(x['correctness_method2'], x['correctness_human']),\n",
    "    'Giskard': spearmanr(x['correctness'], x['correctness_human'])\n",
    "}))\n",
    "\n",
    "# Extract correlations and p-values into separate dataframes\n",
    "spearman_by_question_type = spearman_results.map(lambda x: x.correlation)\n",
    "spearman_pvalues_by_question_type = spearman_results.map(lambda x: x.pvalue)\n",
    "\n",
    "# Add overall Spearman scores\n",
    "spearman_by_question_type.loc['Overall'] = {\n",
    "    'Baseline': spearman_default,\n",
    "    'Prompt 1': spearman_method1,\n",
    "    'Prompt 2': spearman_method2,\n",
    "    'Giskard': spearman_giskard\n",
    "}\n",
    "\n",
    "# Function to calculate overall Spearman scores excluding values based on p-value threshold\n",
    "def calculate_overall_spearman_excluding(df, pvalue_threshold):\n",
    "    filtered_df = df.map(lambda x: x if x.pvalue <= pvalue_threshold else None).dropna(how='any')\n",
    "    overall_spearman = filtered_df.map(lambda x: x.correlation).mean()\n",
    "    return overall_spearman\n",
    "\n",
    "# Calculate overall Spearman scores excluding high p-values\n",
    "spearman_exclude_0_5 = calculate_overall_spearman_excluding(spearman_results, 0.5)\n",
    "spearman_exclude_0_2 = calculate_overall_spearman_excluding(spearman_results, 0.2)\n",
    "spearman_exclude_0_1 = calculate_overall_spearman_excluding(spearman_results, 0.1)\n",
    "spearman_exclude_0_05 = calculate_overall_spearman_excluding(spearman_results, 0.05)\n",
    "\n",
    "# Add these new scores to the dataframe for plotting\n",
    "spearman_by_question_type.loc['Overall Excluding p > 0.5'] = spearman_exclude_0_5\n",
    "spearman_by_question_type.loc['Overall Excluding p > 0.2'] = spearman_exclude_0_2\n",
    "spearman_by_question_type.loc['Overall Excluding p > 0.1'] = spearman_exclude_0_1\n",
    "spearman_by_question_type.loc['Overall Excluding p > 0.05'] = spearman_exclude_0_05\n",
    "\n",
    "# Plotting the overall scores with high p-values excluded\n",
    "fig, ax = plt.subplots(figsize=(14, 14))  # Make the figure square\n",
    "\n",
    "# Prepare the dataframe for overall scores only\n",
    "overall_scores = spearman_by_question_type.loc[['Overall', 'Overall Excluding p > 0.5', 'Overall Excluding p > 0.2', 'Overall Excluding p > 0.1', 'Overall Excluding p > 0.05']]\n",
    "\n",
    "# Update the x labels\n",
    "overall_scores.index = ['Overall', 'Ex. p > 0.5', 'Ex. p > 0.2', 'Ex. p > 0.1', 'Ex. p > 0.05']\n",
    "\n",
    "# Plot overall Spearman's Coefficient\n",
    "overall_scores.plot(kind='bar', ax=ax, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.8)\n",
    "ax.set_xlabel('P-Value Exclusion Criteria', fontsize=40)  # Increase font size by 5\n",
    "ax.set_ylabel(\"Spearman's Coefficient\", fontsize=40)  # Increase font size by 5\n",
    "ax.tick_params(axis='x', labelsize=35)  # Increase font size by 5\n",
    "ax.tick_params(axis='y', labelsize=35)  # Increase font size by 5\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "# Adjust legend properties to be below the plot and span its full width\n",
    "legend = ax.legend(['Baseline', 'Prompt 1', 'Prompt 2', 'Giskard'], fontsize=23.4, title_fontsize=40, frameon=True, loc='upper center', bbox_to_anchor=(0.5, 1.15),  ncol=4, framealpha=1., borderpad=.9, fancybox=True, shadow=False)\n",
    "legend.get_frame().set_linewidth(3)\n",
    "legend.get_frame().set_edgecolor('black')  # Add black border to the legend\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(4)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)  # Set x-labels at 90-degree angle\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract question types from metadata\n",
    "df['question_type'] = df['metadata'].apply(lambda x: eval(x)['question_type'])\n",
    "\n",
    "# Group by question type and compute the mean scores\n",
    "metrics_by_question_type = df.groupby('question_type')[['correctness_default', 'correctness_method1', 'correctness_method2', 'correctness']].mean()\n",
    "\n",
    "# Compute overall mean scores\n",
    "overall_scores = df[['correctness_default', 'correctness_method1', 'correctness_method2', 'correctness']].mean()\n",
    "metrics_by_question_type.loc['Overall'] = overall_scores\n",
    "\n",
    "# Rename columns for plotting\n",
    "metrics_by_question_type.columns = ['Baseline', 'Prompt 1', 'Prompt 2', 'Giskard']\n",
    "\n",
    "# Plotting the scores with enhanced styling\n",
    "fig, ax = plt.subplots(figsize=(20, 14))  # Make the figure wider\n",
    "\n",
    "metrics_by_question_type.plot(kind='bar', ax=ax, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.8)  # Adjust width for larger bars\n",
    "ax.set_title('Evaluation Metrics by Question Type', fontsize=33, weight='bold')\n",
    "ax.set_xlabel('Question Type', fontsize=30)\n",
    "ax.set_ylabel('Evaluation Score', fontsize=30)\n",
    "ax.tick_params(axis='x', labelsize=26)\n",
    "ax.tick_params(axis='y', labelsize=26)\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(10))  # Set smaller y-axis tick intervals\n",
    "ax.legend(['Baseline', 'Prompt 1', 'Prompt 2', 'Giskard'], fontsize=24, title_fontsize=30, frameon=True, edgecolor='black', loc='upper center', bbox_to_anchor=(0.5, -0.27), ncol=4, framealpha=1, borderpad=0.7, fancybox=True, shadow=True, facecolor='white')  # Position the legend below and centered\n",
    "\n",
    "# Setting thicker border around the plot\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Compute F1 scores for each method compared to human evaluation\n",
    "f1_scores = {\n",
    "    'Baseline': f1_score(df['correctness_human'], df['correctness_default']),\n",
    "    'Prompt 1': f1_score(df['correctness_human'], df['correctness_method1']),\n",
    "    'Prompt 2': f1_score(df['correctness_human'], df['correctness_method2']),\n",
    "    'Giskard': f1_score(df['correctness_human'], df['correctness'])\n",
    "}\n",
    "\n",
    "# Compute F1 scores by question type\n",
    "f1_scores_by_question_type = df.groupby('question_type').apply(lambda x: pd.Series({\n",
    "    'Baseline': f1_score(x['correctness_human'], x['correctness_default']),\n",
    "    'Prompt 1': f1_score(x['correctness_human'], x['correctness_method1']),\n",
    "    'Prompt 2': f1_score(x['correctness_human'], x['correctness_method2']),\n",
    "    'Giskard': f1_score(x['correctness_human'], x['correctness'])\n",
    "}))\n",
    "\n",
    "# Add overall F1 scores\n",
    "f1_scores_by_question_type.loc['Overall'] = f1_scores\n",
    "\n",
    "# Plotting the F1 scores with enhanced styling\n",
    "fig, ax = plt.subplots(figsize=(20, 14))  # Make the figure wider\n",
    "\n",
    "f1_scores_by_question_type.plot(kind='bar', ax=ax, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.8)  # Adjust width for larger bars\n",
    "ax.set_title('F1 Scores by Question Type', fontsize=40, weight='bold')\n",
    "ax.set_xlabel('Question Type', fontsize=35)\n",
    "ax.set_ylabel('F1 Score', fontsize=35)\n",
    "ax.tick_params(axis='x', labelsize=30)\n",
    "ax.tick_params(axis='y', labelsize=30)\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(10))  # Set smaller y-axis tick intervals\n",
    "ax.get_legend().remove()\n",
    "# Remove the legend\n",
    "# ax.legend(['Baseline', 'Prompt 1', 'Prompt 2', 'Giskard'], fo?ntsize=24, title_fontsize=30, frameon=True, edgecolor='black', loc='upper center', bbox_to_anchor=(0.5, -0.27), ncol=4, framealpha=1, borderpad=0.7, fancybox=True, shadow=True, facecolor='white')  # Position the legend below and centered\n",
    "\n",
    "# Setting thicker border around the plot\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(4)\n",
    "\n",
    "plt.xticks(rotation=65)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming df is already defined and loaded with the required data\n",
    "# df = pd.read_csv('results_no_rewriter_k3_correctness.csv')  # Example loading data\n",
    "\n",
    "# Compute F1 scores for each method compared to human evaluation\n",
    "f1_scores = {\n",
    "    'Baseline': f1_score(df['correctness_human'], df['correctness_default']),\n",
    "    'Prompt 1': f1_score(df['correctness_human'], df['correctness_method1']),\n",
    "    'Prompt 2': f1_score(df['correctness_human'], df['correctness_method2']),\n",
    "    'Giskard': f1_score(df['correctness_human'], df['correctness'])\n",
    "}\n",
    "\n",
    "# Print the F1 scores\n",
    "for method, score in f1_scores.items():\n",
    "    print(f\"{method}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Recompute confusion matrices for each method compared to human evaluation\n",
    "conf_matrix_default = confusion_matrix(df['correctness_human'], df['correctness_default'])\n",
    "conf_matrix_method1 = confusion_matrix(df['correctness_human'], df['correctness_method1'])\n",
    "conf_matrix_method2 = confusion_matrix(df['correctness_human'], df['correctness_method2'])\n",
    "conf_matrix_giskard = confusion_matrix(df['correctness_human'], df['correctness'])\n",
    "\n",
    "# Plot confusion matrices with enhanced styling and larger fonts\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "\n",
    "sns.heatmap(conf_matrix_default, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0], annot_kws={\"size\": 20})\n",
    "axes[0, 0].set_title('Confusion Matrix: Baseline', fontsize=24)\n",
    "axes[0, 0].set_xlabel('Predicted', fontsize=20)\n",
    "axes[0, 0].set_ylabel('Actual', fontsize=20)\n",
    "axes[0, 0].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "sns.heatmap(conf_matrix_method1, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1], annot_kws={\"size\": 20})\n",
    "axes[0, 1].set_title('Confusion Matrix: Prompt 1', fontsize=24)\n",
    "axes[0, 1].set_xlabel('Predicted', fontsize=20)\n",
    "axes[0, 1].set_ylabel('Actual', fontsize=20)\n",
    "axes[0, 1].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "sns.heatmap(conf_matrix_method2, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0], annot_kws={\"size\": 20})\n",
    "axes[1, 0].set_title('Confusion Matrix: Prompt 2', fontsize=24)\n",
    "axes[1, 0].set_xlabel('Predicted', fontsize=20)\n",
    "axes[1, 0].set_ylabel('Actual', fontsize=20)\n",
    "axes[1, 0].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "sns.heatmap(conf_matrix_giskard, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1], annot_kws={\"size\": 20})\n",
    "axes[1, 1].set_title('Confusion Matrix: Giskard', fontsize=24)\n",
    "axes[1, 1].set_xlabel('Predicted', fontsize=20)\n",
    "axes[1, 1].set_ylabel('Actual', fontsize=20)\n",
    "axes[1, 1].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Compute precision and recall for each method compared to human evaluation\n",
    "precision_default = precision_score(df['correctness_human'], df['correctness_default'])\n",
    "recall_default = recall_score(df['correctness_human'], df['correctness_default'])\n",
    "\n",
    "precision_method1 = precision_score(df['correctness_human'], df['correctness_method1'])\n",
    "recall_method1 = recall_score(df['correctness_human'], df['correctness_method1'])\n",
    "\n",
    "precision_method2 = precision_score(df['correctness_human'], df['correctness_method2'])\n",
    "recall_method2 = recall_score(df['correctness_human'], df['correctness_method2'])\n",
    "\n",
    "precision_giskard = precision_score(df['correctness_human'], df['correctness'])\n",
    "recall_giskard = recall_score(df['correctness_human'], df['correctness'])\n",
    "\n",
    "# Compute precision and recall by question type\n",
    "precision_recall_by_question_type = df.groupby('question_type').apply(lambda x: pd.Series({\n",
    "    'Baseline Precision': precision_score(x['correctness_human'], x['correctness_default']),\n",
    "    'Baseline Recall': recall_score(x['correctness_human'], x['correctness_default']),\n",
    "    'Prompt 1 Precision': precision_score(x['correctness_human'], x['correctness_method1']),\n",
    "    'Prompt 1 Recall': recall_score(x['correctness_human'], x['correctness_method1']),\n",
    "    'Prompt 2 Precision': precision_score(x['correctness_human'], x['correctness_method2']),\n",
    "    'Prompt 2 Recall': recall_score(x['correctness_human'], x['correctness_method2']),\n",
    "    'Giskard Precision': precision_score(x['correctness_human'], x['correctness']),\n",
    "    'Giskard Recall': recall_score(x['correctness_human'], x['correctness'])\n",
    "}))\n",
    "\n",
    "# Add overall precision and recall scores\n",
    "precision_recall_by_question_type.loc['Overall'] = {\n",
    "    'Baseline Precision': precision_default,\n",
    "    'Baseline Recall': recall_default,\n",
    "    'Prompt 1 Precision': precision_method1,\n",
    "    'Prompt 1 Recall': recall_method1,\n",
    "    'Prompt 2 Precision': precision_method2,\n",
    "    'Prompt 2 Recall': recall_method2,\n",
    "    'Giskard Precision': precision_giskard,\n",
    "    'Giskard Recall': recall_giskard\n",
    "}\n",
    "\n",
    "# Plotting Precision and Recall as separate plots\n",
    "\n",
    "# Plotting the precision with enhanced styling\n",
    "fig, ax1 = plt.subplots(figsize=(20, 14))  # Make the figure wider\n",
    "\n",
    "# Plot Precision\n",
    "precision_recall_by_question_type[['Baseline Precision', 'Prompt 1 Precision', 'Prompt 2 Precision', 'Giskard Precision']].plot(kind='bar', ax=ax1, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.8)\n",
    "ax1.set_title(\"Precision by Question Type\", fontsize=40, weight='bold')\n",
    "ax1.set_xlabel('Question Type', fontsize=35)\n",
    "ax1.set_ylabel(\"Precision\", fontsize=35)\n",
    "ax1.tick_params(axis='x', labelsize=30)\n",
    "ax1.tick_params(axis='y', labelsize=30)\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax1.legend(['Baseline', 'Prompt 1', 'Prompt 2', 'Giskard'], fontsize=24, title_fontsize=30, frameon=True, edgecolor='black', loc='upper center', bbox_to_anchor=(0.5, -0.27), ncol=4, framealpha=1, borderpad=0.7, fancybox=True, shadow=True, facecolor='white')\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_linewidth(3)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=65)\n",
    "ax1.get_legend().remove()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the recall with enhanced styling\n",
    "fig, ax2 = plt.subplots(figsize=(20, 14))  # Make the figure wider\n",
    "\n",
    "# Plot Recall\n",
    "precision_recall_by_question_type[['Baseline Recall', 'Prompt 1 Recall', 'Prompt 2 Recall', 'Giskard Recall']].plot(kind='bar', ax=ax2, color=['coral', 'lightblue', 'indianred', 'cornflowerblue'], width=0.8)\n",
    "ax2.set_title(\"Recall by Question Type\", fontsize=40, weight='bold')\n",
    "ax2.set_xlabel('Question Type', fontsize=35)\n",
    "ax2.set_ylabel(\"Recall\", fontsize=35)\n",
    "ax2.tick_params(axis='x', labelsize=30)\n",
    "ax2.tick_params(axis='y', labelsize=30)\n",
    "ax2.yaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "ax2.legend(['Baseline', 'Prompt 1', 'Prompt 2', 'Giskard'], fontsize=24, title_fontsize=30, frameon=True, edgecolor='black', loc='upper center', bbox_to_anchor=(0.5, -0.27), ncol=4, framealpha=1, borderpad=0.7, fancybox=True, shadow=True, facecolor='white')\n",
    "for spine in ax2.spines.values():\n",
    "    spine.set_linewidth(4)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=65)\n",
    "ax2.get_legend().remove()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
