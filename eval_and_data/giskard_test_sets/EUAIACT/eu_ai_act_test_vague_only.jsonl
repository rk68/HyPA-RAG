{"id":"03e11423-cd2b-45f8-8abc-a613992f6512","question":"What considerations might countries have when determining penalties for regulatory breaches?","reference_answer":"Member States should take into account all relevant circumstances of the specific situation, with due regard in particular to the nature, gravity and duration of the infringement and of its consequences and to the size of the provider, in particular if the provider is an SME, including a start-up.","reference_context":"Document 163: EN\nUnited in diversity\nEN\n(168)\nCompliance with this Regulation should be enforceable by means of the imposition of \npenalties and other enforcement measures. Member States should take all necessary \nmeasures to ensure that the provisions of this Regulation are implemented, including by \nlaying down effective, proportionate and dissuasive penalties for their infringement, and to \nrespect the ne bis in idem principle. In order to strengthen and harmonise administrative \npenalties for infringement of this Regulation, the upper limits for setting the \nadministrative fines for certain specific infringements should be laid down. When \nassessing the amount of the fines, Member States should, in each individual case, take \ninto account all relevant circumstances of the specific situation, with due regard in \nparticular to the nature, gravity and duration of the infringement and of its \nconsequences and to the size of the provider, in particular if the provider is an SME, \nincluding a start-up. The European Data Protection Supervisor should have the power to \nimpose fines on Union institutions, agencies and bodies falling within the scope of this \nRegulation.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":163,"topic":"AI Regulation and Compliance"}}
{"id":"c96bd6b6-4017-40a9-bda1-34bc4d684df0","question":"What needs to be secured from individuals before they engage in practical testing?","reference_answer":"Freely-given informed consent shall be obtained from the subjects of testing prior to their participation.","reference_context":"Document 335: EN\nUnited in diversity\nEN\nArticle 61\nInformed consent to participate in testing in real world conditions\n outside AI regulatory sandboxes\n1.\nFor the purpose of testing in real world conditions under Article 60, freely-given \ninformed consent shall be obtained from the subjects of testing prior to their \nparticipation in such testing and after their having been duly informed with concise, \nclear, relevant, and understandable information regarding:\n(a)\nthe nature and objectives of the testing in real world conditions and the possible \ninconvenience that may be linked to their participation;\n(b)\nthe conditions under which the testing in real world conditions is to be conducted, \nincluding the expected duration of the subject or subjects' participation;\n(c)\ntheir rights, and the guarantees regarding their participation, in particular their \nright to refuse to participate in, and the right to withdraw from, testing in real \nworld conditions at any time without any resulting detriment and without having to \nprovide any justification;","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":335,"topic":"AI Testing Regulations"}}
{"id":"6bdf866f-d21e-458f-b356-b3161ec65c2c","question":"How is a certain type of AI model characterized in the given context?","reference_answer":"'General-purpose AI model' means an AI model that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market.","reference_context":"Document 191: EN\nUnited in diversity\nEN\n(62)\n\u2018critical infrastructure\u2019 means critical infrastructure as defined in Article 2, point (4), of \nDirective (EU) 2022\/2557;\n(63)\n\u2018general-purpose AI model\u2019 means an AI model, including where such an AI model is \ntrained with a large amount of data using self-supervision at scale, that displays \nsignificant generality and is capable of competently performing a wide range of distinct \ntasks regardless of the way the model is placed on the market and that can be integrated \ninto a variety of downstream systems or applications, except AI models that are used for \nresearch, development or prototyping activities before they are placed on the market;\n(64)\n\u2018high-impact capabilities\u2019 means capabilities that match or exceed the capabilities \nrecorded in the most advanced general-purpose AI models;\n(65)\n\u2018systemic risk\u2019 means a risk that is specific to the high-impact capabilities of general-\npurpose AI models, having a significant impact on the Union market due to their reach, \nor due to actual or reasonably foreseeable negative effects on public health, safety, \npublic security, fundamental rights, or the society as a whole, that can be propagated at \nscale across the value chain;","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":191,"topic":"General-Purpose AI Models"}}
{"id":"9df78817-b193-452c-95b4-bf2e5bd62b01","question":"What are the obligations for reporting by certain authorities under a specific article of a recent regulation?","reference_answer":"The market surveillance authorities shall report annually to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules, as well as about the use of prohibited practices that occurred during that year and about the measures taken.","reference_context":"Document 363: EN\nUnited in diversity\nEN\n2.\nAs part of their reporting obligations under Article 34(4) of Regulation (EU) 2019\/1020, \nthe market surveillance authorities shall report annually to the Commission and relevant \nnational competition authorities any information identified in the course of market \nsurveillance activities that may be of potential interest for the application of Union law on \ncompetition rules. They shall also annually report to the Commission about the use of \nprohibited practices that occurred during that year and about the measures taken.\n3.\nFor high-risk AI systems related to products covered by the Union harmonisation \nlegislation listed in Section A of Annex I, the market surveillance authority for the \npurposes of this Regulation shall be the authority responsible for market surveillance \nactivities designated under those legal acts.\n By derogation from the first subparagraph, and in appropriate circumstances, Member \nStates may designate another relevant authority to act as a market surveillance \nauthority, provided they ensure coordination with the relevant sectoral market \nsurveillance authorities responsible for the enforcement of the Union harmonisation \nlegislation listed in Annex I.\n4.\nThe procedures referred to in Articles 79 to 83 of this Regulation shall not apply to AI \nsystems related to products covered by the Union harmonisation legislation listed in \nsection A of Annex I, where such legal acts already provide for procedures ensuring an \nequivalent level of protection and having the same objective. In such cases, the relevant \nsectoral procedures shall apply instead.\n\nDocument 150: EN\nUnited in diversity\nEN\n(156)\nIn order to ensure an appropriate and effective enforcement of the requirements and \nobligations set out by this Regulation, which is Union harmonisation legislation, the \nsystem of market surveillance and compliance of products established by Regulation (EU) \n2019\/1020 should apply in its entirety. Market surveillance authorities designated \npursuant to this Regulation should have all enforcement powers laid down in this \nRegulation and in Regulation (EU) 2019\/1020 and should exercise their powers and \ncarry out their duties independently, impartially and without bias. Although the majority \nof AI systems are not subject to specific requirements and obligations under this \nRegulation, market surveillance authorities may take measures in relation to all AI \nsystems when they present a risk in accordance with this Regulation. Due to the specific \nnature of Union institutions, agencies and bodies falling within the scope of this \nRegulation, it is appropriate to designate the European Data Protection Supervisor as a \ncompetent market surveillance authority for them. This should be without prejudice to \nthe designation of national competent authorities by the Member States. Market \nsurveillance activities should not affect the ability of the supervised entities to carry out \ntheir tasks independently, when such independence is required by Union law.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":363,"topic":"AI Regulation and Oversight"}}
{"id":"0e4a9507-1c56-4b51-afd2-c353ab2a23d5","question":"What should each Member State do a few months after the new Regulation takes effect?","reference_answer":"Each Member State shall identify the public authorities or bodies referred to in paragraph 1 and make a list of them publicly available.","reference_context":"Document 371: EN\nUnited in diversity\nEN\n2.\nBy \u2026 [three months after the entry into force of this Regulation], each Member State shall \nidentify the public authorities or bodies referred to in paragraph 1 and make a list of them \npublicly available \u258c . Member States shall notify the list to the Commission and to the \nother Member States, and shall keep the list up to date.\n3.\nWhere the documentation referred to in paragraph 1 is insufficient to ascertain whether an \ninfringement of obligations under Union law protecting fundamental rights has occurred, \nthe public authority or body referred to in paragraph 1 may make a reasoned request to the \nmarket surveillance authority, to organise testing of the high-risk AI system through \ntechnical means. The market surveillance authority shall organise the testing with the close \ninvolvement of the requesting public authority or body within a reasonable time following \nthe request.\n4.\nAny information or documentation obtained by the national public authorities or bodies \nreferred to in paragraph 1 of this Article pursuant to this Article shall be treated in \naccordance with the confidentiality obligations set out in Article 78.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":371,"topic":"AI Regulation and Oversight"}}
{"id":"82aa7f3c-1fc3-4d04-af77-a519da4b5bea","question":"What should certain entities keep according to some financial regulations?","reference_answer":"Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the technical documentation as part of the documentation kept under the relevant Union financial services law.","reference_context":"Document 235: EN\nUnited in diversity\nEN\n2.\nEach Member State shall determine conditions under which the documentation referred \nto in paragraph 1 remains at the disposal of the national competent authorities for the \nperiod indicated in that paragraph for the cases when a provider or its authorised \nrepresentative established on its territory goes bankrupt or ceases its activity prior to the \nend of that period.\n3.\nProviders that are financial institutions subject to requirements regarding their internal \ngovernance, arrangements or processes under Union financial services law shall \nmaintain the technical documentation as part of the documentation kept under the relevant \nUnion financial services law.\n\u258c","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":235,"topic":"AI Regulation and Oversight"}}
{"id":"738f837f-b141-473f-99d7-d6d8e1662281","question":"What does a certain regulation from 2018 pertain to?","reference_answer":"Regulation (EU) 2018\/1139 establishes common rules in the field of civil aviation and establishes a European Union Aviation Safety Agency.","reference_context":"Document 53: 1).\n30\nRegulation (EU) 2018\/1139 of the European Parliament and of the Council of 4 July 2018 \non common rules in the field of civil aviation and establishing a European Union Aviation \nSafety Agency, and amending Regulations (EC) No 2111\/2005, (EC) No 1008\/2008, (EU) \nNo 996\/2010, (EU) No 376\/2014 and Directives 2014\/30\/EU and 2014\/53\/EU of the \nEuropean Parliament and of the Council, and repealing Regulations (EC) No 552\/2004 and \n(EC) No 216\/2008 of the European Parliament and of the Council and Council Regulation \n(EEC) No 3922\/91 (OJ L 212, 22.8.2018, p. 1).\n31\nRegulation (EU) 2019\/2144 of the European Parliament and of the Council of 27 November \n2019 on type-approval requirements for motor vehicles and their trailers, and systems, \ncomponents and separate technical units intended for such vehicles, as regards their general \nsafety and the protection of vehicle occupants and vulnerable road users, amending \nRegulation (EU) 2018\/858 of the European Parliament and of the Council and repealing \nRegulations (EC) No 78\/2009, (EC) No 79\/2009 and (EC) No 661\/2009 of the European \nParliament and of the Council and Commission Regulations (EC) No 631\/2009, (EU) No \n406\/2010, (EU) No 672\/2010, (EU) No 1003\/2010, (EU) No 1005\/2010, (EU) No \n1008\/2010, (EU) No 1009\/2010, (EU) No 19\/2011, (EU) No 109\/2011, (EU) No 458\/2011, \n(EU) No 65\/2012, (EU) No 130\/2012, (EU) No 347\/2012, (EU) No 351\/2012, (EU) No \n1230\/2012 and (EU) 2015\/166 (OJ L 325, 16.12.2019, p.\n\nDocument 52: 146).\n28\nDirective (EU) 2016\/797 of the European Parliament and of the Council of 11 May 2016 on \nthe interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. \n44).\n29\nRegulation (EU) 2018\/858 of the European Parliament and of the Council of 30 May 2018 \non the approval and market surveillance of motor vehicles and their trailers, and of systems, \ncomponents and separate technical units intended for such vehicles, amending Regulations \n(EC) No 715\/2007 and (EC) No 595\/2009 and repealing Directive 2007\/46\/EC (OJ L 151, \n14.6.2018, p. 1).\n30\nRegulation (EU) 2018\/1139 of the European Parliament and of the Council of 4 July 2018 \non common rules in the field of civil aviation and establishing a European Union Aviation \nSafety Agency, and amending Regulations (EC) No 2111\/2005, (EC) No 1008\/2008, (EU) \nNo 996\/2010, (EU) No 376\/2014 and Directives 2014\/30\/EU and 2014\/53\/EU of the \nEuropean Parliament and of the Council, and repealing Regulations (EC) No 552\/2004 and \n(EC) No 216\/2008 of the European Parliament and of the Council and Council Regulation \n(EEC) No 3922\/91 (OJ L 212, 22.8.2018, p. 1).\n\nDocument 434: 1);\n20.\nRegulation (EU) 2018\/1139 of the European Parliament and of the Council of 4 July 2018 \non common rules in the field of civil aviation and establishing a European Union Aviation \nSafety Agency, and amending Regulations (EC) No 2111\/2005, (EC) No 1008\/2008, (EU) \nNo 996\/2010, (EU) No 376\/2014 and Directives 2014\/30\/EU and 2014\/53\/EU of the \nEuropean Parliament and of the Council, and repealing Regulations (EC) No 552\/2004 and \n(EC) No 216\/2008 of the European Parliament and of the Council and Council Regulation \n(EEC) No 3922\/91 (OJ L 212, 22.8.2018, p. 1), in so far as the design, production and \nplacing on the market of aircrafts referred to in Article 2(1), points (a) and (b) thereof, \nwhere it concerns unmanned aircraft and their engines, propellers, parts and equipment to \ncontrol them remotely, are concerned.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":53,"topic":"EU Regulations and Directives"}}
{"id":"34177717-cd1b-43bf-80e8-31a4a49b791c","question":"How is a system that identifies individuals from afar categorized?","reference_answer":"'Remote biometric identification system' means an AI system for the purpose of identifying natural persons, without their active involvement, typically at a distance through the comparison of a person\u2019s biometric data with the biometric data contained in a reference database.","reference_context":"Document 185: EN\nUnited in diversity\nEN\n(39)\n\u2018emotion recognition system\u2019 means an AI system for the purpose of identifying or \ninferring emotions or intentions of natural persons on the basis of their biometric data;\n(40)\n\u2018biometric categorisation system\u2019 means an AI system for the purpose of assigning natural \npersons to specific categories on the basis of their biometric data, unless it is ancillary to \nanother commercial service and strictly necessary for objective technical reasons;\n(41)\n\u2018remote biometric identification system\u2019 means an AI system for the purpose of identifying \nnatural persons, without their active involvement, typically at a distance through the \ncomparison of a person\u2019s biometric data with the biometric data contained in a reference \ndatabase \u258c ;\n(42)\n\u2018real-time remote biometric identification system\u2019 means a remote biometric identification \nsystem, whereby the capturing of biometric data, the comparison and the identification all \noccur without a significant delay, comprising not only instant identification, but also \nlimited short delays in order to avoid circumvention;\n(43)\n\u2018post remote biometric identification system\u2019 means a remote biometric identification \nsystem other than a real-time remote biometric identification system;","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":185,"topic":"Biometric Data and AI Regulation"}}
{"id":"15f8d575-4acb-4f61-9f71-44934116660f","question":"What is the aim of the document from 2008 regarding consumer agreements?","reference_answer":"Directive 2008\/48\/EC is on credit agreements for consumers and repeals Council Directive 87\/102\/EEC.","reference_context":"Document 153: 1).\n47\nDirective 2008\/48\/EC of the European Parliament and of the Council of 23 April 2008 on \ncredit agreements for consumers and repealing Council Directive 87\/102\/EEC (OJ L 133, \n22.5.2008, p. 66).\n48\nDirective 2009\/138\/EC of the European Parliament and of the Council of 25 November \n2009 on the taking-up and pursuit of the business of Insurance and Reinsurance (Solvency \nII) (OJ L 335, 17.12.2009, p. 1).\n49\nDirective 2013\/36\/EU of the European Parliament and of the Council of 26 June 2013 on \naccess to the activity of credit institutions and the prudential supervision of credit \ninstitutions and investment firms, amending Directive 2002\/87\/EC and repealing Directives \n2006\/48\/EC and 2006\/49\/EC (OJ L 176, 27.6.2013, p. 338).\n50\nDirective 2014\/17\/EU of the European Parliament and of the Council of 4 February 2014 on \ncredit agreements for consumers relating to residential immovable property and amending \nDirectives 2008\/48\/EC and 2013\/36\/EU and Regulation (EU) No 1093\/2010 (OJ L 60, \n28.2.2014, p. 34).\n51\nDirective (EU) 2016\/97 of the European Parliament and of the Council of 20 January 2016 \non insurance distribution (OJ L 26, 2.2.2016, p. 19).","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":153,"topic":"EU Regulations and Directives"}}
{"id":"6f14d376-d6a1-4425-8655-064c2156604c","question":"What is the aim of the system introduced by a certain regulation regarding entry and exit data?","reference_answer":"The Entry\/Exit System (EES) is established to register entry and exit data and refusal of entry data of third-country nationals crossing the external borders of the Member States.","reference_context":"Document 466: EN\nUnited in diversity\nEN\n4.\nEntry\/Exit System\nRegulation (EU) 2017\/2226 of the European Parliament and of the Council of \n30 November 2017 establishing an Entry\/Exit System (EES) to register entry and \nexit data and refusal of entry data of third-country nationals crossing the external \nborders of the Member States and determining the conditions for access to the EES \nfor law enforcement purposes, and amending the Convention implementing the \nSchengen Agreement and Regulations (EC) No 767\/2008 and (EU) No 1077\/2011 \n(OJ L 327, 9.12.2017, p. 20).\n5.\nEuropean Travel Information and Authorisation System\n(a)\nRegulation (EU) 2018\/1240 of the European Parliament and of the Council of \n12 September 2018 establishing a European Travel Information and Authorisation \nSystem (ETIAS) and amending Regulations (EU) No 1077\/2011, (EU) No 515\/2014, \n(EU) 2016\/399, (EU) 2016\/1624 and (EU) 2017\/2226 (OJ L 236, 19.9.2018, p. 1).\n(b)\nRegulation (EU) 2018\/1241 of the European Parliament and of the Council of \n12 September 2018 amending Regulation (EU) 2016\/794 for the purpose of \nestablishing a European Travel Information and Authorisation System (ETIAS) \n(OJ L 236, 19.9.2018, p. 72).","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":466,"topic":"EU Regulations and Directives"}}
{"id":"80261449-af3e-4dff-b3b7-535e61e86a12","question":"How is a 'space available to the public' characterized in the guidelines?","reference_answer":"A publicly accessible space is any physical space that is accessible to an undetermined number of natural persons, regardless of whether it is privately or publicly owned, and includes various types of locations such as shops, restaurants, banks, transport stations, and parks. Access may be subject to certain predetermined conditions, but it is not considered publicly accessible if it is limited to specific individuals for public safety or security reasons.","reference_context":"Document 19: EN\nUnited in diversity\nEN\n(19)\nFor the purposes of this Regulation the notion of \u2018publicly accessible space\u2019 should be \nunderstood as referring to any physical space that is accessible to an undetermined \nnumber of natural persons, and irrespective of whether the space in question is privately \nor publicly owned, irrespective of the activity for which the space may be used, such as \nfor commerce, for example, shops, restaurants, caf\u00e9s; for services, for example, banks, \nprofessional activities, hospitality; for sport, for example, swimming pools, gyms, \nstadiums; for transport, for example, bus, metro and railway stations, airports, means of \ntransport; for entertainment, for example, cinemas, theatres, museums, concert and \nconference halls; or for leisure or otherwise, for example, public roads and squares, \nparks, forests, playgrounds. A space should also be classified as being publicly accessible \nif, regardless of potential capacity or security restrictions, access is subject to certain \npredetermined conditions which can be fulfilled by an undetermined number of persons, \nsuch as the purchase of a ticket or title of transport, prior registration or having a certain \nage. In contrast, a space should not be considered to be publicly accessible if access is \nlimited to specific and defined natural persons through either Union or national law \ndirectly related to public safety or security or through the clear manifestation of will by \nthe person having the relevant authority over the space. The factual possibility of access \nalone, such as an unlocked door or an open gate in a fence, does not imply that the space \nis publicly accessible in the presence of indications or circumstances suggesting the \ncontrary, such as. signs prohibiting or restricting access. Company and factory premises, \nas well as offices and workplaces that are intended to be accessed only by relevant \nemployees and service providers, are spaces that are not publicly accessible. Publicly \naccessible spaces should not include prisons or border control. Some other spaces may \ncomprise both publicly accessible and non-publicly accessible spaces, such as the \nhallway of a private residential building necessary to access a doctor's office or an \nairport. Online spaces are not covered, as they are not physical spaces.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":19,"topic":"Biometric Data and AI Regulation"}}
{"id":"a30256ec-30ca-45f7-a896-d742346390a4","question":"When do those managing certain AI systems for public entities need to meet the new rules?","reference_answer":"Operators of high-risk AI systems that are intended to be used by public authorities should take the necessary steps to comply with the requirements of this Regulation by the end of 2030.","reference_context":"Document 169: EN\nUnited in diversity\nEN\n(177)\nIn order to ensure legal certainty, ensure an appropriate adaptation period for operators \nand avoid disruption to the market, including by ensuring continuity of the use of AI \nsystems, it is appropriate that this Regulation applies to the high-risk AI systems that \nhave been placed on the market or put into service before the general date of application \nthereof, only if, from that date, those systems are subject to significant changes in their \ndesign or intended purpose. It is appropriate to clarify that, in this respect, the concept of \nsignificant change should be understood as equivalent in substance to the notion of \nsubstantial modification, which is used with regard only to high-risk AI systems \npursuant to this Regulation. On an exceptional basis and in light of public \naccountability, operators of AI systems which are components of the large-scale IT \nsystems established by the legal acts listed in an annex to this Regulation and operators \nof high-risk AI systems that are intended to be used by public authorities should, \nrespectively, take the necessary steps to comply with the requirements of this Regulation \nby end of 2030 and by ... [ six years from the date of entry into force of this Regulation].\n(178)\nProviders of high-risk AI systems are encouraged to start to comply, on a voluntary \nbasis, with the relevant obligations of this Regulation already during the transitional \nperiod.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":169,"topic":"High-Risk AI Regulation"}}
{"id":"5e372307-a110-4e57-afa7-ccfc47b7da8b","question":"What does the document indicate about consumer entitlements related to AI?","reference_answer":"All rights and remedies provided for by Union law to consumers, and other persons on whom AI systems may have a negative impact, remain unaffected and fully applicable.","reference_context":"Document 8: EN\nUnited in diversity\nEN\nAs a consequence, all rights and remedies provided for by such Union law to consumers, \nand other persons on whom AI systems may have a negative impact, including as \nregards the compensation of possible damages pursuant to Council Directive \n85\/374\/EEC10 remain unaffected and fully applicable. Furthermore, in the context of \nemployment and protection of workers, this Regulation should therefore not affect \nUnion law on social policy and national labour law, in compliance with Union law, \nconcerning employment and working conditions, including health and safety at work \nand the relationship between employers and workers. This Regulation should also not \naffect the exercise of fundamental rights as recognised in the Member States and at \nUnion level, including the right or freedom to strike or to take other action covered by \nthe specific industrial relations systems in Member States as well as the right to \nnegotiate, to conclude and enforce collective agreements or to take collective action in \naccordance with national law. \n10\nCouncil Directive 85\/374\/EEC of 25 July 1985 on the approximation of the laws, \nregulations and administrative provisions of the Member States concerning liability for \ndefective products (OJ L 210, 7.8.1985, p. 29).","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":8,"topic":"AI Regulation and Oversight"}}
{"id":"f419d544-f5d3-46f8-89fd-75ca63c7ac3a","question":"In what scenarios might this Regulation not be relevant to certain foreign authorities and global entities?","reference_answer":"This Regulation should not apply to public authorities of a third country and international organizations when acting in the framework of cooperation or international agreements concluded at Union or national level for law enforcement and judicial cooperation with the Union or the Member States, provided that the relevant third country or international organization provides adequate safeguards with respect to the protection of fundamental rights and freedoms of individuals.","reference_context":"Document 23: EN\nUnited in diversity\nEN\nNonetheless, to take into account existing arrangements and special needs for future \ncooperation with foreign partners with whom information and evidence is exchanged, this \nRegulation should not apply to public authorities of a third country and international \norganisations when acting in the framework of cooperation or international agreements \nconcluded at Union or national level for law enforcement and judicial cooperation with the \nUnion or the Member States, provided that the relevant third country or international \norganisation provides adequate safeguards with respect to the protection of fundamental \nrights and freedoms of individuals. Where relevant, this may cover activities of entities \nentrusted by the third countries to carry out specific tasks in support of such law \nenforcement and judicial cooperation. Such framework for cooperation or agreements \nhave been established bilaterally between Member States and third countries or between \nthe European Union, Europol and other Union agencies and third countries and \ninternational organisations. The authorities competent for supervision of the law \nenforcement and judicial authorities under this Regulation should assess whether those \nframeworks for cooperation or international agreements include adequate safeguards \nwith respect to the protection of fundamental rights and freedoms of individuals. \nRecipient national authorities and Union institutions, bodies, offices and agencies \nmaking use of such outputs in the Union remain accountable to ensure their use \ncomplies with Union law. When those international agreements are revised or new ones \nare concluded in the future, the contracting parties should make utmost efforts to align \nthose agreements with the requirements of this Regulation.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":23,"topic":"AI Regulation in the EU"}}
{"id":"91e3a5f9-67f4-4295-bb58-4dc910ec686a","question":"Which groups are noted for aiding in the enactment of the Regulation?","reference_answer":"The AI-on-demand platform, the European Digital Innovation Hubs, and the testing and experimentation facilities established by the Commission and the Member States are mentioned as contributing to the implementation of this Regulation.","reference_context":"Document 143: EN\nUnited in diversity\nEN\n(144)\nIn order to promote and protect innovation, the AI-on-demand platform, all relevant \nUnion funding programmes and projects, such as Digital Europe Programme, Horizon \nEurope, implemented by the Commission and the Member States at Union or national \nlevel should, as appropriate, contribute to the achievement of the objectives of this \nRegulation.\n(145)\nIn order to minimise the risks to implementation resulting from lack of knowledge and \nexpertise in the market as well as to facilitate compliance of providers, in particular \nSMEs, including start-ups, and notified bodies with their obligations under this \nRegulation, the AI-on-demand platform, the European Digital Innovation Hubs and the \ntesting and experimentation facilities established by the Commission and the Member \nStates at Union or national level should contribute to the implementation of this \nRegulation. Within their respective mission and fields of competence, the AI-on-demand \nplatform, the European Digital Innovation Hubs and the testing and experimentation \nFacilities are able to provide in particular technical and scientific support to providers and \nnotified bodies.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":143,"topic":"Others"}}
{"id":"9c487ca3-3bf5-439f-9512-7c0a629cfc1d","question":"What elements are part of the guidelines for certain AI systems considered to be risky?","reference_answer":"Technical specifications include standards to be applied and, where the relevant harmonised standards are not applied in full or do not cover all of the relevant requirements, the means to ensure compliance with those requirements.","reference_context":"Document 232: EN\nUnited in diversity\nEN\n(e)\ntechnical specifications, including standards, to be applied and, where the relevant \nharmonised standards are not applied in full or do not cover all of the relevant \nrequirements set out in Section 2, the means to be used to ensure that the high-risk \nAI system complies with those requirements \u258c ;\n(f)\nsystems and procedures for data management, including data acquisition, data \ncollection, data analysis, data labelling, data storage, data filtration, data mining, data \naggregation, data retention and any other operation regarding the data that is \nperformed before and for the purpose of the placing on the market or the putting into \nservice of high-risk AI systems;\n(g)\nthe risk management system referred to in Article 9;\n(h)\nthe setting-up, implementation and maintenance of a post-market monitoring system, \nin accordance with Article 72;\n(i)\nprocedures related to the reporting of a serious incident in accordance with \nArticle 73;","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":232,"topic":"Others"}}
{"id":"7154a4e4-2cea-4746-958a-92295c3b6b6d","question":"What actions might someone take if they suspect a breach of certain rules?","reference_answer":"Any natural or legal person that has grounds to consider that there has been an infringement of this Regulation should be entitled to lodge a complaint to the relevant market surveillance authority.","reference_context":"Document 164: EN\nUnited in diversity\nEN\n(169)\nCompliance with the obligations on providers of general-purpose AI models imposed \nunder this Regulation should be enforceable, inter alia, by means of fines. To that end, \nappropriate levels of fines should also be laid down for infringement of those \nobligations, including the failure to comply with measures requested by the Commission \nin accordance with this Regulation, subject to appropriate limitation periods in \naccordance with the principle of proportionality. All decisions taken by the Commission \nunder this Regulation are subject to review by the Court of Justice of the European \nUnion in accordance with the TFEU, including the unlimited jurisdiction of the Court of \nJustice with regard to penalties pursuant to Article 261 TFEU.\n(170)\nUnion and national law already provide effective remedies to natural and legal persons \nwhose rights and freedoms are adversely affected by the use of AI systems. Without \nprejudice to those remedies, any natural or legal person that has grounds to consider \nthat there has been an infringement of this Regulation should be entitled to lodge a \ncomplaint to the relevant market surveillance authority.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":164,"topic":"AI Regulation and Compliance"}}
{"id":"7bf41227-7508-48ce-9982-89705d84361e","question":"What capabilities do national bodies possess concerning AI systems deemed high-risk?","reference_answer":"National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights have the power to request and access any documentation created or maintained under this Regulation in accessible language and format when access to that documentation is necessary for effectively fulfilling their mandates within the limits of their jurisdiction.","reference_context":"Document 370: EN\nUnited in diversity\nEN\n(b)\nto require the provider or prospective provider and the deployer or prospective \ndeployer to modify any aspect of the testing in real world conditions.\n4.\nWhere a market surveillance authority has taken a decision referred to in paragraph 3 of \nthis Article, or has issued an objection within the meaning of Article 60(4), point (b), the \ndecision or the objection shall indicate the grounds therefor and how the provider or \nprospective provider can challenge the decision or objection.\n5.\nWhere applicable, where a market surveillance authority has taken a decision referred to \nin paragraph 3, it shall communicate the grounds therefor to the market surveillance \nauthorities of other Member States in which the AI system has been tested in accordance \nwith the testing plan.\nArticle 77\nPowers of authorities protecting fundamental rights\n1.\nNational public authorities or bodies which supervise or enforce the respect of obligations \nunder Union law protecting fundamental rights, including the right to non-discrimination, \nin relation to the use of high-risk AI systems referred to in Annex III shall have the power \nto request and access any documentation created or maintained under this Regulation in \naccessible language and format when access to that documentation is necessary for \neffectively fulfilling their mandates within the limits of their jurisdiction. The relevant \npublic authority or body shall inform the market surveillance authority of the Member \nState concerned of any such request.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":370,"topic":"Others"}}
{"id":"6346eea0-d4b9-491f-9bb0-8e8b695aa139","question":"What might happen with AI tools in policing based on the provided information?","reference_answer":"The use of AI systems in law enforcement can lead to a significant degree of power imbalance and may result in surveillance, arrest, or deprivation of liberty, as well as adverse impacts on fundamental rights. If the AI system is not trained with high-quality data or is not properly designed and tested, it may discriminate against individuals and hamper important procedural rights like the right to an effective remedy and a fair trial.","reference_context":"Document 67: EN\nUnited in diversity\nEN\n(59)\nGiven their role and responsibility, actions by law enforcement authorities involving \ncertain uses of AI systems are characterised by a significant degree of power imbalance \nand may lead to surveillance, arrest or deprivation of a natural person\u2019s liberty as well as \nother adverse impacts on fundamental rights guaranteed in the Charter. In particular, if the \nAI system is not trained with high-quality data, does not meet adequate requirements in \nterms of its performance, its accuracy or robustness, or is not properly designed and tested \nbefore being put on the market or otherwise put into service, it may single out people in a \ndiscriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of \nimportant procedural fundamental rights, such as the right to an effective remedy and to a \nfair trial as well as the right of defence and the presumption of innocence, could be \nhampered, in particular, where such AI systems are not sufficiently transparent, \nexplainable and documented. It is therefore appropriate to classify as high-risk, insofar as \ntheir use is permitted under relevant Union and national law, a number of AI systems \nintended to be used in the law enforcement context where accuracy, reliability and \ntransparency is particularly important to avoid adverse impacts, retain public trust and \nensure accountability and effective redress.\n\nDocument 68: EN\nUnited in diversity\nEN\nIn view of the nature of the activities and the risks relating thereto, those high-risk AI \nsystems should include in particular AI systems intended to be used by or on behalf of law \nenforcement authorities or by Union institutions, bodies, offices, or agencies in support of \nlaw enforcement authorities for assessing the risk of a natural person to become a victim \nof criminal offences, as polygraphs and similar tools, for the evaluation of the reliability \nof evidence in in the course of investigation or prosecution of criminal offences, and, \ninsofar as not prohibited under this Regulation, for assessing the risk of a natural \nperson offending or reoffending not solely on the basis of the profiling of natural persons \nor the assessment of personality traits and characteristics or the past criminal behaviour of \nnatural persons or groups, for profiling in the course of detection, investigation or \nprosecution of criminal offences \u258c . AI systems specifically intended to be used for \nadministrative proceedings by tax and customs authorities as well as by financial \nintelligence units carrying out administrative tasks analysing information pursuant to \nUnion anti-money laundering law should not be classified as high-risk AI systems used \nby law enforcement authorities for the purpose of prevention, detection, investigation and \nprosecution of criminal offences. The use of AI tools by law enforcement and other \nrelevant authorities should not become a factor of inequality, or exclusion. The impact \nof the use of AI tools on the defence rights of suspects should not be ignored, in \nparticular the difficulty in obtaining meaningful information on the functioning of those \nsystems and the resulting difficulty in challenging their results in court, in particular by \nnatural persons under investigation.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":67,"topic":"AI Regulation and Ethics"}}
{"id":"f37d4b52-9dab-4d48-9471-4206033f7f5f","question":"Which entity is accountable for introducing a high-risk AI system to the market?","reference_answer":"A specific natural or legal person, defined as the provider, takes responsibility for the placing on the market or the putting into service of a high-risk AI system.","reference_context":"Document 87: EN\nUnited in diversity\nEN\n(79)\nIt is appropriate that a specific natural or legal person, defined as the provider, takes \nresponsibility for the placing on the market or the putting into service of a high-risk AI \nsystem, regardless of whether that natural or legal person is the person who designed or \ndeveloped the system.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":87,"topic":"AI Regulation and Ethics"}}
{"id":"51edc28c-7748-4485-aea1-3c199f838650","question":"How is a broadly applicable AI model characterized in the regulations?","reference_answer":"A general-purpose AI model is defined by its key functional characteristics, particularly its generality and capability to competently perform a wide range of distinct tasks, typically trained on large amounts of data through methods such as self-supervised, unsupervised, or reinforcement learning.","reference_context":"Document 101: EN\nUnited in diversity\nEN\n(97)\nThe notion of general-purpose AI models should be clearly defined and set apart from \nthe notion of AI systems to enable legal certainty. The definition should be based on the \nkey functional characteristics of a general-purpose AI model, in particular the generality \nand the capability to competently perform a wide range of distinct tasks. These models \nare typically trained on large amounts of data, through various methods, such as self-\nsupervised, unsupervised or reinforcement learning. General-purpose AI models may be \nplaced on the market in various ways, including through libraries, application \nprogramming interfaces (APIs), as direct download, or as physical copy. These models \nmay be further modified or fine-tuned into new models. Although AI models are \nessential components of AI systems, they do not constitute AI systems on their own. AI \nmodels require the addition of further components, such as for example a user interface, \nto become AI systems. AI models are typically integrated into and form part of AI \nsystems. This Regulation provides specific rules for general-purpose AI models and for \ngeneral-purpose AI models that pose systemic risks, which should apply also when these \nmodels are integrated or form part of an AI system. It should be understood that the \nobligations for the providers of general-purpose AI models should apply once the \ngeneral-purpose AI models are placed on the market.\n\nDocument 102: EN\nUnited in diversity\nEN\nWhen the provider of a general-purpose AI model integrates an own model into its own \nAI system that is made available on the market or put into service, that model should be \nconsidered to be placed on the market and, therefore, the obligations in this Regulation \nfor models should continue to apply in addition to those for AI systems. The obligations \nlaid down for models should in any case not apply when an own model is used for purely \ninternal processes that are not essential for providing a product or a service to third \nparties and the rights of natural persons are not affected. Considering their potential \nsignificantly negative effects, the general-purpose AI models with systemic risk should \nalways be subject to the relevant obligations under this Regulation. The definition \nshould not cover AI models used before their placing on the market for the sole purpose \nof research, development and prototyping activities. This is without prejudice to the \nobligation to comply with this Regulation when, following such activities, a model is \nplaced on the market.\n(98)\nWhereas the generality of a model could, inter alia, also be determined by a number of \nparameters, models with at least a billion of parameters and trained with a large amount \nof data using self-supervision at scale should be considered to display significant \ngenerality and to competently perform a wide range of distinctive tasks.\n(99)\nLarge generative AI models are a typical example for a general-purpose AI model, given \nthat they allow for flexible generation of content, such as in the form of text, audio, \nimages or video, that can readily accommodate a wide range of distinctive tasks.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":101,"topic":"Others"}}
{"id":"aa45b01d-b248-44d0-b468-ce599a922593","question":"What elements might increase the risk of certain individuals being taken advantage of by AI?","reference_answer":"Factors that may make individuals more vulnerable to exploitation by AI systems include their age, disability, or a specific social or economic situation, such as living in extreme poverty or being part of ethnic or religious minorities.","reference_context":"Document 30: This could be facilitated, for example, by machine-brain \ninterfaces or virtual reality as they allow for a higher degree of control of what stimuli \nare presented to persons, insofar as they may materially distort their behaviour in a \nsignificantly harmful manner. In addition, AI systems may also otherwise exploit the \nvulnerabilities of a person or a specific group of persons due to their age, disability within \nthe meaning of Directive (EU) 2019\/882 of the European Parliament and of the \nCouncil16, or a specific social or economic situation that is likely to make those persons \nmore vulnerable to exploitation such as persons living in extreme poverty, ethnic or \nreligious minorities. \n16\nDirective (EU) 2019\/882 of the European Parliament and of the Council of 17 April 2019 on \nthe accessibility requirements for products and services (OJ L 151, 7.6.2019, p. 70).","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":30,"topic":"AI Regulation and Ethics"}}
{"id":"44c9a05d-ec4f-4da4-9c10-5d547758675d","question":"What are the purposes of AI systems concerning urgent services?","reference_answer":"AI systems intended to evaluate and classify emergency calls by natural persons or to be used to dispatch, or to establish priority in the dispatching of, emergency first response services, including by police, firefighters and medical aid, as well as of emergency healthcare patient triage systems.","reference_context":"Document 440: EN\nUnited in diversity\nEN\n(c)\nAI systems intended to be used for risk assessment and pricing in relation to \nnatural persons in the case of life and health insurance;\n(d)\nAI systems intended to evaluate and classify emergency calls by natural persons or \nto be used to dispatch, or to establish priority in the dispatching of, emergency first \nresponse services, including by police, firefighters and medical aid, as well as of \nemergency healthcare patient triage systems.\n6.\nLaw enforcement, in so far as their use is permitted under relevant Union or national \nlaw:\n(a)\nAI systems intended to be used by or on behalf of law enforcement authorities, or by \nUnion institutions, bodies, offices or agencies in support of law enforcement \nauthorities or on their behalf to assess the risk of a natural person becoming the \nvictim of criminal offences;\n(b)\nAI systems intended to be used by or on behalf of law enforcement authorities or by \nUnion institutions, bodies, offices or agencies in support of law enforcement \nauthorities as polygraphs or similar tools;\n\u258c","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":440,"topic":"Others"}}
{"id":"a72f2eaf-abd5-456e-8491-9c0bd732e4f3","question":"What are the responsibilities of certain AI model providers related to broader risks as mentioned in a specific article?","reference_answer":"Providers of general-purpose AI models with systemic risk shall perform model evaluation in accordance with standardised protocols and tools, including conducting and documenting adversarial testing of the model, and assess and mitigate possible systemic risks at Union level.","reference_context":"Document 307: EN\nUnited in diversity\nEN\nSection 3\nObligations of providers of general-purpose AI models\n with systemic risk\nArticle 55\nObligations of providers of general-purpose AI models with systemic risk\n1.\nIn addition to the obligations listed in Articles 53 and 54, providers of general-purpose \nAI models with systemic risk shall:\n(a)\nperform model evaluation in accordance with standardised protocols and tools \nreflecting the state of the art, including conducting and documenting adversarial \ntesting of the model with a view to identifying and mitigating systemic risks;\n(b)\nassess and mitigate possible systemic risks at Union level, including their sources, \nthat may stem from the development, the placing on the market, or the use of \ngeneral-purpose AI models with systemic risk;","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":307,"topic":"General-Purpose AI Models"}}
{"id":"86cd5dc1-8a32-4841-a3f3-39676cf5eba2","question":"What details are required from providers of certain AI systems based on a specific article?","reference_answer":"The following information shall be provided: the name, address and contact details of the provider; the name, address and contact details of another person submitting information on behalf of the provider; the name, address and contact details of the authorised representative; the AI system trade name and any additional reference for identification; a description of the intended purpose of the AI system; the conditions under Article 6(3) for not-high-risk classification; a summary of the grounds for not-high-risk classification; the status of the AI system; and any Member States where the AI system has been placed on the market.","reference_context":"Document 460: EN\nUnited in diversity\nEN\nSection B - Information to be submitted by providers of high-risk AI systems in accordance with \nArticle 49(2)\nThe following information shall be provided and thereafter kept up to date with regard to AI \nsystems to be registered in accordance with Article 49(2):\n1.\nThe name, address and contact details of the provider;\n2.\nWhere submission of information is carried out by another person on behalf of the \nprovider, the name, address and contact details of that person;\n3.\nThe name, address and contact details of the authorised representative, where \napplicable;\n4.\nThe AI system trade name and any additional unambiguous reference allowing the \nidentification and traceability of the AI system;\n5.\nA description of the intended purpose of the AI system;\n6.\nThe condition or conditions under Article 6(3)based on which the AI system is \nconsidered to be not-high-risk;\n7.\nA short summary of the grounds on which the AI system is considered to be not-high-\nrisk in application of the procedure under Article 6(3);\n8.\nThe status of the AI system (on the market, or in service; no longer placed on the \nmarket\/in service, recalled);\n9.\nAny Member States in which the AI system has been placed on the market, put into \nservice or made available in the Union.\n\nDocument 458: EN\nUnited in diversity\nEN\nANNEX VIII\nInformation to be submitted upon the registration \nof high-risk AI systems in accordance with Article 49\nSection A - Information to be submitted by providers of high-risk AI systems in accordance with \nArticle 49(1)\nThe following information shall be provided and thereafter kept up to date with regard to high-risk \nAI systems to be registered in accordance with Article 49(1):\n1.\nThe name, address and contact details of the provider;\n2.\nWhere submission of information is carried out by another person on behalf of the \nprovider, the name, address and contact details of that person;\n3.\nThe name, address and contact details of the authorised representative, where applicable;\n4.\nThe AI system trade name and any additional unambiguous reference allowing the \nidentification and traceability of the AI system;\n5.\nA description of the intended purpose of the AI system and of the components and \nfunctions supported through this AI system;\n6.\nA basic and concise description of the information used by the system (data, inputs) and \nits operating logic;\n\nDocument 461: EN\nUnited in diversity\nEN\nSection C - Information to be submitted by deployers of high-risk AI systems in accordance with \nArticle 49(3)\nThe following information shall be provided and thereafter kept up to date with regard to high-\nrisk AI systems to be registered in accordance with Article 49:\n1.\nThe name, address and contact details of the deployer;\n2.\nThe name, address and contact details of the person submitting information on behalf of \nthe deployer;\n3.\nThe URL of the entry of the AI system in the EU database by its provider;\n4.\nA summary of the findings of the fundamental rights impact assessment conducted in \naccordance with Article 27;\n5.\nA summary of the data protection impact assessment carried out in accordance with \nArticle 35 of Regulation (EU) 2016\/679 or Article 27 of Directive (EU) 2016\/680 as \nspecified in Article 26(8) of this Regulation, where applicable.","conversation_history":[],"metadata":{"question_type":"vague","seed_document_id":460,"topic":"High-Risk AI Regulations"}}
