{"id": "0040d294-9a22-4de2-a96b-46a5197d3c18", "question": "What specific information must national market surveillance authorities and national data protection authorities submit to the Commission, including any annual reporting requirements, regarding the deployment of 'real-time' remote biometric identification systems for law enforcement in publicly accessible spaces?", "reference_answer": "They must submit annual reports on such use, including information on the number of decisions taken by competent judicial authorities or an independent administrative authority whose decision is binding upon requests for authorisations and their result.", "reference_context": "Document 200: EN\nUnited in diversity\nEN\n4.\nWithout prejudice to paragraph 3, each use of a \u2018real-time\u2019 remote biometric \nidentification system in publicly accessible spaces for law enforcement purposes shall be \nnotified to the relevant market surveillance authority and the national data protection \nauthority in accordance with the national rules referred to in paragraph 5. The \nnotification shall, as a minimum, contain the information specified under paragraph 6 \nand shall not include sensitive operational data.\n5.\nA Member State may decide to provide for the possibility to fully or partially authorise the \nuse of \u2018real-time\u2019 remote biometric identification systems in publicly accessible spaces for \nthe purposes of law enforcement within the limits and under the conditions listed in \nparagraph 1, first subparagraph, point (h), and paragraphs 2 and 3. \u258c Member States \nconcerned shall lay down in their national law the necessary detailed rules for the request, \nissuance and exercise of, as well as supervision and reporting relating to, the \nauthorisations referred to in paragraph 3. Those rules shall also specify in respect of which \nof the objectives listed in paragraph 1, first subparagraph, point (h), including which of the \ncriminal offences referred to in point (h)(iii) thereof, the competent authorities may be \nauthorised to use those systems for the purposes of law enforcement. Member States shall \nnotify those rules to the Commission at the latest 30 days following the adoption thereof. \nMember States may introduce, in accordance with Union law, more restrictive laws on \nthe use of remote biometric identification systems.\n\nDocument 198: EN\nUnited in diversity\nEN\n2.\nThe use of \u2018real-time\u2019 remote biometric identification systems in publicly accessible spaces \nfor the purposes of law enforcement for any of the objectives referred to in paragraph 1, \nfirst subparagraph, point (h), shall be deployed for the purposes set out in that point only \nto confirm the identity of the specifically targeted individual, and it shall take into \naccount the following elements:\n(a)\nthe nature of the situation giving rise to the possible use, in particular the seriousness, \nprobability and scale of the harm that would be caused if the system were not used;\n(b)\nthe consequences of the use of the system for the rights and freedoms of all persons \nconcerned, in particular the seriousness, probability and scale of those consequences.\nIn addition, the use of \u2018real-time\u2019 remote biometric identification systems in publicly \naccessible spaces for the purposes of law enforcement for any of the objectives referred to \nin paragraph 1, first subparagraph, point (h), of this Article shall comply with necessary \nand proportionate safeguards and conditions in relation to the use in accordance with the \nnational law authorising the use thereof, in particular as regards the temporal, geographic \nand personal limitations. The use of the \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be authorised only if the law enforcement authority \nhas completed a fundamental rights impact assessment as provided for in Article 27 and \nhas registered the system in the EU database according to Article 49. However, in duly \njustified cases of urgency, the use of such systems may be commenced without the \nregistration in the EU database, provided that such registration is completed without \nundue delay.\n\nDocument 199: EN\nUnited in diversity\nEN\n3.\nFor the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each \u258c use \nfor the purposes of law enforcement of a \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be subject to a prior authorisation granted by a judicial \nauthority or \u258c an independent administrative authority whose decision is binding of the \nMember State in which the use is to take place, issued upon a reasoned request and in \naccordance with the detailed rules of national law referred to in paragraph 5. However, in a \nduly justified situation of urgency, the use of such system may be commenced without an \nauthorisation provided that such authorisation is requested without undue delay, at the \nlatest within 24 hours. If such authorisation is rejected, the use shall be stopped with \nimmediate effect and all the data, as well as the results and outputs of that use shall be \nimmediately discarded and deleted.\nThe competent judicial authority or an independent administrative authority whose \ndecision is binding shall grant the authorisation only where it is satisfied, on the basis of \nobjective evidence or clear indications presented to it, that the use of the \u2018real-time\u2019 remote \nbiometric identification system concerned is necessary for, and proportionate to, achieving \none of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in \nthe request and, in particular, remains limited to what is strictly necessary concerning \nthe period of time as well as the geographic and personal scope. In deciding on the \nrequest, that authority shall take into account the elements referred to in paragraph 2. No \ndecision that produces an adverse legal effect on a person may be taken based solely on \nthe output of the \u2018real-time\u2019 remote biometric identification system.\n\nDocument 201: EN\nUnited in diversity\nEN\n6.\nNational market surveillance authorities and the national data protection authorities of \nMember States that have been notified of the use of \u2018real-time\u2019 remote biometric \nidentification systems in publicly accessible spaces for law enforcement purposes \npursuant to paragraph 4 shall submit to the Commission annual reports on such use. \nFor that purpose, the Commission shall provide Member States and national market \nsurveillance and data protection authorities with a template, including information on \nthe number of the decisions taken by competent judicial authorities or an independent \nadministrative authority whose decision is binding upon requests for authorisations in \naccordance with paragraph 3 and their result.\n7.\nThe Commission shall publish annual reports on the use of real-time remote biometric \nidentification systems in publicly accessible spaces for law enforcement purposes, based \non aggregated data in Member States on the basis of the annual reports referred to in \nparagraph 6. Those annual reports shall not include sensitive operational data of the \nrelated law enforcement activities.\n8.\nThis Article shall not affect the prohibitions that apply where an AI practice infringes \nother Union law.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 200, "topic": "Biometric Data and AI"}}
{"id": "51e5a200-5fc7-44f3-b23e-ac6b9b526ca1", "question": "In the context of employment and workers management, what specific purposes are AI systems designed to fulfill, particularly regarding recruitment, performance evaluation, and decision-making related to employment contracts?", "reference_answer": "AI systems are intended to be used for recruitment or selection of natural persons, to place targeted job advertisements, to analyse and filter job applications, and to evaluate candidates; as well as to make decisions affecting terms of work-related relationships, the promotion or termination of work-related contractual relationships, to allocate tasks based on individual behaviour or personal traits or characteristics, or to monitor and evaluate the performance and behaviour of persons in such relationships.", "reference_context": "Document 439: EN\nUnited in diversity\nEN\n4.\nEmployment, workers management and access to self-employment:\n(a)\nAI systems intended to be used for the recruitment or selection of natural persons, in \nparticular to place targeted job advertisements, to analyse and filter job \napplications, and to evaluate candidates;\n(b)\nAI systems intended to be used to make decisions affecting terms of work-related \nrelationships, the promotion or termination of work-related contractual relationships, \nto allocate tasks based on individual behaviour or personal traits or characteristics \nor to monitor and evaluate the performance and behaviour of persons in such \nrelationships.\n5.\nAccess to and enjoyment of essential private services and essential public services and \nbenefits:\n(a)\nAI systems intended to be used by public authorities or on behalf of public \nauthorities to evaluate the eligibility of natural persons for essential public assistance \nbenefits and services, including healthcare services, as well as to grant, reduce, \nrevoke, or reclaim such benefits and services;\n(b)\nAI systems intended to be used to evaluate the creditworthiness of natural persons or \nestablish their credit score, with the exception of AI systems used for the purpose of \ndetecting financial fraud;", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 439, "topic": "AI Regulation and Ethics"}}
{"id": "67e2c189-c03b-4360-a050-cd85b1af1959", "question": "In the context of the EU AI Act, what specific elements must be detailed in the hardware description of an AI system, including any relevant illustrations or user-interface aspects?", "reference_answer": "The description of the hardware on which the AI system is intended to run.", "reference_context": "Document 445: EN\nUnited in diversity\nEN\n(e)\nthe description of the hardware on which the AI system is intended to run;\n(f)\nwhere the AI system is a component of products, photographs or illustrations \nshowing external features, the marking and internal layout of those products;\n(g)\na basic description of the user-interface provided to the deployer;\n(h)\ninstructions for use for the deployer, and a basic description of the user-interface \nprovided to the deployer, where applicable \u258c ;\n2.\nA detailed description of the elements of the AI system and of the process for its \ndevelopment, including:\n(a)\nthe methods and steps performed for the development of the AI system, including, \nwhere relevant, recourse to pre-trained systems or tools provided by third parties and \nhow those were used, integrated or modified by the provider;\n(b)\nthe design specifications of the system, namely the general logic of the AI system \nand of the algorithms; the key design choices including the rationale and assumptions \nmade, including with regard to persons or groups of persons in respect of who, the \nsystem is intended to be used; the main classification choices; what the system is \ndesigned to optimise for, and the relevance of the different parameters; the \ndescription of the expected output and output quality of the system; the decisions \nabout any possible trade-off made regarding the technical solutions adopted to \ncomply with the requirements set out in Chapter III, Section 2;", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 445, "topic": "AI Regulation and Compliance"}}
{"id": "5875523e-76bf-4677-ab9a-9bce02013a8f", "question": "In the context of AI regulatory sandboxes, what specific responsibilities do national competent authorities hold, particularly regarding resource allocation and cooperation with other relevant entities?", "reference_answer": "National competent authorities shall ensure that they allocate sufficient resources to comply with the regulations effectively and in a timely manner, and they should cooperate with other relevant authorities and allow for the involvement of other actors within the AI ecosystem.", "reference_context": "Document 314: EN\nUnited in diversity\nEN\n2.\nAdditional AI regulatory sandboxes at regional or local level, or established jointly with \nthe competent authorities of other Member States may also be established.\n3.\nThe European Data Protection Supervisor may also establish an AI regulatory sandbox \nfor Union institutions, bodies, offices and agencies, and may exercise the roles and the \ntasks of national competent authorities in accordance with this Chapter.\n4.\nMember States shall ensure that the competent authorities referred to in paragraphs 1 \nand 2 allocate sufficient resources to comply with this Article effectively and in a timely \nmanner. Where appropriate, national competent authorities shall cooperate with other \nrelevant authorities, and may allow for the involvement of other actors within the AI \necosystem. This Article shall not affect other regulatory sandboxes established under \nUnion or national law. Member States shall ensure an appropriate level of cooperation \nbetween the authorities supervising those other sandboxes and the national competent \nauthorities.\n\nDocument 135: EN\nUnited in diversity\nEN\nWhere appropriate, national competent authorities establishing AI regulatory sandboxes \nshould cooperate with other relevant authorities, including those supervising the \nprotection of fundamental rights, and could allow for the involvement of other actors \nwithin the AI ecosystem such as national or European standardisation organisations, \nnotified bodies, testing and experimentation facilities, research and experimentation \nlabs, European Digital Innovation Hubs and relevant stakeholder and civil society \norganisations. To ensure uniform implementation across the Union and economies of \nscale, it is appropriate to establish common rules for the AI regulatory sandboxes\u2019 \nimplementation and a framework for cooperation between the relevant authorities \ninvolved in the supervision of the sandboxes. AI regulatory sandboxes established under \nthis Regulation should be without prejudice to other law allowing for the establishment \nof other sandboxes aiming to ensure compliance with law other than this Regulation. \nWhere appropriate, relevant competent authorities in charge of those other regulatory \nsandboxes should consider the benefits of using those sandboxes also for the purpose of \nensuring compliance of AI systems with this Regulation. Upon agreement between the \nnational competent authorities and the participants in the AI regulatory sandbox, testing \nin real world conditions may also be operated and supervised in the framework of the AI \nregulatory sandbox.\n\nDocument 319: EN\nUnited in diversity\nEN\n13.\nThe AI regulatory sandboxes shall be designed and implemented in such a way that, \nwhere relevant, they facilitate cross-border cooperation between national competent \nauthorities.\n14.\nNational competent authorities \u258c shall coordinate their activities and cooperate within the \nframework of the \u258c Board. \u258c\n15.\nNational competent authorities shall inform the AI Office and the Board of the \nestablishment of a sandbox, and may ask them for support and guidance. The AI Office \nshall make publicly available a list of planned and existing sandboxes and keep it up to \ndate in order to encourage more interaction in the AI regulatory sandboxes and cross-\nborder cooperation.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 314, "topic": "AI Regulatory Sandboxes"}}
{"id": "f9d90bf8-7b88-4771-b239-d6872c2439a0", "question": "What specific measures must notifying authorities implement to prevent conflicts of interest, ensuring both objectivity and impartiality in their operations concerning conformity assessment bodies?", "reference_answer": "Notifying authorities shall be established, organised and operated in such a way that no conflict of interest arises with conformity assessment bodies, and that the objectivity and impartiality of their activities are safeguarded.", "reference_context": "Document 258: EN\nUnited in diversity\nEN\n2.\nMember States may decide that the assessment and monitoring referred to in \nparagraph 1 is to be carried out by a national accreditation body within the meaning of, \nand in accordance with, Regulation (EC) No 765/2008 \u258c .\n3.\nNotifying authorities shall be established, organised and operated in such a way that no \nconflict of interest arises with conformity assessment bodies, and that the objectivity and \nimpartiality of their activities are safeguarded.\n4.\nNotifying authorities shall be organised in such a way that decisions relating to the \nnotification of conformity assessment bodies are taken by competent persons different \nfrom those who carried out the assessment of those bodies.\n5.\nNotifying authorities shall offer or provide neither any activities that conformity \nassessment bodies perform, nor any consultancy services on a commercial or competitive \nbasis.\n6.\nNotifying authorities shall safeguard the confidentiality of the information that they obtain, \nin accordance with Article 78.\n7.\nNotifying authorities shall have an adequate number of competent personnel at their \ndisposal for the proper performance of their tasks. Competent personnel shall have the \nnecessary expertise, where applicable, for their function, in fields such as information \ntechnologies, AI and law, including the supervision of fundamental rights.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 258, "topic": "Notified Bodies Regulations"}}
{"id": "fe651a79-13ee-4e83-b106-180ae107cf99", "question": "What specific consultations are required prior to the disclosure of confidential information shared between national competent authorities concerning high-risk AI systems, particularly in contexts involving law enforcement and national security?", "reference_answer": "Prior consultation of the originating national competent authority and the deployer must be conducted before disclosing confidential information.", "reference_context": "Document 374: EN\nUnited in diversity\nEN\n3.\nWithout prejudice to paragraphs 1 and 2, information exchanged on a confidential basis \nbetween the national competent authorities or between national competent authorities and \nthe Commission shall not be disclosed without prior consultation of the originating \nnational competent authority and the deployer when high-risk AI systems referred to in \npoint 1, 6 or 7 of Annex III are used by law enforcement, border control, immigration or \nasylum authorities and when such disclosure would jeopardise public and national security \ninterests. This exchange of information shall not cover sensitive operational data in \nrelation to the activities of law enforcement, border control, immigration or asylum \nauthorities.\nWhen the law enforcement, immigration or asylum authorities are providers of high-risk \nAI systems referred to in point 1, 6 or 7 of Annex III, the technical documentation referred \nto in Annex IV shall remain within the premises of those authorities. Those authorities \nshall ensure that the market surveillance authorities referred to in Article 74(8) and (9), as \napplicable, can, upon request, immediately access the documentation or obtain a copy \nthereof. Only staff of the market surveillance authority holding the appropriate level of \nsecurity clearance shall be allowed to access that documentation or any copy thereof.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 374, "topic": "AI Regulation and Compliance"}}
{"id": "db3b89c6-f74b-4330-b547-df65b3e102b8", "question": "What specific actions must Member States undertake to establish and enforce penalties for violations of the Regulation, while considering the interests of SMEs and ensuring that these penalties are effective and proportionate?", "reference_answer": "Member States shall lay down the rules on penalties and other enforcement measures, which may also include warnings and non-monetary measures, applicable to infringements of this Regulation by operators.", "reference_context": "Document 402: EN\nUnited in diversity\nEN\nCHAPTER XII\nPENALTIES \nArticle 99\nPenalties\n1.\nIn accordance with the terms and conditions laid down in this Regulation, Member States \nshall lay down the rules on penalties and other enforcement measures, which may also \ninclude warnings and non-monetary measures, applicable to infringements of this \nRegulation by operators, and shall take all measures necessary to ensure that they are \nproperly and effectively implemented, thereby taking into account the guidelines issued \nby the Commission pursuant to Article 96. The penalties provided for shall be effective, \nproportionate and dissuasive. They shall take into \u258c account the interests of SMEs, \nincluding start-ups, and their economic viability.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 402, "topic": "AI Regulation in the EU"}}
{"id": "0678bc2f-62c3-4576-9681-06a3fe89cc29", "question": "What specific obligations and responsibilities must Member States fulfill concerning the resources and operational frameworks of their national competent authorities as outlined in this Regulation?", "reference_answer": "Member States shall ensure that their national competent authorities are provided with adequate technical, financial and human resources, and with infrastructure to fulfil their tasks effectively under this Regulation.", "reference_context": "Document 353: EN\nUnited in diversity\nEN\n2.\nMember States shall communicate to the Commission the identity of the notifying \nauthorities and the market surveillance authorities and the tasks of those authorities, as \nwell as any subsequent changes thereto. Member States shall make publicly available \ninformation on how competent authorities and single points of contact can be contacted, \nthrough electronic communication means by\u2026 [12 months from the date of entry into \nforce of this Regulation]. Member States shall designate a market surveillance authority \nto act as the single point of contact for this Regulation, and shall notify the Commission \nof the identity of the single point of contact. The Commission shall make a list of the \nsingle points of contact publicly available.\n3.\nMember States shall ensure that their national competent authorities are provided with \nadequate technical, financial and human resources, and with infrastructure to fulfil their \ntasks effectively under this Regulation. In particular, the national competent authorities \nshall have a sufficient number of personnel permanently available whose competences and \nexpertise shall include an in-depth understanding of AI technologies, data and data \ncomputing, personal data protection, cybersecurity, fundamental rights, health and safety \nrisks and knowledge of existing standards and legal requirements. Member States shall \nassess and, if necessary, update competence and resource requirements referred to in \nthis paragraph on an annual basis.\n4.\nNational competent authorities shall take appropriate measures to ensure an adequate \nlevel of cybersecurity.\n5.\nWhen performing their tasks, the national competent authorities shall act in accordance \nwith the confidentiality obligations set out in Article 78.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 353, "topic": "AI Regulation and Compliance"}}
{"id": "09b58cbe-e393-40fb-8fca-f643485092d1", "question": "What specific actions and measures can market surveillance authorities implement in response to non-compliance with AI practices, considering the timeframe and procedural rights involved?", "reference_answer": "Market surveillance authorities shall ensure that appropriate restrictive measures are taken in respect of the product or the AI system concerned, such as withdrawal of the product or the AI system from their market, without undue delay.", "reference_context": "Document 381: EN\nUnited in diversity\nEN\n8.\nIn exercising their power to monitor the application of this Article, and in accordance \nwith Article 11 of Regulation (EU) 2019/1020, market surveillance authorities may \nperform appropriate checks, taking into account in particular information stored in the \nEU database referred to in Article 71 of this Regulation.\nArticle 81\nUnion safeguard procedure\n1.\nWhere, within three months of receipt of the notification referred to in Article 79(5), or \nwithin 30 days in the case of non-compliance with the prohibition of the AI practices \nreferred to in Article 5, objections are raised by the market surveillance authority of a \nMember State to a measure taken by another market surveillance authority, or where the \nCommission considers the measure to be contrary to Union law, the Commission shall \nwithout undue delay enter into consultation with the market surveillance authority of the \nrelevant Member State and the operator or operators, and shall evaluate the national \nmeasure. On the basis of the results of that evaluation, the Commission shall, within six \nmonths, or within 60 days in the case of non-compliance with the prohibition of the AI \npractices referred to in Article 5, starting from the notification referred to in Article 79(5), \ndecide whether the national measure is justified and shall notify its decision to the market \nsurveillance authority of the Member State concerned. The Commission shall also inform \nall other market surveillance authorities of its decision.\n\nDocument 378: EN\nUnited in diversity\nEN\n7.\nThe market surveillance authorities other than the market surveillance authority of the \nMember State initiating the procedure shall, without undue delay, inform the Commission \nand the other Member States of any measures adopted and of any additional information at \ntheir disposal relating to the non-compliance of the AI system concerned, and, in the event \nof disagreement with the notified national measure, of their objections.\n8.\nWhere, within three months of receipt of the notification referred to in paragraph 5 of this \nArticle, no objection has been raised by either a market surveillance authority of a \nMember State or by the Commission in respect of a provisional measure taken by a market \nsurveillance authority of another Member State, that measure shall be deemed justified. \nThis shall be without prejudice to the procedural rights of the concerned operator in \naccordance with Article 18 of Regulation (EU) 2019/1020. The three-month period \nreferred to in this paragraph shall be reduced to 30 days in the event of non-compliance \nwith the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9.\nThe market surveillance authorities shall ensure that appropriate restrictive measures are \ntaken in respect of the product or the AI system concerned, such as withdrawal of the \nproduct or the AI system from their market, without undue delay.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 381, "topic": "AI Regulation and Compliance"}}
{"id": "2e538744-e952-4eba-8c4b-81388f17c0e8", "question": "What specific roles and responsibilities are assigned to the scientific panel of independent experts as outlined in Article 68, particularly in relation to supporting enforcement activities under this Regulation?", "reference_answer": "The scientific panel is intended to support the enforcement activities under this Regulation.", "reference_context": "Document 348: EN\nUnited in diversity\nEN\nArticle 68\nScientific panel of independent experts\n1.\nThe Commission shall, by means of an implementing act, make provisions on the \nestablishment of a scientific panel of independent experts (the \u2018scientific panel\u2019) \nintended to support the enforcement activities under this Regulation. That implementing \nact shall be adopted in accordance with the examination procedure referred to in Article \n98(2).\n2.\nThe scientific panel shall consist of experts selected by the Commission on the basis of \nup-to-date scientific or technical expertise in the field of AI necessary for the tasks set \nout in paragraph 3, and shall be able to demonstrate meeting all of the following \nconditions:\n(a)\nhaving particular expertise and competence and scientific or technical expertise in \nthe field of AI;", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 348, "topic": "Others"}}
{"id": "1df2109f-954e-4406-bcac-66b208a37b9b", "question": "What specific conditions must be satisfied to lawfully process personal data in the AI regulatory sandbox aimed at developing AI systems that serve significant public interests?", "reference_answer": "Personal data lawfully collected for other purposes may be processed solely for the purpose of developing, training, and testing certain AI systems in the sandbox when all of the following conditions are met: AI systems shall be developed for safeguarding substantial public interest by a public authority or another natural or legal person.", "reference_context": "Document 325: EN\nUnited in diversity\nEN\nArticle 59\nFurther processing of personal data for developing certain AI systems\n in the public interest in the AI regulatory sandbox\n1.\nIn the AI regulatory sandbox,, personal data lawfully collected for other purposes may be \nprocessed solely for the purpose of developing, training and testing certain \u258c AI systems \nin the sandbox when all of the following conditions are met:\n(a)\n\u258c AI systems shall be developed for safeguarding substantial public interest by a \npublic authority or another natural or legal person and in one or more of the \nfollowing areas:\n(i)\npublic safety and public health, including disease detection, diagnosis \nprevention, control and treatment and improvement of health care systems;\n(ii)\na high level of protection and improvement of the quality of the environment, \nprotection of biodiversity, protection against pollution, green transition \nmeasures, climate change mitigation and adaptation measures;", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 325, "topic": "AI Regulatory Sandboxes"}}
{"id": "9df6ba58-7f4e-493d-9f9f-ffc293aefb0a", "question": "In the event that a serious incident arises during testing in real-world conditions, what specific actions must a provider take to ensure compliance with regulatory requirements and to mitigate potential risks?", "reference_answer": "The provider shall adopt immediate mitigation measures or, failing that, shall suspend the testing in real world conditions until such mitigation takes place, or otherwise terminate it.", "reference_context": "Document 334: EN\nUnited in diversity\nEN\n7.\nAny serious incident identified in the course of the testing in real world conditions shall \nbe reported to the national market surveillance authority in accordance with Article 73. \nThe provider or prospective provider shall adopt immediate mitigation measures or, \nfailing that, shall suspend the testing in real world conditions until such mitigation takes \nplace, or otherwise terminate it. The provider or prospective provider shall establish a \nprocedure for the prompt recall of the AI system upon such termination of the testing in \nreal world conditions.\n8.\nProviders or prospective providers shall notify the national market surveillance authority \nin the Member State where the testing in real world conditions is to be conducted of the \nsuspension or termination of the testing in real world conditions and of the final \noutcomes.\n9.\nThe provider or prospective provider shall be liable under applicable Union and national \nliability law for any damage caused in the course of their testing in real world \nconditions.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 334, "topic": "AI System Testing Regulations"}}
{"id": "09a90775-a4b3-4e6e-956a-ecd93080bc14", "question": "What specific conditions must be met for market surveillance authorities to gain access to the source code of a high-risk AI system, particularly regarding the necessity of such access and the prior efforts made in testing or auditing?", "reference_answer": "Access to source code is necessary to assess the conformity of a high-risk AI system with the requirements set out in Chapter III, Section 2, and testing or auditing procedures and verifications based on the data and documentation provided by the provider have been exhausted or proved insufficient.", "reference_context": "Document 367: EN\nUnited in diversity\nEN\n13.\nMarket surveillance authorities shall be granted access to the source code of the high-\nrisk AI system upon a reasoned request and only when both of the following conditions \nare fulfilled:\n(a)\naccess to source code is necessary to assess the conformity of a high-risk AI system \nwith the requirements set out in Chapter III, Section 2; and,\n(b)\ntesting or auditing procedures and verifications based on the data and \ndocumentation provided by the provider have been exhausted or proved \ninsufficient.\n14.\nAny information or documentation obtained by market surveillance authorities shall be \ntreated in accordance with the confidentiality obligations set out in Article 78.\nArticle 75\nMutual assistance, market surveillance and control of general-purpose AI systems\n1.\nWhere an AI system is based on a general-purpose AI model, and the model and the \nsystem are developed by the same provider, the AI Office shall have powers to monitor \nand supervise compliance of that AI system with obligations under this Regulation. To \ncarry out its monitoring and supervision tasks, the AI Office shall have all the powers of \na market surveillance authority provided for in this Section and Regulation (EU) \n2019/1020.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 367, "topic": "AI Regulation and Compliance"}}
{"id": "4192d213-e2c3-4e2e-8221-250f491e425d", "question": "What specific conditions must be fulfilled for the lawful processing of personal data within the AI regulatory sandbox, particularly regarding the development of AI systems aimed at serving substantial public interests?", "reference_answer": "Personal data lawfully collected for other purposes may be processed in the AI regulatory sandbox solely for developing, training, and testing certain AI systems when they are developed for safeguarding substantial public interest by a public authority or another natural or legal person.", "reference_context": "Document 325: EN\nUnited in diversity\nEN\nArticle 59\nFurther processing of personal data for developing certain AI systems\n in the public interest in the AI regulatory sandbox\n1.\nIn the AI regulatory sandbox,, personal data lawfully collected for other purposes may be \nprocessed solely for the purpose of developing, training and testing certain \u258c AI systems \nin the sandbox when all of the following conditions are met:\n(a)\n\u258c AI systems shall be developed for safeguarding substantial public interest by a \npublic authority or another natural or legal person and in one or more of the \nfollowing areas:\n(i)\npublic safety and public health, including disease detection, diagnosis \nprevention, control and treatment and improvement of health care systems;\n(ii)\na high level of protection and improvement of the quality of the environment, \nprotection of biodiversity, protection against pollution, green transition \nmeasures, climate change mitigation and adaptation measures;", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 325, "topic": "AI Regulatory Sandboxes"}}
{"id": "22b14ddf-1cb7-4a56-b724-f572c47f8f4b", "question": "In which language must certificates, issued by notified bodies in compliance with Annex VII, be prepared to ensure they can be easily understood by the relevant authorities in the respective Member State where the notified body operates?", "reference_answer": "Certificates issued by notified bodies shall be drawn-up in a language which can be easily understood by the relevant authorities in the Member State in which the notified body is established.", "reference_context": "Document 283: EN\nUnited in diversity\nEN\n6.\nThe Commission is empowered to adopt delegated acts in accordance with Article 97 in \norder to amend paragraphs 1 and 2 of this Article in order to subject high-risk AI systems \nreferred to in points 2 to 8 of Annex III to the conformity assessment procedure referred to \nin Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into \naccount the effectiveness of the conformity assessment procedure based on internal control \nreferred to in Annex VI in preventing or minimising the risks to health and safety and \nprotection of fundamental rights posed by such systems, as well as the availability of \nadequate capacities and resources among notified bodies.\nArticle 44\nCertificates\n1.\nCertificates issued by notified bodies in accordance with Annex VII shall be drawn-up in a \nlanguage which can be easily understood by the relevant authorities in the Member State \nin which the notified body is established.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 283, "topic": "Others"}}
{"id": "e62d4e60-cbdf-45da-bb89-cd3761fd05d6", "question": "What specific actions does the Commission advocate for to enhance the effective enforcement of obligations related to the detection and labeling of AI-generated content, particularly in terms of collaboration with stakeholders across the value chain?", "reference_answer": "The Commission may encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content.", "reference_context": "Document 131: EN\nUnited in diversity\nEN\n(135)\nWithout prejudice to the mandatory nature and full applicability of the transparency \nobligations, the Commission may also encourage and facilitate the drawing up of codes \nof practice at Union level to facilitate the effective implementation of the obligations \nregarding the detection and labelling of artificially generated or manipulated content, \nincluding to support practical arrangements for making, as appropriate, the detection \nmechanisms accessible and facilitating cooperation with other actors along the value \nchain, disseminating content or checking its authenticity and provenance to enable the \npublic to effectively distinguish AI-generated content.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 131, "topic": "AI Regulation in the EU"}}
{"id": "2a511835-cf5c-4e83-a971-5bb23e88806b", "question": "What specific categories of vulnerabilities can AI systems potentially exploit, particularly in relation to individuals' age, disabilities, or socio-economic conditions, as outlined in the provided context?", "reference_answer": "AI systems may exploit vulnerabilities due to age, disability, or a specific social or economic situation that makes persons more vulnerable to exploitation, such as those living in extreme poverty or belonging to ethnic or religious minorities.", "reference_context": "Document 30: This could be facilitated, for example, by machine-brain \ninterfaces or virtual reality as they allow for a higher degree of control of what stimuli \nare presented to persons, insofar as they may materially distort their behaviour in a \nsignificantly harmful manner. In addition, AI systems may also otherwise exploit the \nvulnerabilities of a person or a specific group of persons due to their age, disability within \nthe meaning of Directive (EU) 2019/882 of the European Parliament and of the \nCouncil16, or a specific social or economic situation that is likely to make those persons \nmore vulnerable to exploitation such as persons living in extreme poverty, ethnic or \nreligious minorities. \n16\nDirective (EU) 2019/882 of the European Parliament and of the Council of 17 April 2019 on \nthe accessibility requirements for products and services (OJ L 151, 7.6.2019, p. 70).", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 30, "topic": "AI Regulation and Ethics"}}
{"id": "393f88c9-636d-4d16-8e4f-fe7cb9b25f74", "question": "What is the minimum duration, as specified by the EU AI Act, for which logs created by high-risk AI systems must be retained, considering any potential exceptions outlined in Union or national laws?", "reference_answer": "The logs shall be kept for a period of at least six months, unless provided otherwise in applicable Union or national law.", "reference_context": "Document 251: EN\nUnited in diversity\nEN\n6.\nDeployers of high-risk AI systems shall keep the logs automatically generated by that \nhigh-risk AI system \u258c to the extent such logs are under their control, \u258c for a period \u258c \nappropriate to the intended purpose of the high-risk AI system, of at least six months, \nunless provided otherwise in applicable Union or national law, in particular in Union law \non the protection of personal data.\nDeployers that are financial institutions subject to requirements regarding their internal \ngovernance, arrangements or processes under Union financial services law shall \nmaintain the logs as part of the documentation kept pursuant to the relevant Union \nfinancial service law.\n7.\nBefore putting into service or using a high-risk AI system at the workplace, deployers \nwho are employers shall inform workers\u2019 representatives and the affected workers that \nthey will be subject to the use of the high-risk AI system. This information shall be \nprovided, where applicable, in accordance with the rules and procedures laid down in \nUnion and national law and practice on information of workers and their \nrepresentatives.\n8.\nDeployers of high-risk AI systems that are public authorities, or Union institutions, \nbodies, offices or agencies shall comply with the registration obligations referred to in \nArticle 49. When such deployers find that the high-risk AI system that they envisage \nusing has not been registered in the EU database referred to in Article 71, they shall not \nuse that system and shall inform the provider or the distributor.\n\nDocument 236: EN\nUnited in diversity\nEN\nArticle 19\nAutomatically generated logs\n1.\nProviders of high-risk AI systems shall keep the logs referred to in Article 12(1), \nautomatically generated by their high-risk AI systems, to the extent such logs are under \ntheir control. Without prejudice to applicable Union or national law, the logs shall be \nkept for a period \u258c appropriate to the intended purpose of the high-risk AI system, of at \nleast six months, unless provided otherwise in the applicable Union or national law, in \nparticular in Union law on the protection of personal data.\n2.\nProviders that are financial institutions subject to requirements regarding their internal \ngovernance, arrangements or processes under Union financial services law shall \nmaintain the logs automatically generated by their high-risk AI systems as part of the \ndocumentation kept under the relevant financial services law.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 251, "topic": "High-Risk AI Regulation"}}
{"id": "a8a7603f-d649-45a4-be3b-14f033180d02", "question": "What specific fundamental rights must be taken into account when determining if an AI system qualifies as high risk, especially in relation to their potential adverse impacts?", "reference_answer": "The rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, the right to non-discrimination, the right to education, consumer protection, workers\u2019 rights, the rights of persons with disabilities, gender equality, intellectual property rights, the right to an effective remedy and to a fair trial, the right of defence and the presumption of innocence, and the right to good administration.", "reference_context": "Document 49: EN\nUnited in diversity\nEN\n(48)\nThe extent of the adverse impact caused by the AI system on the fundamental rights \nprotected by the Charter is of particular relevance when classifying an AI system as high \nrisk. Those rights include the right to human dignity, respect for private and family life, \nprotection of personal data, freedom of expression and information, freedom of \nassembly and of association, the right to non-discrimination, the right to education, \nconsumer protection, workers\u2019 rights, the rights of persons with disabilities, gender \nequality, intellectual property rights, the right to an effective remedy and to a fair trial, \nthe right of defence and the presumption of innocence, and the right to good \nadministration. In addition to those rights, it is important to highlight the fact that \nchildren have specific rights as enshrined in Article 24 of the Charter and in the United \nNations Convention on the Rights of the Child, further developed in the UNCRC \nGeneral Comment No 25 as regards the digital environment, both of which require \nconsideration of the children\u2019s vulnerabilities and provision of such protection and care \nas necessary for their well-being. The fundamental right to a high level of environmental \nprotection enshrined in the Charter and implemented in Union policies should also be \nconsidered when assessing the severity of the harm that an AI system can cause, \nincluding in relation to the health and safety of persons.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 49, "topic": "AI Regulation and Ethics"}}
{"id": "14000e9f-e2d9-4c7f-ae94-bf236a108d46", "question": "According to Article 58, what essential components must be included in the implementing acts for AI regulatory sandboxes to ensure uniformity across the Union, particularly regarding eligibility, participation procedures, and participant conditions?", "reference_answer": "The implementing acts must include common principles on eligibility and selection criteria for participation, procedures for application, participation, monitoring, exiting, and termination, as well as the terms and conditions applicable to the participants.", "reference_context": "Document 321: EN\nUnited in diversity\nEN\nArticle 58\nDetailed arrangements for, and functioning of, AI regulatory sandboxes\n1.\nIn order to avoid fragmentation across the Union, the Commission shall adopt \nimplementing acts specifying the detailed arrangements for the establishment, \ndevelopment, implementation, operation and supervision of the AI regulatory sandboxes. \nThe implementing acts shall include common principles on the following issues:\n(a)\neligibility and selection criteria for participation in the AI regulatory sandbox;\n(b)\nprocedures for the application, participation, monitoring, exiting from and \ntermination of the AI regulatory sandbox, including the sandbox plan and the exit \nreport;\n(c)\nthe terms and conditions applicable to the participants.\nThose implementing acts shall be adopted in accordance with the examination \nprocedure referred to in Article 98(2).\n2.\nThe implementing acts referred to in paragraph 1 shall ensure:\n(a)\nthat AI regulatory sandboxes are open to any applying provider or prospective \nprovider of an AI system who fulfils eligibility and selection criteria, which shall be \ntransparent and fair, and that national competent authorities inform applicants of \ntheir decision within three months of the application;", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 321, "topic": "AI Regulatory Sandboxes"}}
{"id": "50b5ac3a-8d53-4bb1-8342-e96a59331f39", "question": "What specific considerations and requirements must be taken into account when formulating acts related to Artificial Intelligence systems that are defined as safety components under the new EU regulations, particularly those outlined in Chapter III, Section 2 of Regulation (EU) 2024/\u2026?", "reference_answer": "The requirements set out in Chapter III, Section 2, of Regulation (EU) 2024/\u2026 shall be taken into account.", "reference_context": "Document 420: EN\nUnited in diversity\nEN\n(6)\nin Article 58, the following paragraph is added:\n\u20183.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019.\nArticle 109\nAmendment to Regulation (EU) 2019/2144\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\u20183.\nWhen adopting the implementing acts pursuant to paragraph 2, concerning artificial \nintelligence systems which are safety components within the meaning of Regulation (EU) \n2024/\u2026 of the European Parliament and of the Council*++, the requirements set out in \nChapter III, Section 2, of that Regulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n++\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 419: EN\nUnited in diversity\nEN\n(3)\nin Article 43, the following paragraph is added:\n\u20184.\nWhen adopting implementing acts pursuant to paragraph 1 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n(4)\nin Article 47, the following paragraph is added:\n\u20183.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n(5)\nin Article 57, the following subparagraph is added:\n\u2018When adopting those implementing acts concerning Artificial Intelligence systems which \nare safety components within the meaning of Regulation (EU) 2024/\u2026+, the requirements \nset out in Chapter III, Section 2, of that Regulation shall be taken into account.\u2019;\n+\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n\nDocument 418: EN\nUnited in diversity\nEN\nArticle 108\nAmendments to Regulation (EU) 2018/1139\nRegulation (EU) 2018/1139 is amended as follows:\n(1)\nin Article 17, the following paragraph is added:\n\u20183.\nWithout prejudice to paragraph 2, when adopting implementing acts pursuant to \nparagraph 1 concerning Artificial Intelligence systems which are safety components \nwithin the meaning of Regulation (EU) 2024/\u2026 of the European Parliament and of \nthe Council*+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying \ndown harmonised rules on artificial intelligence and amending Regulations (EC) No \n300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 \nand (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) \n2020/1828 (Artificial Intelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019;\n(2)\nin Article 19, the following paragraph is added:\n\u20184.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026++, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n++\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n\nDocument 416: EN\nUnited in diversity\nEN\nArticle 106\nAmendment to Directive (EU) 2016/797\nIn Article 5 of Directive (EU) 2016/797, the following paragraph is added:\n\u201812.\nWhen adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to \nparagraph 11 concerning Artificial Intelligence systems which are safety components \nwithin the meaning of Regulation (EU) 2024/\u2026 of the European Parliament and of the \nCouncil*+, the requirements set out in Chapter III, Section 2, of that Regulation shall be \ntaken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 420, "topic": "Artificial Intelligence Regulation"}}
{"id": "f02cc6b9-7bff-492c-b83c-a203c271111a", "question": "What are the key reasons for enforcing the regulations outlined in this document on AI system providers, particularly regarding the protection of individual rights and maintaining market integrity across the EU, regardless of their geographical location?", "reference_answer": "The purpose is to ensure a level playing field and an effective protection of rights and freedoms of individuals across the Union, irrespective of whether the providers are established within the Union or in a third country.", "reference_context": "Document 22: EN\nUnited in diversity\nEN\n(21)\nIn order to ensure a level playing field and an effective protection of rights and freedoms of \nindividuals across the Union, the rules established by this Regulation should apply to \nproviders of AI systems in a non-discriminatory manner, irrespective of whether they are \nestablished within the Union or in a third country, and to deployers of AI systems \nestablished within the Union.\n(22)\nIn light of their digital nature, certain AI systems should fall within the scope of this \nRegulation even when they are not placed on the market, put into service, or used in the \nUnion. This is the case, for example, where an operator established in the Union contracts \ncertain services to an operator established in a third country in relation to an activity to be \nperformed by an AI system that would qualify as high-risk \u258c . In those circumstances, the \nAI system used in a third country by the operator could process data lawfully collected in \nand transferred from the Union, and provide to the contracting operator in the Union the \noutput of that AI system resulting from that processing, without that AI system being \nplaced on the market, put into service or used in the Union. To prevent the circumvention \nof this Regulation and to ensure an effective protection of natural persons located in the \nUnion, this Regulation should also apply to providers and deployers of AI systems that are \nestablished in a third country, to the extent the output produced by those systems is \nintended to be used in the Union.\n\nDocument 3: EN\nUnited in diversity\nEN\n(3)\n\u258c AI systems \u258c can be easily deployed in a large variety of sectors of the economy and \nmany parts of society, including across borders, and can easily circulate throughout the \nUnion. Certain Member States have already explored the adoption of national rules to \nensure that AI is trustworthy and safe and is developed and used in accordance with \nfundamental rights obligations. Diverging national rules may lead to the fragmentation of \nthe internal market and may decrease legal certainty for operators that develop, import or \nuse AI systems. A consistent and high level of protection throughout the Union should \ntherefore be ensured in order to achieve trustworthy AI, while divergences hampering the \nfree circulation, innovation, deployment and the uptake of AI systems and related \nproducts and services within the internal market should be prevented by laying down \nuniform obligations for operators and guaranteeing the uniform protection of overriding \nreasons of public interest and of rights of persons throughout the internal market on the \nbasis of Article 114 of the Treaty on the Functioning of the European Union (TFEU). To \nthe extent that this Regulation contains specific rules on the protection of individuals with \nregard to the processing of personal data concerning restrictions of the use of AI systems \nfor remote biometric identification for the purpose of law enforcement, of the use of AI \nsystems for risk assessments of natural persons for the purpose of law enforcement and \nof the use of AI systems of biometric categorisation for the purpose of law enforcement, it \nis appropriate to base this Regulation, in so far as those specific rules are concerned, on \nArticle 16 TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is \nappropriate to consult the European Data Protection Board.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 22, "topic": "AI Regulation in the EU"}}
{"id": "96d9cee8-eb06-447b-818c-28befb20c4c4", "question": "What specific actions should providers of general-purpose AI models with systemic risks take immediately after a serious incident occurs, despite their previous risk mitigation strategies, including how they should report this to the Commission and national authorities?", "reference_answer": "The general-purpose AI model provider should without undue delay keep track of the incident and report any relevant information and possible corrective measures to the Commission and national competent authorities.", "reference_context": "Document 115: EN\nUnited in diversity\nEN\n(115)\nProviders of general-purpose AI models with systemic risks should assess and mitigate \npossible systemic risks. If, despite efforts to identify and prevent risks related to a \ngeneral-purpose AI model that may present systemic risks, the development or use of the \nmodel causes a serious incident, the general-purpose AI model provider should without \nundue delay keep track of the incident and report any relevant information and possible \ncorrective measures to the Commission and national competent authorities. \nFurthermore, providers should ensure an adequate level of cybersecurity protection for \nthe model and its physical infrastructure, if appropriate, along the entire model lifecycle. \nCybersecurity protection related to systemic risks associated with malicious use or \nattacks should duly consider accidental model leakage, unauthorised releases, \ncircumvention of safety measures, and defence against cyberattacks, unauthorised \naccess or model theft. That protection could be facilitated by securing model weights, \nalgorithms, servers, and data sets, such as through operational security measures for \ninformation security, specific cybersecurity policies, adequate technical and established \nsolutions, and cyber and physical access controls, appropriate to the relevant \ncircumstances and the risks involved.\n\nDocument 114: EN\nUnited in diversity\nEN\n(113)\nIf the Commission becomes aware of the fact that a general-purpose AI model meets the \nrequirements to classify as a general-purpose AI model with systemic risk, which \npreviously had either not been known or of which the relevant provider has failed to \nnotify the Commission, the Commission should be empowered to designate it so. A \nsystem of qualified alerts should ensure that the AI Office is made aware by the \nscientific panel of general-purpose AI models that should possibly be classified as \ngeneral-purpose AI models with systemic risk, in addition to the monitoring activities of \nthe AI Office.\n(114)\nThe providers of general-purpose AI models presenting systemic risks should be subject, \nin addition to the obligations provided for providers of general-purpose AI models, to \nobligations aimed at identifying and mitigating those risks and ensuring an adequate \nlevel of cybersecurity protection, regardless of whether it is provided as a standalone \nmodel or embedded in an AI system or a product. To achieve those objectives, this \nRegulation should require providers to perform the necessary model evaluations, in \nparticular prior to its first placing on the market, including conducting and documenting \nadversarial testing of models, also, as appropriate, through internal or independent \nexternal testing. In addition, providers of general-purpose AI models with systemic risks \nshould continuously assess and mitigate systemic risks, including for example by putting \nin place risk-management policies, such as accountability and governance processes, \nimplementing post-market monitoring, taking appropriate measures along the entire \nmodel\u2019s lifecycle and cooperating with relevant actors along the AI value chain.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 115, "topic": "Systemic Risks in AI"}}
{"id": "89aad031-ced4-4eb5-b8ee-64d74a26447e", "question": "Could you enumerate specific data-preparation processing operations that are deemed relevant, as outlined in the context, while also considering their implications for data quality and compliance?", "reference_answer": "Relevant data-preparation processing operations include annotation, labelling, cleaning, updating, enrichment, and aggregation.", "reference_context": "Document 215: EN\nUnited in diversity\nEN\n(c)\nrelevant data-preparation processing operations, such as annotation, labelling, \ncleaning, updating, enrichment and aggregation;\n(d)\nthe formulation of \u258c assumptions, in particular with respect to the information that \nthe data are supposed to measure and represent;\n(e)\nan assessment of the availability, quantity and suitability of the data sets that are \nneeded;\n(f)\nexamination in view of possible biases that are likely to affect the health and safety \nof persons, have a negative impact on fundamental rights or lead to discrimination \nprohibited under Union law, especially where data outputs influence inputs for \nfuture operations;\n(g)\nappropriate measures to detect, prevent and mitigate possible biases identified \naccording to point (f);\n(h)\nthe identification of relevant data gaps or shortcomings that prevent compliance \nwith this Regulation, and how those gaps and shortcomings can be addressed.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 215, "topic": "AI Regulation in the EU"}}
{"id": "a5a71220-9584-491c-a226-466def2643d6", "question": "What specific obligations must providers of high-risk AI systems adhere to in order to maintain compliance with the Regulation, considering factors such as organizational size and accountability measures?", "reference_answer": "Providers shall, in any event, respect the degree of rigour and the level of protection required to ensure the compliance of their high-risk AI systems with this Regulation.", "reference_context": "Document 233: EN\nUnited in diversity\nEN\n(j)\nthe handling of communication with national competent authorities, other relevant \nauthorities, including those providing or supporting the access to data, notified \nbodies, other operators, customers or other interested parties;\n(k)\nsystems and procedures for record-keeping of all relevant documentation and \ninformation;\n(l)\nresource management, including security-of-supply related measures;\n(m)\nan accountability framework setting out the responsibilities of the management and \nother staff with regard to all the aspects listed in this paragraph.\n2.\nThe implementation of the aspects referred to in paragraph 1 shall be proportionate to the \nsize of the provider\u2019s organisation. Providers shall, in any event, respect the degree of \nrigour and the level of protection required to ensure the compliance of their high-risk AI \nsystems with this Regulation.\n3.\nProviders of high-risk AI systems that are subject to obligations regarding quality \nmanagement systems or an equivalent function under relevant sectoral Union law may \ninclude the aspects listed in paragraph 1 as part of the quality management systems \npursuant to that law.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 233, "topic": "High-Risk AI Regulation"}}
{"id": "3188661d-e0f3-4352-9ea0-0e5ec9956907", "question": "What specific responsibilities and initiatives is the AI Office mandated to implement in relation to small and medium-sized enterprises (SMEs), particularly concerning awareness, communication, and standardisation?", "reference_answer": "The AI Office shall provide standardised templates for areas covered by this Regulation, as specified by the Board in its request.", "reference_context": "Document 337: EN\nUnited in diversity\nEN\n(b)\norganise specific awareness raising and training activities on the application of this \nRegulation tailored to the needs of SMEs including start-ups, deployers and, as \nappropriate, local public authorities;\n(c)\nutilise existing dedicated channels and where appropriate, establish new ones for \ncommunication with SMEs including start-ups, deployers, other innovators and, as \nappropriate, local public authorities to provide advice and respond to queries about \nthe implementation of this Regulation, including as regards participation in AI \nregulatory sandboxes;\n(d)\nfacilitate the participation of SMEs and other relevant stakeholders in the \nstandardisation development process.\n2.\nThe specific interests and needs of the SME providers, including start-ups, shall be taken \ninto account when setting the fees for conformity assessment under Article 43, reducing \nthose fees proportionately to their size, market size and other relevant indicators.\n3.\nThe AI Office shall undertake the following actions:\n(a)\nprovide standardised templates for areas covered by this Regulation, as specified by \nthe Board in its request;", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 337, "topic": "Others"}}
{"id": "32036b22-a438-408d-b915-f3ccac45f55c", "question": "In the absence of harmonised standards for high-risk AI systems, what specific steps must a provider undertake to ensure compliance, particularly when the system is intended for use by governmental authorities?", "reference_answer": "The provider shall follow the conformity assessment procedure set out in Annex VII.", "reference_context": "Document 281: EN\nUnited in diversity\nEN\nFor the purposes of the conformity assessment procedure referred to in Annex VII, the \nprovider may choose any of the notified bodies. However, where the high-risk AI system is \nintended to be put into service by law enforcement, immigration or asylum authorities or \nby Union institutions, bodies, offices or agencies, the market surveillance authority \nreferred to in Article 74(8) or (9), as applicable, shall act as a notified body.\n2.\nFor high-risk AI systems referred to in points 2 to 8 of Annex III, \u258c providers shall follow \nthe conformity assessment procedure based on internal control as referred to in Annex VI, \nwhich does not provide for the involvement of a notified body. \u258c\n3.\nFor high-risk AI systems covered by the Union harmonisation legislation listed in Section \nA of Annex I, the provider shall follow the relevant conformity assessment procedure as \nrequired under those legal acts. The requirements set out in Section 2 of this Chapter shall \napply to those high-risk AI systems and shall be part of that assessment. Points 4.3., 4.4., \n4.5. and the fifth paragraph of point 4.6 of Annex VII shall also apply.\nFor the purposes of that assessment, notified bodies which have been notified under those \nlegal acts shall be entitled to control the conformity of the high-risk AI systems with the \nrequirements set out in Section 2, provided that the compliance of those notified bodies \nwith requirements laid down in Article 31(4), (5), (10) and (11) has been assessed in the \ncontext of the notification procedure under those legal acts.\n\nDocument 280: EN\nUnited in diversity\nEN\nArticle 43\nConformity assessment\n1.\nFor high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the \ncompliance of a high-risk AI system with the requirements set out in Section 2, the \nprovider has applied harmonised standards referred to in Article 40, or, where applicable, \ncommon specifications referred to in Article 41, the provider shall opt for one of the \nfollowing conformity assessment procedures based on:\n(a)\nthe internal control referred to in Annex VI; or\n(b)\nthe assessment of the quality management system and the assessment of the technical \ndocumentation, with the involvement of a notified body, referred to in Annex VII.\n\u258cIn demonstrating the compliance of a high-risk AI system with the requirements set out \nin Section 2, the provider shall follow the conformity assessment procedure set out in \nAnnex VII where:\n(a)\nharmonised standards referred to in Article 40 \u258c do not exist, and common \nspecifications referred to in Article 41 are not available;\n(b)\nthe provider has not applied, or has applied only part of, the harmonised standard;\n(c)\nthe common specifications referred to in point (a) exist, but the provider has not \napplied them;\n(d)\none or more of the harmonised standards referred to in point (a) has been \npublished with a restriction, and only on the part of the standard that was \nrestricted.\n\nDocument 262: EN\nUnited in diversity\nEN\n4.\nNotified bodies shall be independent of the provider of a high-risk AI system in relation to \nwhich they perform conformity assessment activities. Notified bodies shall also be \nindependent of any other operator having an economic interest in high-risk AI systems \nassessed, as well as of any competitors of the provider. This shall not preclude the use of \nassessed high-risk AI systems that are necessary for the operations of the conformity \nassessment body, or the use of such high-risk AI systems for personal purposes.\n5.\nNeither a conformity assessment body, its top-level management nor the personnel \nresponsible for carrying out its conformity assessment tasks shall be directly involved in \nthe design, development, marketing or use of high-risk AI systems, nor shall they \nrepresent the parties engaged in those activities. They shall not engage in any activity \nthat might conflict with their independence of judgement or integrity in relation to \nconformity assessment activities for which they are notified. This shall, in particular, \napply to consultancy services.\n6.\nNotified bodies shall be organised and operated so as to safeguard the independence, \nobjectivity and impartiality of their activities. Notified bodies shall document and \nimplement a structure and procedures to safeguard impartiality and to promote and apply \nthe principles of impartiality throughout their organisation, personnel and assessment \nactivities.\n\nDocument 123: EN\nUnited in diversity\nEN\n(124)\nIt is appropriate that, in order to minimise the burden on operators and avoid any possible \nduplication, for high-risk AI systems related to products which are covered by existing \nUnion harmonisation legislation based on the New Legislative Framework, the compliance \nof those AI systems with the requirements of this Regulation should be assessed as part of \nthe conformity assessment already provided for in that law. The applicability of the \nrequirements of this Regulation should thus not affect the specific logic, methodology or \ngeneral structure of conformity assessment under the relevant Union harmonisation \nlegislation. \u258c\n(125)\nGiven the complexity of high-risk AI systems and the risks that are associated with them, \nit is important to develop an adequate conformity assessment procedure for high-risk AI \nsystems involving notified bodies, so-called third party conformity assessment. However, \ngiven the current experience of professional pre-market certifiers in the field of product \nsafety and the different nature of risks involved, it is appropriate to limit, at least in an \ninitial phase of application of this Regulation, the scope of application of third-party \nconformity assessment for high-risk AI systems other than those related to products. \nTherefore, the conformity assessment of such systems should be carried out as a general \nrule by the provider under its own responsibility, with the only exception of AI systems \nintended to be used for biometrics.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 281, "topic": "Others"}}
{"id": "8570692a-164f-42ce-a01f-51a7dc0c0431", "question": "What specific elements must a provider incorporate in their formal request for the reassessment of their general-purpose AI model's classification as posing systemic risks, considering the new criteria that may have emerged since the initial designation?", "reference_answer": "Such a request shall contain objective, detailed and new reasons that have arisen since the designation decision.", "reference_context": "Document 301: EN\nUnited in diversity\nEN\n5.\nUpon a reasoned request of a provider whose model has been designated as a general-\npurpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take \nthe request into account and may decide to reassess whether the general-purpose AI \nmodel can still be considered to present systemic risks on the basis of the criteria set out \nin Annex XIII. Such a request shall contain objective, detailed and new reasons that \nhave arisen since the designation decision. Providers may request reassessment at the \nearliest six months after the designation decision. Where the Commission, following its \nreassessment, decides to maintain the designation as a general-purpose AI model with \nsystemic risk, providers may request reassessment at the earliest six months after that \ndecision.\n6.\nThe Commission shall ensure that a list of general-purpose AI models with systemic risk \nis published and shall keep that list up to date, without prejudice to the need to observe \nand protect intellectual property rights and confidential business information or trade \nsecrets in accordance with Union and national law.\n\nDocument 114: EN\nUnited in diversity\nEN\n(113)\nIf the Commission becomes aware of the fact that a general-purpose AI model meets the \nrequirements to classify as a general-purpose AI model with systemic risk, which \npreviously had either not been known or of which the relevant provider has failed to \nnotify the Commission, the Commission should be empowered to designate it so. A \nsystem of qualified alerts should ensure that the AI Office is made aware by the \nscientific panel of general-purpose AI models that should possibly be classified as \ngeneral-purpose AI models with systemic risk, in addition to the monitoring activities of \nthe AI Office.\n(114)\nThe providers of general-purpose AI models presenting systemic risks should be subject, \nin addition to the obligations provided for providers of general-purpose AI models, to \nobligations aimed at identifying and mitigating those risks and ensuring an adequate \nlevel of cybersecurity protection, regardless of whether it is provided as a standalone \nmodel or embedded in an AI system or a product. To achieve those objectives, this \nRegulation should require providers to perform the necessary model evaluations, in \nparticular prior to its first placing on the market, including conducting and documenting \nadversarial testing of models, also, as appropriate, through internal or independent \nexternal testing. In addition, providers of general-purpose AI models with systemic risks \nshould continuously assess and mitigate systemic risks, including for example by putting \nin place risk-management policies, such as accountability and governance processes, \nimplementing post-market monitoring, taking appropriate measures along the entire \nmodel\u2019s lifecycle and cooperating with relevant actors along the AI value chain.", "conversation_history": [], "metadata": {"question_type": "complex", "seed_document_id": 301, "topic": "Systemic Risks in AI"}}
{"id": "52e7f0f3-a72a-471e-9498-bb4b84eafb6c", "question": "As a startup developing AI solutions, I need clarity on the binding rules for AI systems. What specific risk-based approach should I follow to ensure compliance, considering the ethical guidelines from 2019?", "reference_answer": "The basis for the binding rules for AI systems is a clearly defined risk-based approach that tailors the type and content of such rules to the intensity and scope of the risks that AI systems can generate.", "reference_context": "Document 27: EN\nUnited in diversity\nEN\n(26)\nIn order to introduce a proportionate and effective set of binding rules for AI systems, a \nclearly defined risk-based approach should be followed. That approach should tailor the \ntype and content of such rules to the intensity and scope of the risks that AI systems can \ngenerate. It is therefore necessary to prohibit certain unacceptable AI practices, to lay \ndown requirements for high-risk AI systems and obligations for the relevant operators, and \nto lay down transparency obligations for certain AI systems.\n(27)\nWhile the risk-based approach is the basis for a proportionate and effective set of \nbinding rules, it is important to recall the 2019 Ethics guidelines for trustworthy AI \ndeveloped by the independent AI HLEG appointed by the Commission. In those \nguidelines, the AI HLEG developed seven non-binding ethical principles for AI which \nare intended to help ensure that AI is trustworthy and ethically sound. The seven \nprinciples include human agency and oversight; technical robustness and safety; privacy \nand data governance; transparency; diversity, non-discrimination and fairness; societal \nand environmental well-being and accountability. Without prejudice to the legally \nbinding requirements of this Regulation and any other applicable Union law, those \nguidelines contribute to the design of coherent, trustworthy and human-centric AI, in \nline with the Charter and with the values on which the Union is founded. According to \nthe guidelines of the AI HLEG, human agency and oversight means that AI systems are \ndeveloped and used as a tool that serves people, respects human dignity and personal \nautonomy, and that is functioning in a way that can be appropriately controlled and \noverseen by humans.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 27, "topic": "AI Regulation and Ethics"}}
{"id": "bc146c69-8332-4d2d-8f70-fffa411324f0", "question": "As a manager of critical infrastructure, which AI systems are high-risk, and what implications might their failure have on public safety?", "reference_answer": "AI systems intended to be used as safety components in the management and operation of critical digital infrastructure, road traffic, and the supply of water, gas, heating, and electricity are classified as high-risk.", "reference_context": "Document 62: EN\nUnited in diversity\nEN\n(55)\nAs regards the management and operation of critical infrastructure, it is appropriate to \nclassify as high-risk the AI systems intended to be used as safety components in the \nmanagement and operation of critical digital infrastructure as listed in point (8) of the \nAnnex to Directive (EU) 2022/2557, road traffic and the supply of water, gas, heating and \nelectricity, since their failure or malfunctioning may put at risk the life and health of \npersons at large scale and lead to appreciable disruptions in the ordinary conduct of social \nand economic activities. Safety components of critical infrastructure, including critical \ndigital infrastructure, are systems used to directly protect the physical integrity of critical \ninfrastructure or the health and safety of persons and property but which are not \nnecessary in order for the system to function. The failure or malfunctioning of such \ncomponents might directly lead to risks to the physical integrity of critical infrastructure \nand thus to risks to health and safety of persons and property. Components intended to \nbe used solely for cybersecurity purposes should not qualify as safety components. \nExamples of safety components of such critical infrastructure may include systems for \nmonitoring water pressure or fire alarm controlling systems in cloud computing centres.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 62, "topic": "High-Risk AI Systems"}}
{"id": "6984ef0f-38d4-40cf-9eb1-935b7a1ae908", "question": "As a Member State considering compliance with the EU AI Act, what specific fees might I incur for expert advice under Article 69, and how should I prepare for potential budget impacts?", "reference_answer": "Member States may be required to pay fees for the advice and support provided by the experts.", "reference_context": "Document 351: EN\nUnited in diversity\nEN\nArticle 69\nAccess to the pool of experts by the Member States\n1.\nMember States may call upon experts of the scientific panel to support their enforcement \nactivities under this Regulation.\n2.\nThe Member States may be required to pay fees for the advice and support provided by \nthe experts. The structure and the level of fees as well as the scale and structure of \nrecoverable costs shall be set out in the implementing act referred to in Article 68(1), \ntaking into account the objectives of the adequate implementation of this Regulation, \ncost-effectiveness and the necessity of ensuring effective access to experts for all \nMember States.\n3.\nThe Commission shall facilitate timely access to the experts by the Member States, as \nneeded, and ensure that the combination of support activities carried out by Union AI \ntesting support pursuant to Article 84 and experts pursuant to this Article is efficiently \norganised and provides the best possible added value.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 351, "topic": "AI Regulation and Governance"}}
{"id": "54061039-3378-4ce1-8857-6f5e655717ba", "question": "As a business developing AI solutions, how does this Regulation ensure my compliance while protecting health and rights, and what risks might I face if I fail to adhere to its requirements?", "reference_answer": "The purpose of this Regulation is to improve the functioning of the internal market and promote the uptake of human-centric and trustworthy artificial intelligence (AI), while ensuring a high level of protection of health, safety, fundamental rights enshrined in the Charter, including democracy, the rule of law and environmental protection, against the harmful effects of AI systems in the Union and supporting innovation.", "reference_context": "Document 172: EN\nUnited in diversity\nEN\nCHAPTER I\nGENERAL PROVISIONS\nArticle 1\nSubject matter`\n1.\nThe purpose of this Regulation is to improve the functioning of the internal market and \npromote the uptake of human-centric and trustworthy artificial intelligence (AI), while \nensuring a high level of protection of health, safety, fundamental rights enshrined in the \nCharter, including democracy, the rule of law and environmental protection, against the \nharmful effects of AI systems in the Union and supporting innovation.\n2.\nThis Regulation lays down:\n(a)\nharmonised rules for the placing on the market, the putting into service, and the use \nof AI systems in the Union;\n(b)\nprohibitions of certain AI practices;\n(c)\nspecific requirements for high-risk AI systems and obligations for operators of such \nsystems;\n\nDocument 2: EN\nUnited in diversity\nEN\nWhereas:\n(1)\nThe purpose of this Regulation is to improve the functioning of the internal market by \nlaying down a uniform legal framework in particular for the development, the placing on \nthe market, the putting into service and the use of artificial intelligence systems (AI \nsystems) in the Union, in accordance with Union values, to promote the uptake of human \ncentric and trustworthy artificial intelligence (AI) while ensuring a high level of \nprotection of health, safety, fundamental rights as enshrined in the Charter of \nFundamental Rights of the European Union (the \u2018Charter\u2019), including democracy, the \nrule of law and environmental protection, to protect against the harmful effects of AI \nsystems in the Union, and to support innovation. This Regulation ensures the free \nmovement, cross-border, of AI-based goods and services, thus preventing Member States \nfrom imposing restrictions on the development, marketing and use of AI systems, unless \nexplicitly authorised by this Regulation.\n(2)\nThis Regulation should be applied in accordance with the values of the Union enshrined \nas in the Charter, facilitating the protection of natural persons, undertakings, \ndemocracy, the rule of law and environmental protection, while boosting innovation and \nemployment and making the Union a leader in the uptake of trustworthy AI.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 172, "topic": "AI Regulation and Compliance"}}
{"id": "a606c16a-9a21-4fe8-84f4-9d1a10f94902", "question": "As a developer of an AI system, what specific details must I include in the technical documentation per Article 11(1) to ensure compliance, especially regarding its intended purpose and interactions with other systems?", "reference_answer": "The technical documentation shall contain at least a general description of the AI system, including its intended purpose, the name of the provider, the version of the system, how it interacts with hardware or software, the versions of relevant software or firmware, and the description of all forms in which the AI system is placed on the market or put into service.", "reference_context": "Document 444: EN\nUnited in diversity\nEN\nANNEX IV\nTechnical documentation referred to in Article 11(1)\nThe technical documentation referred to in Article 11(1) shall contain at least the following \ninformation, as applicable to the relevant AI system:\n1.\nA general description of the AI system including:\n(a)\nits intended purpose, the name of the provider and the version of the system \nreflecting its relation to previous versions;\n(b)\nhow the AI system interacts with, or can be used to interact with, hardware or \nsoftware, including with other AI systems, that are not part of the AI system itself, \nwhere applicable;\n(c)\nthe versions of relevant software or firmware, and any requirements related to \nversion updates;\n(d)\nthe description of all the forms in which the AI system is placed on the market or put \ninto service, such as software packages embedded into hardware, downloads, or \nAPIs;", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 444, "topic": "AI Regulation and Governance"}}
{"id": "9eb1eb47-6b41-473c-97a9-2ac0a69c3047", "question": "As a developer of high-risk AI systems, what specific compliance requirements must I meet for market placement in the EU, especially considering potential impacts on public interests?", "reference_answer": "High-risk AI systems should only be placed on the Union market, put into service or used if they comply with certain mandatory requirements that ensure they do not pose unacceptable risks to important Union public interests as recognized and protected by Union law.", "reference_context": "Document 162: EN\nUnited in diversity\nEN\n(166)\nIt is important that AI systems related to products that are not high-risk in accordance with \nthis Regulation and thus are not required to comply with the requirements set out for high-\nrisk AI systems are nevertheless safe when placed on the market or put into service. To \ncontribute to this objective, Regulation (EU) 2023/988 of the European Parliament and of \nthe Council53 would apply as a safety net.\n(167)\nIn order to ensure trustful and constructive cooperation of competent authorities on Union \nand national level, all parties involved in the application of this Regulation should respect \nthe confidentiality of information and data obtained in carrying out their tasks, in \naccordance with Union or national law. They should carry out their tasks and activities \nin such a manner as to protect, in particular, intellectual property rights, confidential \nbusiness information and trade secrets, the effective implementation of this Regulation, \npublic and national security interests, the integrity of criminal and administrative \nproceedings, and the integrity of classified information.\n53\nRegulation (EU) 2023/988 of the European Parliament and of the Council of 10 May 2023 \non general product safety, amending Regulation (EU) No 1025/2012 of the European \nParliament and of the Council and Directive (EU) 2020/1828 of the European Parliament \nand the Council, and repealing Directive 2001/95/EC of the European Parliament and of \nthe Council and Council Directive 87/357/EEC (OJ L 135, 23.5.2023, p. 1).\n\nDocument 46: EN\nUnited in diversity\nEN\n(46)\nHigh-risk AI systems should only be placed on the Union market, put into service or used \nif they comply with certain mandatory requirements. Those requirements should ensure \nthat high-risk AI systems available in the Union or whose output is otherwise used in the \nUnion do not pose unacceptable risks to important Union public interests as recognised and \nprotected by Union law. On the basis of the New Legislative Framework, as clarified in \nthe Commission notice \u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules \n2022\u201d20, the general rule is that more than one legal act of Union harmonisation \nlegislation, such as Regulations (EU) 2017/74521 and (EU) 2017/74622 of the European \nParliament and of the Council or Directive 2006/42/EC of the European Parliament and \nof the Council23, may be applicable to one product, since the making available or putting \ninto service can take place only when the product complies with all applicable Union \nharmonisation legislation. To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 162, "topic": "Others"}}
{"id": "ce8928ba-060b-40a2-8f68-dd3d7b71bee2", "question": "As a deployer of a high-risk AI system, if I notice changes in assessment elements, what specific steps must I take to update my compliance and inform the market surveillance authority?", "reference_answer": "The deployer shall take the necessary steps to update the information.", "reference_context": "Document 256: EN\nUnited in diversity\nEN\n(d)\nthe specific risks of harm likely to have an impact on the categories of natural \npersons or groups of persons identified pursuant to point (c) of this paragraph, \ntaking into account the information given by the provider pursuant to Article 13;\n(e)\na description of the implementation of human oversight measures, according to the \ninstructions for use;\n(f)\nthe measures to be taken in the case of the materialisation of those risks, including \nthe arrangements for internal governance and complaint mechanisms.\n2.\nThe obligation laid down in paragraph 1 applies to the first use of the high-risk AI \nsystem. The deployer may, in similar cases, rely on previously conducted fundamental \nrights impact assessments or existing impact assessments carried out by provider. If, \nduring the use of the high-risk AI system, the deployer considers that any of the elements \nlisted in paragraph 1 has changed or is no longer up to date, the deployer shall take the \nnecessary steps to update the information.\n3.\nOnce the assessment referred to in paragraph 1 of this Article has been performed, the \ndeployer shall notify the market surveillance authority of its results, submitting the \nfilled-out template referred to in paragraph 5 of this Article as part of the notification. In \nthe case referred to in Article 46(1), deployers may be exempt from that obligation to \nnotify.\n\nDocument 100: EN\nUnited in diversity\nEN\nThe assessment should also include the identification of specific risks of harm likely to \nhave an impact on the fundamental rights of those persons or groups. While performing \nthis assessment, the deployer should take into account information relevant to a proper \nassessment of the impact, including but not limited to the information given by the \nprovider of the high-risk AI system in the instructions for use. In light of the risks \nidentified, deployers should determine measures to be taken in the case of a \nmaterialisation of those risks, including for example governance arrangements in that \nspecific context of use, such as arrangements for human oversight according to the \ninstructions of use or, complaint handling and redress procedures, as they could be \ninstrumental in mitigating risks to fundamental rights in concrete use-cases. After \nperforming that impact assessment, the deployer should notify the relevant market \nsurveillance authority. Where appropriate, to collect relevant information necessary to \nperform the impact assessment, deployers of high-risk AI system, in particular when AI \nsystems are used in the public sector, could involve relevant stakeholders, including the \nrepresentatives of groups of persons likely to be affected by the AI system, independent \nexperts, and civil society organisations in conducting such impact assessments and \ndesigning measures to be taken in the case of materialisation of the risks. The European \nArtificial Intelligence Office (AI Office) should develop a template for a questionnaire in \norder to facilitate compliance and reduce the administrative burden for deployers.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 256, "topic": "AI Regulation and Ethics"}}
{"id": "92e2873f-ec2e-41a8-8979-5ed36d0236c1", "question": "As a policymaker, which AI solutions should I prioritize to address social inequalities and environmental goals in my region?", "reference_answer": "Member States are encouraged to support and promote AI solutions that increase accessibility for persons with disabilities, tackle socio-economic inequalities, or meet environmental targets.", "reference_context": "Document 140: EN\nUnited in diversity\nEN\n(142)\nTo ensure that AI leads to socially and environmentally beneficial outcomes, Member \nStates are encouraged to support and promote research and development of AI solutions \nin support of socially and environmentally beneficial outcomes, such as AI-based \nsolutions to increase accessibility for persons with disabilities, tackle socio-economic \ninequalities, or meet environmental targets, by allocating sufficient resources, including \npublic and Union funding, and, where appropriate and provided that the eligibility and \nselection criteria are fulfilled, considering in particular projects which pursue such \nobjectives. Such projects should be based on the principle of interdisciplinary \ncooperation between AI developers, experts on inequality and non-discrimination, \naccessibility, consumer, environmental, and digital rights, as well as academics.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 140, "topic": "AI Regulation in the EU"}}
{"id": "c8224c64-f61d-43e5-9d58-27792419527a", "question": "As a developer of high-risk AI systems, what specific enforcement powers do market surveillance authorities have to ensure compliance, especially regarding financial services?", "reference_answer": "Market surveillance authorities should have all powers under this Regulation and Regulation (EU) 2019/1020 to enforce the requirements and obligations of this Regulation, including powers to carry out ex post market surveillance activities.", "reference_context": "Document 364: EN\nUnited in diversity\nEN\n5.\nWithout prejudice to the powers of market surveillance authorities under Article 14 of \nRegulation (EU) 2019/1020, for the purpose of ensuring the effective enforcement of \nthis Regulation, market surveillance authorities may exercise the powers referred to in \nArticle 14(4), points (d) and (j), of that Regulation remotely, as appropriate.\n6.\nFor high-risk AI systems placed on the market, put into service, or used by financial \ninstitutions regulated by Union financial services law, the market surveillance authority for \nthe purposes of this Regulation shall be the relevant national authority responsible for the \nfinancial supervision of those institutions under that legislation in so far as the placing on \nthe market, putting into service, or the use of the AI system is in direct connection with \nthe provision of those financial services.\n7.\nBy way of derogation from paragraph 6, in appropriate circumstances, and provided that \ncoordination is ensured, another relevant authority may be identified by the Member \nState as market surveillance authority for the purposes of this Regulation.\nNational market surveillance authorities supervising regulated credit institutions \nregulated under Directive 2013/36/EU, which are participating in the Single Supervisory \nMechanism established by Regulation (EU) No 1024/2013, should report, without delay, \nto the European Central Bank any information identified in the course of their market \nsurveillance activities that may be of potential interest for the prudential supervisory \ntasks of the European Central Bank specified in that Regulation.\n\nDocument 154: EN\nUnited in diversity\nEN\nThose competent authorities should have all powers under this Regulation and \nRegulation (EU) 2019/1020 to enforce the requirements and obligations of this \nRegulation, including powers to carry our ex post market surveillance activities that can \nbe integrated, as appropriate, into their existing supervisory mechanisms and procedures \nunder the relevant Union financial services law. It is appropriate to envisage that, when \nacting as market surveillance authorities under this Regulation, the national authorities \nresponsible for the supervision of credit institutions regulated under Directive \n2013/36/EU, which are participating in the Single Supervisory Mechanism established \nby Council Regulation (EU) No 1024/201352, should report, without delay, to the \nEuropean Central Bank any information identified in the course of their market \nsurveillance activities that may be of potential interest for the European Central Bank\u2019s \nprudential supervisory tasks as specified in that Regulation. \n52\nCouncil Regulation (EU) No 1024/2013 of 15 October 2013 conferring specific tasks on the \nEuropean Central Bank concerning policies relating to the prudential supervision of credit \ninstitutions (OJ L 287, 29.10.2013, p. 63).\n\nDocument 156: EN\nUnited in diversity\nEN\n(159)\nEach market surveillance authority for high-risk AI systems in the area of biometrics, as \nlisted in an annex to this Regulation insofar as those systems are used for the purposes \nof law enforcement, migration, asylum and border control management, or the \nadministration of justice and democratic processes, should have effective investigative \nand corrective powers, including at least the power to obtain access to all personal data \nthat are being processed and to all information necessary for the performance of its \ntasks. The market surveillance authorities should be able to exercise their powers by \nacting with complete independence. Any limitations of their access to sensitive \noperational data under this Regulation should be without prejudice to the powers \nconferred to them by Directive (EU) 2016/680. No exclusion on disclosing data to \nnational data protection authorities under this Regulation should affect the current or \nfuture powers of those authorities beyond the scope of this Regulation.\n(160)\nThe market surveillance authorities and the Commission should be able to propose joint \nactivities, including joint investigations, to be conducted by market surveillance \nauthorities or market surveillance authorities jointly with the Commission, that have the \naim of promoting compliance, identifying non-compliance, raising awareness and \nproviding guidance in relation to this Regulation with respect to specific categories of \nhigh-risk AI systems that are found to present a serious risk across two or more Member \nStates. Joint activities to promote compliance should be carried out in accordance with \nArticle 9 of Regulation (EU) 2019/1020. The AI Office should provide coordination \nsupport for joint investigations.\n\nDocument 150: EN\nUnited in diversity\nEN\n(156)\nIn order to ensure an appropriate and effective enforcement of the requirements and \nobligations set out by this Regulation, which is Union harmonisation legislation, the \nsystem of market surveillance and compliance of products established by Regulation (EU) \n2019/1020 should apply in its entirety. Market surveillance authorities designated \npursuant to this Regulation should have all enforcement powers laid down in this \nRegulation and in Regulation (EU) 2019/1020 and should exercise their powers and \ncarry out their duties independently, impartially and without bias. Although the majority \nof AI systems are not subject to specific requirements and obligations under this \nRegulation, market surveillance authorities may take measures in relation to all AI \nsystems when they present a risk in accordance with this Regulation. Due to the specific \nnature of Union institutions, agencies and bodies falling within the scope of this \nRegulation, it is appropriate to designate the European Data Protection Supervisor as a \ncompetent market surveillance authority for them. This should be without prejudice to \nthe designation of national competent authorities by the Member States. Market \nsurveillance activities should not affect the ability of the supervised entities to carry out \ntheir tasks independently, when such independence is required by Union law.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 364, "topic": "AI Regulation and Compliance"}}
{"id": "4035d586-e41e-4bd0-91d9-e22f454bc539", "question": "As a business developing AI tools, what specific prohibitions on biometric categorisation systems must I adhere to under the EU AI Act, especially regarding their use in public spaces?", "reference_answer": "The placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation systems that categorise individually natural persons based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation is prohibited.", "reference_context": "Document 196: EN\nUnited in diversity\nEN\n(g)\nthe placing on the market, the putting into service for this specific purpose, or the \nuse of biometric categorisation systems that categorise individually natural persons \nbased on their biometric data to deduce or infer their race, political opinions, trade \nunion membership, religious or philosophical beliefs, sex life or sexual \norientation; this prohibition does not cover any labelling or filtering of lawfully \nacquired biometric datasets, such as images, based on biometric data or \ncategorizing of biometric data in the area of law enforcement;\n(h)\nthe use of \u2018real-time\u2019 remote biometric identification systems in publicly accessible \nspaces for the purposes of law enforcement, \u258c unless and in so far as such use is \nstrictly necessary for one of the following objectives:\n(i)\nthe targeted search for specific \u258c victims of abduction, trafficking in human \nbeings or sexual exploitation of human beings, as well as the search for \nmissing persons;", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 196, "topic": "Biometric Data and AI"}}
{"id": "37dfd72c-9969-4820-8a4b-144fd7e7e73f", "question": "As a developer of high-risk AI systems, what specific powers should market surveillance authorities have to ensure compliance, especially regarding data access and independence?", "reference_answer": "Market surveillance authorities should have effective investigative and corrective powers, including at least the power to obtain access to all personal data that are being processed and to all information necessary for the performance of their tasks.", "reference_context": "Document 365: EN\nUnited in diversity\nEN\n8.\nFor high-risk AI systems listed in point 1 of Annex III to this Regulation, in so far as the \nsystems are used for law enforcement purposes, border management and justice and \ndemocracy, and for high-risk AI systems listed in points 6, 7 and 8 of Annex III to this \nRegulation, Member States shall designate as market surveillance authorities for the \npurposes of this Regulation either the competent data protection supervisory authorities \nunder Regulation (EU) 2016/679 or Directive (EU) 2016/680, or any other authority \ndesignated pursuant to the same conditions laid down in Articles 41 to 44 of Directive \n(EU) 2016/680. Market surveillance activities shall in no way affect the independence of \njudicial authorities, or otherwise interfere with their activities when acting in their \njudicial capacity.\n9.\nWhere Union institutions, bodies, offices or agencies fall within the scope of this \nRegulation, the European Data Protection Supervisor shall act as their market surveillance \nauthority, except in relation to the Court of Justice of the European Union acting in its \njudicial capacity.\n10.\nMember States shall facilitate coordination between market surveillance authorities \ndesignated under this Regulation and other relevant national authorities or bodies which \nsupervise the application of Union harmonisation legislation listed in Annex I, or in other \nUnion law, that might be relevant for the high-risk AI systems referred to in Annex III.\n\nDocument 156: EN\nUnited in diversity\nEN\n(159)\nEach market surveillance authority for high-risk AI systems in the area of biometrics, as \nlisted in an annex to this Regulation insofar as those systems are used for the purposes \nof law enforcement, migration, asylum and border control management, or the \nadministration of justice and democratic processes, should have effective investigative \nand corrective powers, including at least the power to obtain access to all personal data \nthat are being processed and to all information necessary for the performance of its \ntasks. The market surveillance authorities should be able to exercise their powers by \nacting with complete independence. Any limitations of their access to sensitive \noperational data under this Regulation should be without prejudice to the powers \nconferred to them by Directive (EU) 2016/680. No exclusion on disclosing data to \nnational data protection authorities under this Regulation should affect the current or \nfuture powers of those authorities beyond the scope of this Regulation.\n(160)\nThe market surveillance authorities and the Commission should be able to propose joint \nactivities, including joint investigations, to be conducted by market surveillance \nauthorities or market surveillance authorities jointly with the Commission, that have the \naim of promoting compliance, identifying non-compliance, raising awareness and \nproviding guidance in relation to this Regulation with respect to specific categories of \nhigh-risk AI systems that are found to present a serious risk across two or more Member \nStates. Joint activities to promote compliance should be carried out in accordance with \nArticle 9 of Regulation (EU) 2019/1020. The AI Office should provide coordination \nsupport for joint investigations.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 365, "topic": "AI Regulation and Compliance"}}
{"id": "95dcf97a-56d1-48ba-b69c-bfa6bf6c796f", "question": "As a provider of high-risk AI systems, I need to know the minimum log retention period for compliance. Given that I'm also subject to national data protection laws, how should I determine the appropriate duration for keeping these logs?", "reference_answer": "The logs shall be kept for a period of at least six months, unless provided otherwise in applicable Union or national law.", "reference_context": "Document 236: EN\nUnited in diversity\nEN\nArticle 19\nAutomatically generated logs\n1.\nProviders of high-risk AI systems shall keep the logs referred to in Article 12(1), \nautomatically generated by their high-risk AI systems, to the extent such logs are under \ntheir control. Without prejudice to applicable Union or national law, the logs shall be \nkept for a period \u258c appropriate to the intended purpose of the high-risk AI system, of at \nleast six months, unless provided otherwise in the applicable Union or national law, in \nparticular in Union law on the protection of personal data.\n2.\nProviders that are financial institutions subject to requirements regarding their internal \ngovernance, arrangements or processes under Union financial services law shall \nmaintain the logs automatically generated by their high-risk AI systems as part of the \ndocumentation kept under the relevant financial services law.\n\nDocument 251: EN\nUnited in diversity\nEN\n6.\nDeployers of high-risk AI systems shall keep the logs automatically generated by that \nhigh-risk AI system \u258c to the extent such logs are under their control, \u258c for a period \u258c \nappropriate to the intended purpose of the high-risk AI system, of at least six months, \nunless provided otherwise in applicable Union or national law, in particular in Union law \non the protection of personal data.\nDeployers that are financial institutions subject to requirements regarding their internal \ngovernance, arrangements or processes under Union financial services law shall \nmaintain the logs as part of the documentation kept pursuant to the relevant Union \nfinancial service law.\n7.\nBefore putting into service or using a high-risk AI system at the workplace, deployers \nwho are employers shall inform workers\u2019 representatives and the affected workers that \nthey will be subject to the use of the high-risk AI system. This information shall be \nprovided, where applicable, in accordance with the rules and procedures laid down in \nUnion and national law and practice on information of workers and their \nrepresentatives.\n8.\nDeployers of high-risk AI systems that are public authorities, or Union institutions, \nbodies, offices or agencies shall comply with the registration obligations referred to in \nArticle 49. When such deployers find that the high-risk AI system that they envisage \nusing has not been registered in the EU database referred to in Article 71, they shall not \nuse that system and shall inform the provider or the distributor.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 236, "topic": "High-Risk AI Regulation"}}
{"id": "57610873-2fc0-4e8c-8c06-771a05ad67e3", "question": "As a law enforcement agency, what specific authorizations and assessments are needed for using real-time biometric identification in public spaces?", "reference_answer": "Each use for the purposes of law enforcement of a \u2018real-time\u2019 remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or an independent administrative authority whose decision is binding of the Member State in which the use is to take place.", "reference_context": "Document 200: EN\nUnited in diversity\nEN\n4.\nWithout prejudice to paragraph 3, each use of a \u2018real-time\u2019 remote biometric \nidentification system in publicly accessible spaces for law enforcement purposes shall be \nnotified to the relevant market surveillance authority and the national data protection \nauthority in accordance with the national rules referred to in paragraph 5. The \nnotification shall, as a minimum, contain the information specified under paragraph 6 \nand shall not include sensitive operational data.\n5.\nA Member State may decide to provide for the possibility to fully or partially authorise the \nuse of \u2018real-time\u2019 remote biometric identification systems in publicly accessible spaces for \nthe purposes of law enforcement within the limits and under the conditions listed in \nparagraph 1, first subparagraph, point (h), and paragraphs 2 and 3. \u258c Member States \nconcerned shall lay down in their national law the necessary detailed rules for the request, \nissuance and exercise of, as well as supervision and reporting relating to, the \nauthorisations referred to in paragraph 3. Those rules shall also specify in respect of which \nof the objectives listed in paragraph 1, first subparagraph, point (h), including which of the \ncriminal offences referred to in point (h)(iii) thereof, the competent authorities may be \nauthorised to use those systems for the purposes of law enforcement. Member States shall \nnotify those rules to the Commission at the latest 30 days following the adoption thereof. \nMember States may introduce, in accordance with Union law, more restrictive laws on \nthe use of remote biometric identification systems.\n\nDocument 198: EN\nUnited in diversity\nEN\n2.\nThe use of \u2018real-time\u2019 remote biometric identification systems in publicly accessible spaces \nfor the purposes of law enforcement for any of the objectives referred to in paragraph 1, \nfirst subparagraph, point (h), shall be deployed for the purposes set out in that point only \nto confirm the identity of the specifically targeted individual, and it shall take into \naccount the following elements:\n(a)\nthe nature of the situation giving rise to the possible use, in particular the seriousness, \nprobability and scale of the harm that would be caused if the system were not used;\n(b)\nthe consequences of the use of the system for the rights and freedoms of all persons \nconcerned, in particular the seriousness, probability and scale of those consequences.\nIn addition, the use of \u2018real-time\u2019 remote biometric identification systems in publicly \naccessible spaces for the purposes of law enforcement for any of the objectives referred to \nin paragraph 1, first subparagraph, point (h), of this Article shall comply with necessary \nand proportionate safeguards and conditions in relation to the use in accordance with the \nnational law authorising the use thereof, in particular as regards the temporal, geographic \nand personal limitations. The use of the \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be authorised only if the law enforcement authority \nhas completed a fundamental rights impact assessment as provided for in Article 27 and \nhas registered the system in the EU database according to Article 49. However, in duly \njustified cases of urgency, the use of such systems may be commenced without the \nregistration in the EU database, provided that such registration is completed without \nundue delay.\n\nDocument 199: EN\nUnited in diversity\nEN\n3.\nFor the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each \u258c use \nfor the purposes of law enforcement of a \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be subject to a prior authorisation granted by a judicial \nauthority or \u258c an independent administrative authority whose decision is binding of the \nMember State in which the use is to take place, issued upon a reasoned request and in \naccordance with the detailed rules of national law referred to in paragraph 5. However, in a \nduly justified situation of urgency, the use of such system may be commenced without an \nauthorisation provided that such authorisation is requested without undue delay, at the \nlatest within 24 hours. If such authorisation is rejected, the use shall be stopped with \nimmediate effect and all the data, as well as the results and outputs of that use shall be \nimmediately discarded and deleted.\nThe competent judicial authority or an independent administrative authority whose \ndecision is binding shall grant the authorisation only where it is satisfied, on the basis of \nobjective evidence or clear indications presented to it, that the use of the \u2018real-time\u2019 remote \nbiometric identification system concerned is necessary for, and proportionate to, achieving \none of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in \nthe request and, in particular, remains limited to what is strictly necessary concerning \nthe period of time as well as the geographic and personal scope. In deciding on the \nrequest, that authority shall take into account the elements referred to in paragraph 2. No \ndecision that produces an adverse legal effect on a person may be taken based solely on \nthe output of the \u2018real-time\u2019 remote biometric identification system.\n\nDocument 201: EN\nUnited in diversity\nEN\n6.\nNational market surveillance authorities and the national data protection authorities of \nMember States that have been notified of the use of \u2018real-time\u2019 remote biometric \nidentification systems in publicly accessible spaces for law enforcement purposes \npursuant to paragraph 4 shall submit to the Commission annual reports on such use. \nFor that purpose, the Commission shall provide Member States and national market \nsurveillance and data protection authorities with a template, including information on \nthe number of the decisions taken by competent judicial authorities or an independent \nadministrative authority whose decision is binding upon requests for authorisations in \naccordance with paragraph 3 and their result.\n7.\nThe Commission shall publish annual reports on the use of real-time remote biometric \nidentification systems in publicly accessible spaces for law enforcement purposes, based \non aggregated data in Member States on the basis of the annual reports referred to in \nparagraph 6. Those annual reports shall not include sensitive operational data of the \nrelated law enforcement activities.\n8.\nThis Article shall not affect the prohibitions that apply where an AI practice infringes \nother Union law.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 200, "topic": "Biometric Data and AI"}}
{"id": "4b5b98ee-860a-4a87-a520-f73dc36e4a73", "question": "As a Member State, what immediate actions must I take if the Commission justifies a measure on an AI system, especially regarding market withdrawal and compliance?", "reference_answer": "All Member States shall ensure that they take appropriate restrictive measures in respect of the AI system concerned, such as requiring the withdrawal of the AI system from their market without undue delay, and shall inform the Commission accordingly.", "reference_context": "Document 382: EN\nUnited in diversity\nEN\n2.\nWhere the Commission considers the measure taken by the relevant Member State to be \njustified, all Member States shall ensure that they take appropriate restrictive measures in \nrespect of the AI system concerned, such as requiring the withdrawal of the AI system \nfrom their market without undue delay, and shall inform the Commission accordingly. \nWhere the Commission considers the national measure to be unjustified, the Member State \nconcerned shall withdraw the measure and shall inform the Commission accordingly.\n3.\nWhere the national measure is considered justified and the non-compliance of the AI \nsystem is attributed to shortcomings in the harmonised standards or common specifications \nreferred to in Articles 40 and 41 of this Regulation, the Commission shall apply the \nprocedure provided for in Article 11 of Regulation (EU) No 1025/2012.\nArticle 82\nCompliant AI systems which present a risk\n1.\nWhere, having performed an evaluation under Article 79, after consulting the relevant \nnational public authority referred to in Article 77(1), the market surveillance authority of \na Member State finds that although a high-risk AI system complies with this Regulation, it \nnevertheless presents a risk to the health or safety of persons, \u258c to fundamental rights, or to \nother aspects of public interest protection, it shall require the relevant operator to take all \nappropriate measures to ensure that the AI system concerned, when placed on the market \nor put into service, no longer presents that risk without undue delay, within a \u258c period \u258c it \nmay prescribe.\n\nDocument 376: EN\nUnited in diversity\nEN\n2.\nWhere the market surveillance authority of a Member State has sufficient reason to \nconsider an AI system to present a risk as referred to in paragraph 1 of this Article, it shall \ncarry out an evaluation of the AI system concerned in respect of its compliance with all the \nrequirements and obligations laid down in this Regulation. Particular attention shall be \ngiven to AI systems presenting a risk to vulnerable groups. Where risks to fundamental \nrights are identified, the market surveillance authority shall also inform and fully \ncooperate with the relevant national public authorities or bodies referred to in Article \n77(1). The relevant operators shall cooperate as necessary with the market surveillance \nauthority and with the other national public authorities or bodies referred to in Article \n77(1).\nWhere, in the course of that evaluation, the market surveillance authority or, where \napplicable the market surveillance authority in cooperation with the national public \nauthority referred to in Article 77(1), finds that the AI system does not comply with the \nrequirements and obligations laid down in this Regulation, it shall without undue delay \nrequire the relevant operator to take all appropriate corrective actions to bring the AI \nsystem into compliance, to withdraw the AI system from the market, or to recall it within a \nperiod the market surveillance authority may prescribe, and in any event within the \nshorter of 15 working days, or as provided for in the relevant Union harmonisation \nlegislation.\nThe market surveillance authority shall inform the relevant notified body accordingly. \nArticle 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the \nsecond subparagraph of this paragraph.\n3.\nWhere the market surveillance authority considers that the non-compliance is not restricted \nto its national territory, it shall inform the Commission and the other Member States \nwithout undue delay of the results of the evaluation and of the actions which it has \nrequired the operator to take.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 382, "topic": "AI Regulation and Compliance"}}
{"id": "005bc2eb-9a21-407e-a071-0a9d93ee4c7d", "question": "As a provider of high-risk AI systems, what specific elements must I include in my post-market monitoring plan to comply with the EU regulation, considering I already have an existing system in place?", "reference_answer": "The post-market monitoring system shall be based on a post-market monitoring plan, which shall be part of the technical documentation referred to in Annex IV.", "reference_context": "Document 358: EN\nUnited in diversity\nEN\n3.\nThe post-market monitoring system shall be based on a post-market monitoring plan. The \npost-market monitoring plan shall be part of the technical documentation referred to in \nAnnex IV. The Commission shall adopt an implementing act laying down detailed \nprovisions establishing a template for the post-market monitoring plan and the list of \nelements to be included in the plan by \u2026 [18 months after the entry into force of this \nRegulation]. That implementing act shall be adopted in accordance with the \nexamination procedure referred to in Article 98(2).\n4.\nFor high-risk AI systems covered by the Union harmonisation legislation listed in Section \nA of Annex I, where a post-market monitoring system and plan are already established \nunder that legislation, in order to ensure consistency, avoid duplications and minimise \nadditional burdens, providers shall have a choice of integrating, as appropriate, the \nnecessary elements described in paragraphs 1, 2 and 3 using the template referred in \nparagraph 3 into systems and plans already existing under that legislation, provided that \nit achieves an equivalent level of protection.\nThe first subparagraph of this paragraph shall also apply \u258cto high-risk AI systems referred \nto in point 5 of Annex III placed on the market or put into service by financial institutions \nthat are subject to requirements under Union financial services law regarding their \ninternal governance, arrangements or processes.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 358, "topic": "High-Risk AI Regulation"}}
{"id": "d95cf655-de0e-4cd0-ad74-e7f7cbf09913", "question": "After a market surveillance authority finds issues with my AI system, what corrective actions must I take and what timeline should I follow to comply?", "reference_answer": "The provider or other relevant operator shall ensure that corrective action is taken in respect of all the AI systems concerned that it has made available on the Union market within the timeline prescribed by the market surveillance authority.", "reference_context": "Document 383: EN\nUnited in diversity\nEN\n2.\nThe provider or other relevant operator shall ensure that corrective action is taken in \nrespect of all the AI systems concerned that it has made available on the Union market \nwithin the timeline prescribed by the market surveillance authority of the Member State \nreferred to in paragraph 1.\n3.\nThe Member States shall immediately inform the Commission and the other Member \nStates of a finding under paragraph 1. That information shall include all available details, \nin particular the data necessary for the identification of the AI system concerned, the origin \nand the supply chain of the AI system, the nature of the risk involved and the nature and \nduration of the national measures taken.\n4.\nThe Commission shall without undue delay enter into consultation with the Member States \nconcerned and the relevant operators, and shall evaluate the national measures taken. On \nthe basis of the results of that evaluation, the Commission shall decide whether the \nmeasure is justified and, where necessary, propose other appropriate measures.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 383, "topic": "AI Regulation and Compliance"}}
{"id": "4cee0637-8275-4f69-82c8-ab6c8e7f9b9c", "question": "As a law enforcement agency, how can we apply remote biometric identification for serious criminal offences while ensuring compliance with the EU Regulation, especially considering potential risks to privacy and civil liberties?", "reference_answer": "The Regulation states that the list of criminal offences provided is based on the 32 criminal offences listed in the Council Framework Decision 2002/584/JHA, and that the recourse to \u2018real-time\u2019 remote biometric identification could be necessary and proportionate to varying degrees for the practical pursuit of the localisation or identification of a perpetrator or suspect of the different criminal offences listed.", "reference_context": "Document 35: EN\nUnited in diversity\nEN\nMoreover, the list of criminal offences provided in an annex to this Regulation is based \non the 32 criminal offences listed in the Council Framework Decision 2002/584/JHA18, \ntaking into account that some of those offences are, in practice, likely to be more relevant \nthan others, in that the recourse to \u2018real-time\u2019 remote biometric identification could, \nforeseeably, be necessary and proportionate to highly varying degrees for the practical \npursuit of the localisation or identification of a perpetrator or suspect of the different \ncriminal offences listed and having regard to the likely differences in the seriousness, \nprobability and scale of the harm or possible negative consequences. An imminent threat \nto life or the physical safety of natural persons could also result from a serious \ndisruption of critical infrastructure, as defined in Article 2, point (4) of Directive (EU) \n2022/2557 of the European Parliament and of the Council19, where the disruption or \ndestruction of such critical infrastructure would result in an imminent threat to life or \nthe physical safety of a person, including through serious harm to the provision of basic \nsupplies to the population or to the exercise of the core function of the State. In addition, \nthis Regulation should preserve the ability for law enforcement, border control, \nimmigration or asylum authorities to carry out identity checks in the presence of the \nperson concerned in accordance with the conditions set out in Union and national law \nfor such checks. In particular, law enforcement, border control, immigration or asylum \nauthorities should be able to use information systems, in accordance with Union or \nnational law, to identify persons who, during an identity check, either refuse to be \nidentified or are unable to state or prove their identity, without being required by this \nRegulation to obtain prior authorisation. This could be, for example, a person involved \nin a crime, being unwilling, or unable due to an accident or a medical condition, to \ndisclose their identity to law enforcement authorities.\n18\nCouncil Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest \nwarrant and the surrender procedures between Member States (OJ L 190, 18.7.2002, p. \n1).", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 35, "topic": "Biometric Data and AI"}}
{"id": "89afaced-0ee0-49dc-92a6-1745ef0da25c", "question": "As a developer of high-risk AI systems, what specific powers should I expect market surveillance authorities to have, especially regarding data access and compliance investigations?", "reference_answer": "Market surveillance authorities should have effective investigative and corrective powers, including at least the power to obtain access to all personal data that are being processed and to all information necessary for the performance of their tasks.", "reference_context": "Document 365: EN\nUnited in diversity\nEN\n8.\nFor high-risk AI systems listed in point 1 of Annex III to this Regulation, in so far as the \nsystems are used for law enforcement purposes, border management and justice and \ndemocracy, and for high-risk AI systems listed in points 6, 7 and 8 of Annex III to this \nRegulation, Member States shall designate as market surveillance authorities for the \npurposes of this Regulation either the competent data protection supervisory authorities \nunder Regulation (EU) 2016/679 or Directive (EU) 2016/680, or any other authority \ndesignated pursuant to the same conditions laid down in Articles 41 to 44 of Directive \n(EU) 2016/680. Market surveillance activities shall in no way affect the independence of \njudicial authorities, or otherwise interfere with their activities when acting in their \njudicial capacity.\n9.\nWhere Union institutions, bodies, offices or agencies fall within the scope of this \nRegulation, the European Data Protection Supervisor shall act as their market surveillance \nauthority, except in relation to the Court of Justice of the European Union acting in its \njudicial capacity.\n10.\nMember States shall facilitate coordination between market surveillance authorities \ndesignated under this Regulation and other relevant national authorities or bodies which \nsupervise the application of Union harmonisation legislation listed in Annex I, or in other \nUnion law, that might be relevant for the high-risk AI systems referred to in Annex III.\n\nDocument 156: EN\nUnited in diversity\nEN\n(159)\nEach market surveillance authority for high-risk AI systems in the area of biometrics, as \nlisted in an annex to this Regulation insofar as those systems are used for the purposes \nof law enforcement, migration, asylum and border control management, or the \nadministration of justice and democratic processes, should have effective investigative \nand corrective powers, including at least the power to obtain access to all personal data \nthat are being processed and to all information necessary for the performance of its \ntasks. The market surveillance authorities should be able to exercise their powers by \nacting with complete independence. Any limitations of their access to sensitive \noperational data under this Regulation should be without prejudice to the powers \nconferred to them by Directive (EU) 2016/680. No exclusion on disclosing data to \nnational data protection authorities under this Regulation should affect the current or \nfuture powers of those authorities beyond the scope of this Regulation.\n(160)\nThe market surveillance authorities and the Commission should be able to propose joint \nactivities, including joint investigations, to be conducted by market surveillance \nauthorities or market surveillance authorities jointly with the Commission, that have the \naim of promoting compliance, identifying non-compliance, raising awareness and \nproviding guidance in relation to this Regulation with respect to specific categories of \nhigh-risk AI systems that are found to present a serious risk across two or more Member \nStates. Joint activities to promote compliance should be carried out in accordance with \nArticle 9 of Regulation (EU) 2019/1020. The AI Office should provide coordination \nsupport for joint investigations.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 365, "topic": "AI Regulation and Compliance"}}
{"id": "06548ca4-5805-4d7e-ab3b-23fd76238e06", "question": "As a notifying authority, how can I ensure impartiality with conformity assessment bodies while maintaining confidentiality and avoiding conflicts of interest?", "reference_answer": "Notifying authorities shall be established, organised and operated in such a way that no conflict of interest arises with conformity assessment bodies, and that the objectivity and impartiality of their activities are safeguarded.", "reference_context": "Document 258: EN\nUnited in diversity\nEN\n2.\nMember States may decide that the assessment and monitoring referred to in \nparagraph 1 is to be carried out by a national accreditation body within the meaning of, \nand in accordance with, Regulation (EC) No 765/2008 \u258c .\n3.\nNotifying authorities shall be established, organised and operated in such a way that no \nconflict of interest arises with conformity assessment bodies, and that the objectivity and \nimpartiality of their activities are safeguarded.\n4.\nNotifying authorities shall be organised in such a way that decisions relating to the \nnotification of conformity assessment bodies are taken by competent persons different \nfrom those who carried out the assessment of those bodies.\n5.\nNotifying authorities shall offer or provide neither any activities that conformity \nassessment bodies perform, nor any consultancy services on a commercial or competitive \nbasis.\n6.\nNotifying authorities shall safeguard the confidentiality of the information that they obtain, \nin accordance with Article 78.\n7.\nNotifying authorities shall have an adequate number of competent personnel at their \ndisposal for the proper performance of their tasks. Competent personnel shall have the \nnecessary expertise, where applicable, for their function, in fields such as information \ntechnologies, AI and law, including the supervision of fundamental rights.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 258, "topic": "Notified Bodies Regulations"}}
{"id": "056dbb2e-9ecb-4add-b003-5b0a2da2fa9e", "question": "As a Member State, what specific steps must I take if the Commission deems action against an AI system justified, considering my current compliance status?", "reference_answer": "All Member States shall ensure that they take appropriate restrictive measures in respect of the AI system concerned, such as requiring the withdrawal of the AI system from their market without undue delay, and shall inform the Commission accordingly.", "reference_context": "Document 382: EN\nUnited in diversity\nEN\n2.\nWhere the Commission considers the measure taken by the relevant Member State to be \njustified, all Member States shall ensure that they take appropriate restrictive measures in \nrespect of the AI system concerned, such as requiring the withdrawal of the AI system \nfrom their market without undue delay, and shall inform the Commission accordingly. \nWhere the Commission considers the national measure to be unjustified, the Member State \nconcerned shall withdraw the measure and shall inform the Commission accordingly.\n3.\nWhere the national measure is considered justified and the non-compliance of the AI \nsystem is attributed to shortcomings in the harmonised standards or common specifications \nreferred to in Articles 40 and 41 of this Regulation, the Commission shall apply the \nprocedure provided for in Article 11 of Regulation (EU) No 1025/2012.\nArticle 82\nCompliant AI systems which present a risk\n1.\nWhere, having performed an evaluation under Article 79, after consulting the relevant \nnational public authority referred to in Article 77(1), the market surveillance authority of \na Member State finds that although a high-risk AI system complies with this Regulation, it \nnevertheless presents a risk to the health or safety of persons, \u258c to fundamental rights, or to \nother aspects of public interest protection, it shall require the relevant operator to take all \nappropriate measures to ensure that the AI system concerned, when placed on the market \nor put into service, no longer presents that risk without undue delay, within a \u258c period \u258c it \nmay prescribe.\n\nDocument 376: EN\nUnited in diversity\nEN\n2.\nWhere the market surveillance authority of a Member State has sufficient reason to \nconsider an AI system to present a risk as referred to in paragraph 1 of this Article, it shall \ncarry out an evaluation of the AI system concerned in respect of its compliance with all the \nrequirements and obligations laid down in this Regulation. Particular attention shall be \ngiven to AI systems presenting a risk to vulnerable groups. Where risks to fundamental \nrights are identified, the market surveillance authority shall also inform and fully \ncooperate with the relevant national public authorities or bodies referred to in Article \n77(1). The relevant operators shall cooperate as necessary with the market surveillance \nauthority and with the other national public authorities or bodies referred to in Article \n77(1).\nWhere, in the course of that evaluation, the market surveillance authority or, where \napplicable the market surveillance authority in cooperation with the national public \nauthority referred to in Article 77(1), finds that the AI system does not comply with the \nrequirements and obligations laid down in this Regulation, it shall without undue delay \nrequire the relevant operator to take all appropriate corrective actions to bring the AI \nsystem into compliance, to withdraw the AI system from the market, or to recall it within a \nperiod the market surveillance authority may prescribe, and in any event within the \nshorter of 15 working days, or as provided for in the relevant Union harmonisation \nlegislation.\nThe market surveillance authority shall inform the relevant notified body accordingly. \nArticle 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the \nsecond subparagraph of this paragraph.\n3.\nWhere the market surveillance authority considers that the non-compliance is not restricted \nto its national territory, it shall inform the Commission and the other Member States \nwithout undue delay of the results of the evaluation and of the actions which it has \nrequired the operator to take.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 382, "topic": "AI Regulation and Compliance"}}
{"id": "b0d78078-0059-480b-ae35-b1ca2817c5e2", "question": "As a provider of a general-purpose AI model, what specific factors will the Commission weigh when determining fines for potential compliance failures, especially regarding the duration and gravity of my situation?", "reference_answer": "The Commission considers the nature, gravity and duration of the infringement, taking due account of the principles of proportionality and appropriateness.", "reference_context": "Document 411: EN\nUnited in diversity\nEN\n(b)\nfailed to comply with a request for a document or for information pursuant to \nArticle 91, or supplied incorrect, incomplete or misleading information;\n(c)\nfailed to comply with a measure requested under Article 93;\n(d)\nfailed to make available to the Commission access to the general-purpose AI model \nor general-purpose AI model with systemic risk with a view to conducting an \nevaluation pursuant to Article 92.\nIn fixing the amount of the fine or periodic penalty payment, regard shall be had to the \nnature, gravity and duration of the infringement, taking due account of the principles of \nproportionality and appropriateness. The Commission shall also into account \ncommitments made in accordance with Article 93(3) or made in relevant codes of \npractice in accordance with Article 56.\n2.\nBefore adopting the decision pursuant to paragraph 1, the Commission shall \ncommunicate its preliminary findings to the provider of the general-purpose AI model \nand give it an opportunity to be heard.\n3.\nFines imposed in accordance with this Article shall be effective, proportionate and \ndissuasive.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 411, "topic": "AI Regulation in the EU"}}
{"id": "b4cca128-5a5c-42fb-bfb0-980b504f4be5", "question": "As a compliance officer for my organization, I need clarity on the role of designated representatives in ensuring regulatory consistency. Can you explain their purpose and how they interact with national authorities and stakeholders?", "reference_answer": "The designated representatives of the Member States are empowered to facilitate consistency and coordination between national competent authorities in their Member State regarding the implementation of this Regulation.", "reference_context": "Document 341: EN\nUnited in diversity\nEN\n(b)\nare designated as a single contact point vis-\u00e0-vis the Board and, where appropriate, \ntaking into account Member States\u2019 needs, as a single contact point for \nstakeholders;\n(c)\nare empowered to facilitate consistency and coordination between national \ncompetent authorities in their Member State as regards the implementation of this \nRegulation, including through the collection of relevant data and information for \nthe purpose of fulfilling their tasks on the Board.\n5.\nThe designated representatives of the Member States shall adopt the Board\u2019s rules of \nprocedure by a two-thirds majority. The rules of procedure shall, in particular, lay down \nprocedures for the selection process, the duration of the mandate of, and specifications \nof the tasks of, the Chair, detailed arrangements for voting, and the organisation of the \nBoard\u2019s activities and those of its sub-groups.\n6.\nThe Board shall establish two standing sub-groups to provide a platform for cooperation \nand exchange among market surveillance authorities and notifying authorities about \nissues related to market surveillance and notified bodies respectively.\nThe standing sub-group for market surveillance should act as the administrative \ncooperation group (ADCO) for this Regulation within the meaning of Article 30 of \nRegulation (EU) 2019/1020.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 341, "topic": "AI Regulation and Compliance"}}
{"id": "40c344f6-88bd-403b-87a5-923a1dfe3cf3", "question": "As a developer of a high-risk AI system, who should I designate as responsible for its market placing, considering potential legal implications?", "reference_answer": "A specific natural or legal person, defined as the provider, takes responsibility for the placing on the market or the putting into service of a high-risk AI system.", "reference_context": "Document 87: EN\nUnited in diversity\nEN\n(79)\nIt is appropriate that a specific natural or legal person, defined as the provider, takes \nresponsibility for the placing on the market or the putting into service of a high-risk AI \nsystem, regardless of whether that natural or legal person is the person who designed or \ndeveloped the system.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 87, "topic": "AI Regulation and Ethics"}}
{"id": "b4534ec2-9b17-4405-b9be-c0f2c1d95028", "question": "As a provider of a general-purpose AI model, what specific evaluation strategies should I implement to address systemic risks, considering my model's energy consumption and training resources?", "reference_answer": "A detailed description of the evaluation strategies, including evaluation results, on the basis of available public evaluation protocols and tools or otherwise of other evaluation methodologies. Evaluation strategies shall include evaluation criteria, metrics and the methodology on the identification of limitations.", "reference_context": "Document 470: EN\nUnited in diversity\nEN\n(d)\nthe computational resources used to train the model (e.g. number of floating point \noperations ), training time, and other relevant details related to the training;\n(e)\nknown or estimated energy consumption of the model. \nWith regard to point (e), where the energy consumption of the model is unknown, the \nenergy consumption may be based on information about computational resources \nused.\nSection 2\nAdditional information to be provided by providers of general-purpose AI models with systemic \nrisk\n1.\nA detailed description of the evaluation strategies, including evaluation results, on the \nbasis of available public evaluation protocols and tools or otherwise of other evaluation \nmethodologies. Evaluation strategies shall include evaluation criteria, metrics and the \nmethodology on the identification of limitations.\n2.\nWhere applicable, a detailed description of the measures put in place for the purpose of \nconducting internal and/or external adversarial testing (e.g., red teaming), model \nadaptations, including alignment and fine-tuning.\n3.\nWhere applicable, a detailed description of the system architecture explaining how \nsoftware components build or feed into each other and integrate into the overall \nprocessing.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 470, "topic": "Systemic Risks in AI"}}
{"id": "4dfc1372-d95b-40d0-8ad5-e42967a7ecb3", "question": "As a tech startup in the EU, I need to understand Directive 2014/53/EU's purpose. How does it impact my ability to market radio equipment, and what compliance risks should I be aware of?", "reference_answer": "The purpose of Directive 2014/53/EU is the harmonisation of the laws of the Member States relating to the making available on the market of radio equipment.", "reference_context": "Document 431: EN\nUnited in diversity\nEN\n6.\nDirective 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on \nthe harmonisation of the laws of the Member States relating to the making available on the \nmarket of radio equipment and repealing Directive 1999/5/EC (OJ L 153, 22.5.2014, \np. 62);\n7.\nDirective 2014/68/EU of the European Parliament and of the Council of 15 May 2014 on \nthe harmonisation of the laws of the Member States relating to the making available on the \nmarket of pressure equipment (OJ L 189, 27.6.2014, p. 164);\n8.\nRegulation (EU) 2016/424 of the European Parliament and of the Council of 9 March 2016 \non cableway installations and repealing Directive 2000/9/EC (OJ L 81, 31.3.2016, p. 1);\n9.\nRegulation (EU) 2016/425 of the European Parliament and of the Council of 9 March 2016 \non personal protective equipment and repealing Council Directive 89/686/EEC (OJ L 81, \n31.3.2016, p. 51);\n10.\nRegulation (EU) 2016/426 of the European Parliament and of the Council of 9 March 2016 \non appliances burning gaseous fuels and repealing Directive 2009/142/EC (OJ L 81, \n31.3.2016, p. 99);\n11.\nRegulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 \non medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and \nRegulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and \n93/42/EEC (OJ L 117, 5.5.2017, p. 1);", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 431, "topic": "EU Regulations and Directives"}}
{"id": "cc8d8a0a-f88a-47b1-9c21-ed2c7aa71558", "question": "As a project leader focused on socially beneficial AI, what principles should guide our interdisciplinary cooperation to ensure effective outcomes?", "reference_answer": "Such projects should be based on the principle of interdisciplinary cooperation between AI developers, experts on inequality and non-discrimination, accessibility, consumer, environmental, and digital rights, as well as academics.", "reference_context": "Document 140: EN\nUnited in diversity\nEN\n(142)\nTo ensure that AI leads to socially and environmentally beneficial outcomes, Member \nStates are encouraged to support and promote research and development of AI solutions \nin support of socially and environmentally beneficial outcomes, such as AI-based \nsolutions to increase accessibility for persons with disabilities, tackle socio-economic \ninequalities, or meet environmental targets, by allocating sufficient resources, including \npublic and Union funding, and, where appropriate and provided that the eligibility and \nselection criteria are fulfilled, considering in particular projects which pursue such \nobjectives. Such projects should be based on the principle of interdisciplinary \ncooperation between AI developers, experts on inequality and non-discrimination, \naccessibility, consumer, environmental, and digital rights, as well as academics.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 140, "topic": "AI Regulation in the EU"}}
{"id": "6c6d8d20-695b-47f7-927f-8bd605627b64", "question": "As a provider of general-purpose AI models, who specifically supervises and enforces Chapter V, and what implications does this have for my compliance efforts?", "reference_answer": "The Commission shall have exclusive powers to supervise and enforce Chapter V.", "reference_context": "Document 388: EN\nUnited in diversity\nEN\nSection 5\nSupervision, investigation, enforcement and monitoring in respect of \nproviders of general-purpose AI models\nArticle 88\nEnforcement of the obligations of providers of general-purpose AI models\n1.\nThe Commission shall have exclusive powers to supervise and enforce Chapter V, taking \ninto account the procedural guarantees under Article 94. The Commission shall entrust \nthe implementation of these tasks to the AI Office, without prejudice to the powers of \norganisation of the Commission and the division of competences between Member States \nand the Union based on the Treaties.\n2.\nWithout prejudice to Article 75(3), market surveillance authorities may request the \nCommission to exercise the powers laid down in this Section, where that is necessary \nand proportionate to assist with the fulfilment of their tasks under this Regulation.", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 388, "topic": "AI Regulation and Governance"}}
{"id": "7a40a739-ad43-4a6b-b674-950bf2ae23d8", "question": "As a business affected by this Regulation, can you clarify its main objective regarding AI and how it impacts my operations?", "reference_answer": "The objective of this Regulation is to improve the functioning of the internal market and to promote the uptake of human centric and trustworthy AI, while ensuring a high level of protection of health, safety, fundamental rights, including democracy, the rule of law and environmental protection against harmful effects of AI systems in the Union and supporting innovation.", "reference_context": "Document 168: EN\nUnited in diversity\nEN\n(175)\nIn order to ensure uniform conditions for the implementation of this Regulation, \nimplementing powers should be conferred on the Commission. Those powers should be \nexercised in accordance with Regulation (EU) No 182/2011 of the European Parliament \nand of the Council56.\n(176)\nSince the objective of this Regulation, namely to improve the functioning of the internal \nmarket and to promote the uptake of human centric and trustworthy AI, while ensuring a \nhigh level of protection of health, safety, fundamental rights enshrined in the Charter, \nincluding democracy, the rule of law and environmental protection against harmful effects \nof AI systems in the Union and supporting innovation, cannot be sufficiently achieved by \nthe Member States and can rather, by reason of the scale or effects of the action, be better \nachieved at Union level, the Union may adopt measures in accordance with the principle of \nsubsidiarity as set out in Article 5 TEU. In accordance with the principle of proportionality \nas set out in that Article, this Regulation does not go beyond what is necessary in order to \nachieve that objective.\n56\nRegulation (EU) No 182/2011 of the European Parliament and of the Council of 16 \nFebruary 2011 laying down the rules and general principles concerning mechanisms for \ncontrol by Member States of the Commission\u2019s exercise of implementing powers (OJ L 55, \n28.2.2011, p. 13).", "conversation_history": [], "metadata": {"question_type": "complex_situational", "seed_document_id": 168, "topic": "AI Regulation in the EU"}}
{"id": "f7d46f40-fe82-4046-bb4a-18c450af584e", "question": "What is the role of notifying authorities in ensuring objectivity and impartiality, and what specific requirements must they meet regarding their personnel?", "reference_answer": "Notifying authorities play a crucial role in maintaining objectivity and impartiality, ensuring that no conflict of interest arises with conformity assessment bodies. They must also have a sufficient number of competent personnel to effectively perform their tasks, thereby supporting their role in safeguarding the integrity of their activities.", "reference_context": "Document 258: EN\nUnited in diversity\nEN\n2.\nMember States may decide that the assessment and monitoring referred to in \nparagraph 1 is to be carried out by a national accreditation body within the meaning of, \nand in accordance with, Regulation (EC) No 765/2008 \u258c .\n3.\nNotifying authorities shall be established, organised and operated in such a way that no \nconflict of interest arises with conformity assessment bodies, and that the objectivity and \nimpartiality of their activities are safeguarded.\n4.\nNotifying authorities shall be organised in such a way that decisions relating to the \nnotification of conformity assessment bodies are taken by competent persons different \nfrom those who carried out the assessment of those bodies.\n5.\nNotifying authorities shall offer or provide neither any activities that conformity \nassessment bodies perform, nor any consultancy services on a commercial or competitive \nbasis.\n6.\nNotifying authorities shall safeguard the confidentiality of the information that they obtain, \nin accordance with Article 78.\n7.\nNotifying authorities shall have an adequate number of competent personnel at their \ndisposal for the proper performance of their tasks. Competent personnel shall have the \nnecessary expertise, where applicable, for their function, in fields such as information \ntechnologies, AI and law, including the supervision of fundamental rights.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the role of notifying authorities according to the context?", "answer": "Notifying authorities shall be established, organised and operated in such a way that no conflict of interest arises with conformity assessment bodies, and that the objectivity and impartiality of their activities are safeguarded."}, {"question": "What requirements must notifying authorities meet regarding personnel?", "answer": "Notifying authorities shall have an adequate number of competent personnel at their disposal for the proper performance of their tasks."}], "seed_document_id": 258, "topic": "Notified Bodies Regulations"}}
{"id": "87f1910a-ffc3-4d59-b8e3-df4e5ae13aeb", "question": "What actions should operators of high-risk AI systems take if their systems change significantly after the Regulation's application date, and how does this relate to the encouragement for providers to comply during the transitional period?", "reference_answer": "Operators of high-risk AI systems must ensure compliance with the Regulation only if significant changes occur after the application date, while providers are encouraged to voluntarily start complying with the Regulation during the transitional period. This highlights a proactive approach for providers, while operators must assess their systems' changes to determine compliance obligations.", "reference_context": "Document 169: EN\nUnited in diversity\nEN\n(177)\nIn order to ensure legal certainty, ensure an appropriate adaptation period for operators \nand avoid disruption to the market, including by ensuring continuity of the use of AI \nsystems, it is appropriate that this Regulation applies to the high-risk AI systems that \nhave been placed on the market or put into service before the general date of application \nthereof, only if, from that date, those systems are subject to significant changes in their \ndesign or intended purpose. It is appropriate to clarify that, in this respect, the concept of \nsignificant change should be understood as equivalent in substance to the notion of \nsubstantial modification, which is used with regard only to high-risk AI systems \npursuant to this Regulation. On an exceptional basis and in light of public \naccountability, operators of AI systems which are components of the large-scale IT \nsystems established by the legal acts listed in an annex to this Regulation and operators \nof high-risk AI systems that are intended to be used by public authorities should, \nrespectively, take the necessary steps to comply with the requirements of this Regulation \nby end of 2030 and by ... [ six years from the date of entry into force of this Regulation].\n(178)\nProviders of high-risk AI systems are encouraged to start to comply, on a voluntary \nbasis, with the relevant obligations of this Regulation already during the transitional \nperiod.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What should operators of high-risk AI systems do if their systems undergo significant changes after the Regulation's application date?", "answer": "Operators of high-risk AI systems that have been placed on the market or put into service before the general date of application must ensure compliance with the Regulation only if those systems are subject to significant changes in their design or intended purpose."}, {"question": "What is encouraged for providers of high-risk AI systems during the transitional period?", "answer": "Providers of high-risk AI systems are encouraged to start to comply, on a voluntary basis, with the relevant obligations of this Regulation already during the transitional period."}], "seed_document_id": 169, "topic": "High-Risk AI Regulation"}}
{"id": "58e9f38a-742d-4a8e-893b-eb20bdcfc9d0", "question": "What are the conditions for a software or data model to be classified as free and open-source under the EU AI Act, and what transparency obligations do providers of general-purpose AI models have when released under such a licence?", "reference_answer": "To be considered free and open-source under the EU AI Act, a software or data model must allow users to run, copy, distribute, study, change, and improve it, while crediting the original provider and respecting distribution terms. Additionally, providers of general-purpose AI models must produce a summary of the training content and ensure compliance with Union copyright law, highlighting the importance of both licensing conditions and transparency in fostering responsible AI usage.", "reference_context": "Document 104: EN\nUnited in diversity\nEN\n(102)\nSoftware and data, including models, released under a free and open-source licence that \nallows them to be openly shared and where users can freely access, use, modify and \nredistribute them or modified versions thereof, can contribute to research and \ninnovation in the market and can provide significant growth opportunities for the Union \neconomy. General-purpose AI models released under free and open-source licences \nshould be considered to ensure high levels of transparency and openness if their \nparameters, including the weights, the information on the model architecture, and the \ninformation on model usage are made publicly available. The licence should be \nconsidered to be free and open-source also when it allows users to run, copy, distribute, \nstudy, change and improve software and data, including models under the condition that \nthe original provider of the model is credited, the identical or comparable terms of \ndistribution are respected. \n(103)\nFree and open-source AI components covers the software and data, including models \nand general-purpose AI models, tools, services or processes of an AI system. Free and \nopen-source AI components can be provided through different channels, including their \ndevelopment on open repositories. For the purposes of this Regulation, AI components \nthat are provided against a price or otherwise monetised, including through the \nprovision of technical support or other services, including through a software platform, \nrelated to the AI component, or the use of personal data for reasons other than \nexclusively for improving the security, compatibility or interoperability of the software, \nwith the exception of transactions between microenterprises, should not benefit from the \nexceptions provided to free and open-source AI components. The fact of making AI \ncomponents available through open repositories should not, in itself, constitute a \nmonetisation.\n\nDocument 105: EN\nUnited in diversity\nEN\n(104)\nThe providers of general-purpose AI models that are released under a free and open-\nsource licence, and whose parameters, including the weights, the information on the \nmodel architecture, and the information on model usage, are made publicly available \nshould be subject to exceptions as regards the transparency-related requirements \nimposed on general-purpose AI models, unless they can be considered to present a \nsystemic risk, in which case the circumstance that the model is transparent and \naccompanied by an open-source license should not be considered to be a sufficient \nreason to exclude compliance with the obligations under this Regulation. In any case, \ngiven that the release of general-purpose AI models under free and open-source licence \ndoes not necessarily reveal substantial information on the data set used for the training \nor fine-tuning of the model and on how compliance of copyright law was thereby \nensured, the exception provided for general-purpose AI models from compliance with \nthe transparency-related requirements should not concern the obligation to produce a \nsummary about the content used for model training and the obligation to put in place a \npolicy to comply with Union copyright law, in particular to identify and comply with the \nreservation of rights pursuant to Article 4(3) of Directive (EU) 2019/790 of the \nEuropean Parliament and of the Council40.\n40\nDirective (EU) 2019/790 of the European Parliament and of the Council of 17 April 2019 on \ncopyright and related rights in the Digital Single Market and amending Directives 96/9/EC \nand 2001/29/EC (OJ L 130, 17.5.2019, p. 92).", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What are the conditions for a software or data model to be considered free and open-source under the EU AI Act?", "answer": "The licence should be considered to be free and open-source when it allows users to run, copy, distribute, study, change, and improve software and data, including models, under the condition that the original provider of the model is credited and the identical or comparable terms of distribution are respected."}, {"question": "What obligations do providers of general-purpose AI models released under a free and open-source licence have regarding transparency?", "answer": "Providers should produce a summary about the content used for model training and put in place a policy to comply with Union copyright law, even if they are subject to exceptions regarding other transparency-related requirements."}], "seed_document_id": 104, "topic": "Others"}}
{"id": "eddb0987-92e2-4c22-80f7-e133f7c4c84e", "question": "How do Member States utilize the experts of the scientific panel according to Article 69, and what role does the Commission play in facilitating access to these experts?", "reference_answer": "Member States can call upon the experts of the scientific panel to support their enforcement activities, while the Commission plays a crucial role by facilitating timely access to these experts as needed. This collaboration ensures that Member States have the necessary expertise to effectively implement regulations.", "reference_context": "Document 351: EN\nUnited in diversity\nEN\nArticle 69\nAccess to the pool of experts by the Member States\n1.\nMember States may call upon experts of the scientific panel to support their enforcement \nactivities under this Regulation.\n2.\nThe Member States may be required to pay fees for the advice and support provided by \nthe experts. The structure and the level of fees as well as the scale and structure of \nrecoverable costs shall be set out in the implementing act referred to in Article 68(1), \ntaking into account the objectives of the adequate implementation of this Regulation, \ncost-effectiveness and the necessity of ensuring effective access to experts for all \nMember States.\n3.\nThe Commission shall facilitate timely access to the experts by the Member States, as \nneeded, and ensure that the combination of support activities carried out by Union AI \ntesting support pursuant to Article 84 and experts pursuant to this Article is efficiently \norganised and provides the best possible added value.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What can Member States do with the experts of the scientific panel according to Article 69?", "answer": "Member States may call upon experts of the scientific panel to support their enforcement activities under this Regulation."}, {"question": "What role does the Commission play in relation to the access of experts for Member States?", "answer": "The Commission shall facilitate timely access to the experts by the Member States, as needed."}], "seed_document_id": 351, "topic": "AI Regulation and Governance"}}
{"id": "ba29f717-59df-4f33-8473-6a678146d93a", "question": "What obligations do employers have in informing workers about AI systems, and why is this information requirement particularly necessary for high-risk AI systems?", "reference_answer": "Employers are obligated to inform or consult workers about AI systems under Union or national law, including Directive 2002/14/EC, to ensure transparency and protect workers' rights. This information requirement is especially crucial for high-risk AI systems as it safeguards fundamental rights and ensures that workers and their representatives are aware of potential impacts on their work environment.", "reference_context": "Document 96: EN\nUnited in diversity\nEN\n(92)\nThis Regulation is without prejudice to obligations for employers to inform or to inform \nand consult workers or their representatives under Union or national law and practice, \nincluding Directive 2002/14/EC of the European Parliament and of the Council39, on \ndecisions to put into service or use AI systems. It remains necessary to ensure \ninformation of workers and their representatives on the planned deployment of high-risk \nAI systems at the workplace where the conditions for those information or information \nand consultation obligations in other legal instruments are not fulfilled. Moreover, such \ninformation right is ancillary and necessary to the objective of protecting fundamental \nrights that underlies this Regulation. Therefore, an information requirement to that \neffect should be laid down in this Regulation, without affecting any existing rights of \nworkers.\n39\nDirective 2002/14/EC of the European Parliament and of the Council of 11 March 2002 \nestablishing a general framework for informing and consulting employees in the European \nCommunity (OJ L 80, 23.3.2002, p. 29).", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What obligations do employers have regarding informing workers about AI systems?", "answer": "Employers are obligated to inform or consult workers or their representatives under Union or national law and practice, including Directive 2002/14/EC."}, {"question": "Why is the information requirement regarding high-risk AI systems necessary?", "answer": "The information requirement is necessary to protect fundamental rights and ensures that workers and their representatives are informed about the planned deployment of high-risk AI systems at the workplace."}], "seed_document_id": 96, "topic": "AI Regulation and Compliance"}}
{"id": "5bfeb6bd-f574-4b0d-8b0f-0cfdf7b6c176", "question": "What conditions define a product manufacturer as the provider of a high-risk AI system, and what are the mandatory requirements for high-risk AI systems in the Union?", "reference_answer": "A product manufacturer is considered the provider of a high-risk AI system if it is marketed or put into service under their name or trademark. Additionally, high-risk AI systems must comply with mandatory requirements to ensure they do not pose unacceptable risks to public interests, emphasizing the importance of accountability in both the manufacturer's role and the safety standards for AI systems in the Union.", "reference_context": "Document 247: EN\nUnited in diversity\nEN\n3.\nIn the case of high-risk AI systems that are safety components of products covered by the \nUnion harmonisation legislation listed in Section A of Annex I, the product manufacturer \nshall be considered to be the provider of the high-risk AI system, and shall be subject to \nthe obligations under Article 16 under either of the following circumstances:\n(a)\nthe high-risk AI system is placed on the market together with the product under the \nname or trademark of the product manufacturer;\n(b)\nthe high-risk AI system is put into service under the name or trademark of the \nproduct manufacturer after the product has been placed on the market.\n4.\nThe provider of a high-risk AI system and the third party that supplies an AI system, \ntools, services, components, or processes that are used or integrated in a high-risk AI \nsystem shall, by written agreement, specify the necessary information, capabilities, \ntechnical access and other assistance based on the generally acknowledged state of the \nart, in order to enable the provider of the high-risk AI system to fully comply with the \nobligations set out in this Regulation. This paragraph shall not apply to third parties \nmaking accessible to the public tools, services, processes, or components, other than \ngeneral-purpose AI models, under a free and open-source licence.\n\nDocument 91: EN\nUnited in diversity\nEN\n(84)\nTo ensure legal certainty, it is necessary to clarify that, under certain specific conditions, \nany distributor, importer, deployer or other third-party should be considered to be a \nprovider of a high-risk AI system and therefore assume all the relevant obligations. This \nwould be the case if that party puts its name or trademark on a high-risk AI system \nalready placed on the market or put into service, without prejudice to contractual \narrangements stipulating that the obligations are allocated otherwise. This would also be \nthe case if that party makes a substantial modification to a high-risk AI system that has \nalready been placed on the market or has already been put into service in a way that it \nremains a high-risk AI system in accordance with this Regulation, or if it modifies the \nintended purpose of an AI system, including a general-purpose AI system, which has not \nbeen classified as high-risk and has already been placed on the market or put into \nservice, in a way that the AI system becomes a high-risk AI system in accordance with \nthis Regulation. Those provisions should apply without prejudice to more specific \nprovisions established in certain Union harmonisation legislation based on the New \nLegislative Framework, together with which this Regulation should apply. For example, \nArticle 16(2) of Regulation (EU) 2017/745, establishing that certain changes should not \nbe considered to be modifications of a device that could affect its compliance with the \napplicable requirements, should continue to apply to high-risk AI systems that are \nmedical devices within the meaning of that Regulation.\n\nDocument 202: EN\nUnited in diversity\nEN\nCHAPTER III\nHIGH-RISK AI SYSTEMS\nSection 1\nClassification of AI systems as high-risk\nArticle 6\nClassification rules for high-risk AI systems\n1.\nIrrespective of whether an AI system is placed on the market or put into service \nindependently of the products referred to in points (a) and (b), that AI system shall be \nconsidered to be high-risk where both of the following conditions are fulfilled:\n(a)\nthe AI system is intended to be used as a safety component of a product, or the AI \nsystem is itself a product, covered by the Union harmonisation legislation listed in \nAnnex I;\n(b)\nthe product whose safety component pursuant to point (a) is the AI system, or the \nAI system itself as a product, is required to undergo a third-party conformity \nassessment, with a view to the placing on the market or the putting into service of \nthat product pursuant to the Union harmonisation legislation listed in Annex I.\n\nDocument 46: EN\nUnited in diversity\nEN\n(46)\nHigh-risk AI systems should only be placed on the Union market, put into service or used \nif they comply with certain mandatory requirements. Those requirements should ensure \nthat high-risk AI systems available in the Union or whose output is otherwise used in the \nUnion do not pose unacceptable risks to important Union public interests as recognised and \nprotected by Union law. On the basis of the New Legislative Framework, as clarified in \nthe Commission notice \u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules \n2022\u201d20, the general rule is that more than one legal act of Union harmonisation \nlegislation, such as Regulations (EU) 2017/74521 and (EU) 2017/74622 of the European \nParliament and of the Council or Directive 2006/42/EC of the European Parliament and \nof the Council23, may be applicable to one product, since the making available or putting \ninto service can take place only when the product complies with all applicable Union \nharmonisation legislation. To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "Under what conditions is a product manufacturer considered the provider of a high-risk AI system?", "answer": "The product manufacturer shall be considered to be the provider of the high-risk AI system if the high-risk AI system is placed on the market together with the product under the name or trademark of the product manufacturer, or if it is put into service under the name or trademark of the product manufacturer after the product has been placed on the market."}, {"question": "What are the mandatory requirements for high-risk AI systems in the Union?", "answer": "High-risk AI systems should only be placed on the Union market, put into service, or used if they comply with certain mandatory requirements that ensure they do not pose unacceptable risks to important Union public interests as recognized and protected by Union law."}], "seed_document_id": 247, "topic": "High-Risk AI Regulation"}}
{"id": "91b52a31-b03a-4c19-9e3f-41fad90e0ba8", "question": "What is the purpose of applying the rules established by the Regulation to AI system providers, and why is it important to maintain a consistent level of protection for these systems throughout the Union?", "reference_answer": "The purpose of applying the Regulation's rules to AI system providers is to ensure a level playing field and protect individual rights across the Union, while maintaining a consistent level of protection is crucial for fostering trustworthy AI and preventing divergences that could disrupt the internal market. Together, these objectives promote innovation and the effective deployment of AI systems throughout the Union.", "reference_context": "Document 22: EN\nUnited in diversity\nEN\n(21)\nIn order to ensure a level playing field and an effective protection of rights and freedoms of \nindividuals across the Union, the rules established by this Regulation should apply to \nproviders of AI systems in a non-discriminatory manner, irrespective of whether they are \nestablished within the Union or in a third country, and to deployers of AI systems \nestablished within the Union.\n(22)\nIn light of their digital nature, certain AI systems should fall within the scope of this \nRegulation even when they are not placed on the market, put into service, or used in the \nUnion. This is the case, for example, where an operator established in the Union contracts \ncertain services to an operator established in a third country in relation to an activity to be \nperformed by an AI system that would qualify as high-risk \u258c . In those circumstances, the \nAI system used in a third country by the operator could process data lawfully collected in \nand transferred from the Union, and provide to the contracting operator in the Union the \noutput of that AI system resulting from that processing, without that AI system being \nplaced on the market, put into service or used in the Union. To prevent the circumvention \nof this Regulation and to ensure an effective protection of natural persons located in the \nUnion, this Regulation should also apply to providers and deployers of AI systems that are \nestablished in a third country, to the extent the output produced by those systems is \nintended to be used in the Union.\n\nDocument 3: EN\nUnited in diversity\nEN\n(3)\n\u258c AI systems \u258c can be easily deployed in a large variety of sectors of the economy and \nmany parts of society, including across borders, and can easily circulate throughout the \nUnion. Certain Member States have already explored the adoption of national rules to \nensure that AI is trustworthy and safe and is developed and used in accordance with \nfundamental rights obligations. Diverging national rules may lead to the fragmentation of \nthe internal market and may decrease legal certainty for operators that develop, import or \nuse AI systems. A consistent and high level of protection throughout the Union should \ntherefore be ensured in order to achieve trustworthy AI, while divergences hampering the \nfree circulation, innovation, deployment and the uptake of AI systems and related \nproducts and services within the internal market should be prevented by laying down \nuniform obligations for operators and guaranteeing the uniform protection of overriding \nreasons of public interest and of rights of persons throughout the internal market on the \nbasis of Article 114 of the Treaty on the Functioning of the European Union (TFEU). To \nthe extent that this Regulation contains specific rules on the protection of individuals with \nregard to the processing of personal data concerning restrictions of the use of AI systems \nfor remote biometric identification for the purpose of law enforcement, of the use of AI \nsystems for risk assessments of natural persons for the purpose of law enforcement and \nof the use of AI systems of biometric categorisation for the purpose of law enforcement, it \nis appropriate to base this Regulation, in so far as those specific rules are concerned, on \nArticle 16 TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is \nappropriate to consult the European Data Protection Board.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the purpose of applying the rules established by the Regulation to providers of AI systems?", "answer": "The purpose is to ensure a level playing field and an effective protection of rights and freedoms of individuals across the Union."}, {"question": "Why is it important to have a consistent level of protection for AI systems throughout the Union?", "answer": "A consistent and high level of protection is necessary to achieve trustworthy AI and to prevent divergences that could hamper the free circulation, innovation, deployment, and uptake of AI systems within the internal market."}], "seed_document_id": 22, "topic": "AI Regulation in the EU"}}
{"id": "86d86d06-6437-476b-81b2-740ebc8277d1", "question": "What is the purpose of the advisory forum established under this Regulation and who is responsible for appointing its members?", "reference_answer": "The purpose of the advisory forum is to advise and provide technical expertise to the Board and the Commission, while the Commission is responsible for appointing the members of the forum according to specified criteria. This highlights the forum's role in supporting the Commission with expert advice, and the Commission's authority in its composition.", "reference_context": "Document 147: EN\nUnited in diversity\nEN\n(150)\nWith a view to ensuring the involvement of stakeholders in the implementation and \napplication of this Regulation, an advisory forum should be established to advise and \nprovide technical expertise to the Board and the Commission. To ensure a varied and \nbalanced stakeholder representation between commercial and non-commercial interest \nand, within the category of commercial interests, with regards to SMEs and other \nundertakings, the advisory forum should comprise inter alia industry, start-ups, SMEs, \nacademia, civil society, including the social partners, as well as the Fundamental Rights \nAgency, ENISA, the European Committee for Standardization (CEN), the European \nCommittee for Electrotechnical Standardization (CENELEC) and the European \nTelecommunications Standards Institute (ETSI).\n(151)\nTo support the implementation and enforcement of this Regulation, in particular the \nmonitoring activities of the AI Office as regards general-purpose AI models, a scientific \npanel of independent experts should be established. The independent experts \nconstituting the scientific panel should be selected on the basis of up-to-date scientific or \ntechnical expertise in the field of AI and should perform their tasks with impartiality, \nobjectivity and ensure the confidentiality of information and data obtained in carrying \nout their tasks and activities. To allow the reinforcement of national capacities necessary \nfor the effective enforcement of this Regulation, Member States should be able to request \nsupport from the pool of experts constituting the scientific panel for their enforcement \nactivities.\n\nDocument 346: EN\nUnited in diversity\nEN\n(o)\nreceive opinions by the Member States on qualified alerts regarding general-purpose AI \nmodels, and on national experiences and practices on the monitoring and enforcement \nof AI systems, in particular systems integrating the general-purpose AI models.\nArticle 67\nAdvisory forum\n1.\nAn advisory forum shall be established to provide technical expertise and advise the \nBoard and the Commission, and to contribute to their tasks under this Regulation.\n2.\nThe membership of the advisory forum shall represent a balanced selection of \nstakeholders, including industry, start-ups, SMEs, civil society and academia. The \nmembership of the advisory forum shall be balanced with regard to commercial and \nnon-commercial interests and, within the category of commercial interests, with regard \nto SMEs and other undertakings.\n3.\nThe Commission shall appoint the members of the advisory forum, in accordance with \nthe criteria set out in paragraph 2, from amongst stakeholders with recognised expertise \nin the field of AI.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the purpose of the advisory forum established under this Regulation?", "answer": "The advisory forum should be established to advise and provide technical expertise to the Board and the Commission."}, {"question": "Who will appoint the members of the advisory forum?", "answer": "The Commission shall appoint the members of the advisory forum, in accordance with the criteria set out in paragraph 2."}], "seed_document_id": 147, "topic": "AI Regulation and Governance"}}
{"id": "646d5e43-fb1a-4c79-a6d3-cda99cb58634", "question": "What is the purpose of processing personal data in AI regulatory sandboxes, and what compliance requirements must this processing meet?", "reference_answer": "The purpose of processing personal data in AI regulatory sandboxes is to prevent, investigate, detect, or prosecute criminal offenses and ensure public security. This processing must comply with specific Union or national laws and adhere to the same cumulative conditions, ensuring that it is both lawful and aligned with regulatory standards.", "reference_context": "Document 328: EN\nUnited in diversity\nEN\n2.\nFor the purposes of the prevention, investigation, detection or prosecution of criminal \noffences or the execution of criminal penalties, including safeguarding against and \npreventing threats to public security, under the control and responsibility of law \nenforcement authorities, the processing of personal data in AI regulatory sandboxes \nshall be based on a specific Union or national law and subject to the same cumulative \nconditions as referred to in paragraph 1.\n3.\nParagraph 1 is without prejudice to Union or national law which excludes processing of \npersonal data for other purposes than those explicitly mentioned in that law, as well as to \nUnion or national law laying down the basis for the processing of personal data which is \nnecessary for the purpose of developing, testing or training of innovative AI systems or \nany other legal basis, in compliance with Union law on the protection of personal data.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the purpose of processing personal data in AI regulatory sandboxes?", "answer": "The purpose is for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including safeguarding against and preventing threats to public security."}, {"question": "What must the processing of personal data in AI regulatory sandboxes comply with?", "answer": "It must be based on a specific Union or national law and subject to the same cumulative conditions as referred to in paragraph 1."}], "seed_document_id": 328, "topic": "AI Regulatory Sandboxes"}}
{"id": "adabbb29-7aff-45f2-8c58-e1d6c2d745f5", "question": "What rights or obligations remain unchanged by Paragraphs 1, 2, and 3 in relation to the definition of AI systems presenting a risk under EU regulations?", "reference_answer": "Paragraphs 1, 2, and 3 do not affect the rights or obligations of the Commission, Member States, and their authorities concerning information exchange and warnings, while AI systems presenting a risk are defined as 'products presenting a risk' under Article 3, point 19 of Regulation (EU) 2019/1020. This highlights that while certain obligations remain intact, the definition of risk in AI systems is a separate regulatory concern.", "reference_context": "Document 375: EN\nUnited in diversity\nEN\n4.\nParagraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member \nStates and their relevant authorities, as well as those of notified bodies, with regard to the \nexchange of information and the dissemination of warnings, including in the context of \ncross-border cooperation, nor shall they affect the obligations of the parties concerned to \nprovide information under criminal law of the Member States.\n5.\nThe Commission and Member States may exchange, where necessary and in accordance \nwith relevant provisions of international and trade agreements, confidential information \nwith regulatory authorities of third countries with which they have concluded bilateral or \nmultilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.\nArticle 79\nProcedure at national level for dealing with AI systems presenting a risk \n1.\nAI systems presenting a risk shall be understood as a \u201cproduct presenting a risk\u201d as defined \nin Article 3, point 19 of Regulation (EU) 2019/1020, in so far as they present risks to the \nhealth or safety, or to \u258c fundamental rights, of persons.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What rights or obligations are not affected by Paragraphs 1, 2, and 3?", "answer": "Paragraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member States and their relevant authorities, as well as those of notified bodies, with regard to the exchange of information and the dissemination of warnings."}, {"question": "How are AI systems presenting a risk defined in the context of the EU regulations?", "answer": "AI systems presenting a risk shall be understood as a 'product presenting a risk' as defined in Article 3, point 19 of Regulation (EU) 2019/1020."}], "seed_document_id": 375, "topic": "AI Regulation and Compliance"}}
{"id": "03f51d42-8147-488e-8ed5-34ed32f928ac", "question": "What are the classifications of high-risk AI systems in critical infrastructure management, and how do safety components relate to these classifications?", "reference_answer": "High-risk AI systems in critical infrastructure management are those designed to serve as safety components, which are specifically aimed at protecting the physical integrity of the infrastructure and ensuring the health and safety of individuals and property. Therefore, safety components are integral to the classification of these AI systems, as they directly influence the risk assessment based on their role in safeguarding critical infrastructure.", "reference_context": "Document 62: EN\nUnited in diversity\nEN\n(55)\nAs regards the management and operation of critical infrastructure, it is appropriate to \nclassify as high-risk the AI systems intended to be used as safety components in the \nmanagement and operation of critical digital infrastructure as listed in point (8) of the \nAnnex to Directive (EU) 2022/2557, road traffic and the supply of water, gas, heating and \nelectricity, since their failure or malfunctioning may put at risk the life and health of \npersons at large scale and lead to appreciable disruptions in the ordinary conduct of social \nand economic activities. Safety components of critical infrastructure, including critical \ndigital infrastructure, are systems used to directly protect the physical integrity of critical \ninfrastructure or the health and safety of persons and property but which are not \nnecessary in order for the system to function. The failure or malfunctioning of such \ncomponents might directly lead to risks to the physical integrity of critical infrastructure \nand thus to risks to health and safety of persons and property. Components intended to \nbe used solely for cybersecurity purposes should not qualify as safety components. \nExamples of safety components of such critical infrastructure may include systems for \nmonitoring water pressure or fire alarm controlling systems in cloud computing centres.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What AI systems are classified as high-risk in the management of critical infrastructure?", "answer": "AI systems intended to be used as safety components in the management and operation of critical digital infrastructure are classified as high-risk."}, {"question": "What are safety components of critical infrastructure?", "answer": "Safety components of critical infrastructure are systems used to directly protect the physical integrity of critical infrastructure or the health and safety of persons and property."}], "seed_document_id": 62, "topic": "High-Risk AI Systems"}}
{"id": "d21bb82b-1680-4c41-818d-2c0c8724692e", "question": "What is the main goal of the proposed amendments to existing acts regarding high-risk AI systems, and how should these amendments be implemented without disrupting existing structures?", "reference_answer": "The main goal of the proposed amendments is to ensure that mandatory requirements for high-risk AI systems are considered, while the amendments should be implemented in a way that does not interfere with the established governance, conformity assessment, and enforcement mechanisms. This approach aims to enhance the regulation of high-risk AI without undermining the current regulatory framework.", "reference_context": "Document 55: EN\nUnited in diversity\nEN\nappropriate to amend those acts to ensure that the Commission takes into account, on the \nbasis of the technical and regulatory specificities of each sector, and without interfering \nwith existing governance, conformity assessment and enforcement mechanisms and \nauthorities established therein, the mandatory requirements for high-risk AI systems laid \ndown in this Regulation when adopting any relevant delegated or implementing acts on the \nbasis of those acts.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the main goal of the proposed amendments to existing acts regarding high-risk AI systems?", "answer": "The main goal is to ensure that the Commission takes into account the mandatory requirements for high-risk AI systems laid down in this Regulation."}, {"question": "How should the amendments be made without affecting existing structures?", "answer": "The amendments should be made without interfering with existing governance, conformity assessment, and enforcement mechanisms and authorities established therein."}], "seed_document_id": 55, "topic": "AI Regulation and Compliance"}}
{"id": "0321fc1c-42df-4865-a3d5-8b84b163da6d", "question": "What is the relationship between the requirements for high-risk AI systems under Union harmonisation legislation and the provider's responsibility for compliance with Section 2 of the EU AI Act?", "reference_answer": "For high-risk AI systems, a single EU declaration of conformity is required for all applicable Union law, indicating a comprehensive compliance framework. The provider is responsible for ensuring that these requirements are met, highlighting the provider's crucial role in adhering to both the declaration and the specific compliance standards set forth in Section 2 of the EU AI Act.", "reference_context": "Document 290: EN\nUnited in diversity\nEN\n3.\nWhere high-risk AI systems are subject to other Union harmonisation legislation which \nalso requires an EU declaration of conformity, a single EU declaration of conformity shall \nbe drawn up in respect of all Union law applicable to the high-risk AI system. The \ndeclaration shall contain all the information required to identify the Union harmonisation \nlegislation to which the declaration relates.\n4.\nBy drawing up the EU declaration of conformity, the provider shall assume responsibility \nfor compliance with the requirements set out in Section 2. The provider shall keep the EU \ndeclaration of conformity up-to-date as appropriate.\n5.\nThe Commission is empowered to adopt delegated acts in accordance with Article 97 in \norder to amend Annex V by updating the content of the EU declaration of conformity set \nout in that Annex, in order to introduce elements that become necessary in light of \ntechnical progress.\nArticle 48\nCE marking \n1.\nThe CE marking shall be subject to the general principles set out in Article 30 of \nRegulation (EC) No 765/2008.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is required for high-risk AI systems that are subject to other Union harmonisation legislation?", "answer": "A single EU declaration of conformity shall be drawn up in respect of all Union law applicable to the high-risk AI system."}, {"question": "Who is responsible for compliance with the requirements set out in Section 2 of the EU AI Act?", "answer": "The provider shall assume responsibility for compliance with the requirements set out in Section 2."}], "seed_document_id": 290, "topic": "Others"}}
{"id": "15bf4bba-af55-4bb3-9af9-a888a10cde05", "question": "What actions should a provider take if their general-purpose AI model is classified as having systemic risks, and how does the Commission determine this designation?", "reference_answer": "If a provider's general-purpose AI model meets the criteria for systemic risks, they must notify the AI Office within two weeks. The Commission designates a model as having systemic risk based on its capabilities or impact reaching a specific threshold, reflecting a regulatory approach that considers both provider responsibilities and the Commission's evaluative criteria.", "reference_context": "Document 113: EN\nUnited in diversity\nEN\n(112)\nIt is also necessary to clarify a procedure for the classification of a general-purpose AI \nmodel with systemic risks. A general-purpose AI model that meets the applicable \nthreshold for high-impact capabilities should be presumed to be a general-purpose AI \nmodels with systemic risk. The provider should notify the AI Office at the latest two \nweeks after the requirements are met or it becomes known that a general-purpose AI \nmodel will meet the requirements that lead to the presumption. This is especially relevant \nin relation to the threshold of floating point operations because training of general-\npurpose AI models takes considerable planning which includes the upfront allocation of \ncompute resources and, therefore, providers of general-purpose AI models are able to \nknow if their model would meet the threshold before the training is completed. In the \ncontext of that notification, the provider should be able to demonstrate that, because of \nits specific characteristics, a general-purpose AI model exceptionally does not present \nsystemic risks, and that it thus should not be classified as a general-purpose AI model \nwith systemic risks. That information is valuable for the AI Office to anticipate the \nplacing on the market of general-purpose AI models with systemic risks and the \nproviders can start to engage with the AI Office early on. That information is especially \nimportant with regard to general-purpose AI models that are planned to be released as \nopen-source, given that, after the open-source model release, necessary measures to \nensure compliance with the obligations under this Regulation may be more difficult to \nimplement.\n\nDocument 111: EN\nUnited in diversity\nEN\n(111)\nIt is appropriate to establish a methodology for the classification of general-purpose AI \nmodels as general-purpose AI model with systemic risks. Since systemic risks result from \nparticularly high capabilities, a general-purpose AI model should be considered to \npresent systemic risks if it has high-impact capabilities, evaluated on the basis of \nappropriate technical tools and methodologies, or significant impact on the internal \nmarket due to its reach. High-impact capabilities in general-purpose AI models means \ncapabilities that match or exceed the capabilities recorded in the most advanced general-\npurpose AI models. The full range of capabilities in a model could be better understood \nafter its placing on the market or when deployers interact with the model. According to \nthe state of the art at the time of entry into force of this Regulation, the cumulative \namount of computation used for the training of the general-purpose AI model measured \nin floating point operations is one of the relevant approximations for model capabilities. \nThe cumulative amount of computation used for training includes the computation used \nacross the activities and methods that are intended to enhance the capabilities of the \nmodel prior to deployment, such as pre-training, synthetic data generation and fine-\ntuning. Therefore, an initial threshold of floating point operations should be set, which, \nif met by a general-purpose AI model, leads to a presumption that the model is a \ngeneral-purpose AI model with systemic risks. This threshold should be adjusted over \ntime to reflect technological and industrial changes, such as algorithmic improvements \nor increased hardware efficiency, and should be supplemented with benchmarks and \nindicators for model capability.\n\nDocument 112: EN\nUnited in diversity\nEN\nTo inform this, the AI Office should engage with the scientific community, industry, civil \nsociety and other experts. Thresholds, as well as tools and benchmarks for the \nassessment of high-impact capabilities, should be strong predictors of generality, its \ncapabilities and associated systemic risk of general-purpose AI models, and could take \ninto account the way the model will be placed on the market or the number of users it \nmay affect. To complement this system, there should be a possibility for the Commission \nto take individual decisions designating a general-purpose AI model as a general-\npurpose AI model with systemic risk if it is found that such model has capabilities or an \nimpact equivalent to those captured by the set threshold. That decision should be taken \non the basis of an overall assessment of the criteria for the designation of a general-\npurpose AI model with systemic risk set out in an annex to this Regulation, such as \nquality or size of the training data set, number of business and end users, its input and \noutput modalities, its level of autonomy and scalability, or the tools it has access to. \nUpon a reasoned request of a provider whose model has been designated as a general-\npurpose AI model with systemic risk, the Commission should take the request into \naccount and may decide to reassess whether the general-purpose AI model can still be \nconsidered to present systemic risks.\n\nDocument 114: EN\nUnited in diversity\nEN\n(113)\nIf the Commission becomes aware of the fact that a general-purpose AI model meets the \nrequirements to classify as a general-purpose AI model with systemic risk, which \npreviously had either not been known or of which the relevant provider has failed to \nnotify the Commission, the Commission should be empowered to designate it so. A \nsystem of qualified alerts should ensure that the AI Office is made aware by the \nscientific panel of general-purpose AI models that should possibly be classified as \ngeneral-purpose AI models with systemic risk, in addition to the monitoring activities of \nthe AI Office.\n(114)\nThe providers of general-purpose AI models presenting systemic risks should be subject, \nin addition to the obligations provided for providers of general-purpose AI models, to \nobligations aimed at identifying and mitigating those risks and ensuring an adequate \nlevel of cybersecurity protection, regardless of whether it is provided as a standalone \nmodel or embedded in an AI system or a product. To achieve those objectives, this \nRegulation should require providers to perform the necessary model evaluations, in \nparticular prior to its first placing on the market, including conducting and documenting \nadversarial testing of models, also, as appropriate, through internal or independent \nexternal testing. In addition, providers of general-purpose AI models with systemic risks \nshould continuously assess and mitigate systemic risks, including for example by putting \nin place risk-management policies, such as accountability and governance processes, \nimplementing post-market monitoring, taking appropriate measures along the entire \nmodel\u2019s lifecycle and cooperating with relevant actors along the AI value chain.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What should a provider do if their general-purpose AI model meets the requirements for classification as a model with systemic risks?", "answer": "The provider should notify the AI Office at the latest two weeks after the requirements are met or it becomes known that a general-purpose AI model will meet the requirements that lead to the presumption."}, {"question": "How does the Commission decide to designate a general-purpose AI model as one with systemic risk?", "answer": "The Commission may take individual decisions designating a general-purpose AI model as a general-purpose AI model with systemic risk if it is found that such model has capabilities or an impact equivalent to those captured by the set threshold."}], "seed_document_id": 113, "topic": "Systemic Risks in AI"}}
{"id": "6dde6267-88d1-4a4c-891c-a8e079e07510", "question": "What are the criteria for reassessing systemic risk in general-purpose AI models, and what obligations do providers have to ensure safety and compliance?", "reference_answer": "To reassess systemic risk, a provider must submit a reasoned request with new and detailed information since the last decision, while they are also obligated to identify and mitigate risks and maintain adequate cybersecurity measures. Both aspects highlight the importance of ongoing evaluation and proactive risk management in the deployment of general-purpose AI models.", "reference_context": "Document 301: EN\nUnited in diversity\nEN\n5.\nUpon a reasoned request of a provider whose model has been designated as a general-\npurpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take \nthe request into account and may decide to reassess whether the general-purpose AI \nmodel can still be considered to present systemic risks on the basis of the criteria set out \nin Annex XIII. Such a request shall contain objective, detailed and new reasons that \nhave arisen since the designation decision. Providers may request reassessment at the \nearliest six months after the designation decision. Where the Commission, following its \nreassessment, decides to maintain the designation as a general-purpose AI model with \nsystemic risk, providers may request reassessment at the earliest six months after that \ndecision.\n6.\nThe Commission shall ensure that a list of general-purpose AI models with systemic risk \nis published and shall keep that list up to date, without prejudice to the need to observe \nand protect intellectual property rights and confidential business information or trade \nsecrets in accordance with Union and national law.\n\nDocument 114: EN\nUnited in diversity\nEN\n(113)\nIf the Commission becomes aware of the fact that a general-purpose AI model meets the \nrequirements to classify as a general-purpose AI model with systemic risk, which \npreviously had either not been known or of which the relevant provider has failed to \nnotify the Commission, the Commission should be empowered to designate it so. A \nsystem of qualified alerts should ensure that the AI Office is made aware by the \nscientific panel of general-purpose AI models that should possibly be classified as \ngeneral-purpose AI models with systemic risk, in addition to the monitoring activities of \nthe AI Office.\n(114)\nThe providers of general-purpose AI models presenting systemic risks should be subject, \nin addition to the obligations provided for providers of general-purpose AI models, to \nobligations aimed at identifying and mitigating those risks and ensuring an adequate \nlevel of cybersecurity protection, regardless of whether it is provided as a standalone \nmodel or embedded in an AI system or a product. To achieve those objectives, this \nRegulation should require providers to perform the necessary model evaluations, in \nparticular prior to its first placing on the market, including conducting and documenting \nadversarial testing of models, also, as appropriate, through internal or independent \nexternal testing. In addition, providers of general-purpose AI models with systemic risks \nshould continuously assess and mitigate systemic risks, including for example by putting \nin place risk-management policies, such as accountability and governance processes, \nimplementing post-market monitoring, taking appropriate measures along the entire \nmodel\u2019s lifecycle and cooperating with relevant actors along the AI value chain.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What criteria must be met for a general-purpose AI model to be reassessed for systemic risk?", "answer": "A provider must submit a reasoned request containing objective, detailed, and new reasons that have arisen since the designation decision."}, {"question": "What obligations do providers of general-purpose AI models with systemic risks have?", "answer": "They are subject to obligations aimed at identifying and mitigating risks and ensuring an adequate level of cybersecurity protection."}], "seed_document_id": 301, "topic": "Systemic Risks in AI"}}
{"id": "c786f1e9-f205-4e9b-9c65-60b1450a4d98", "question": "What are the potential adverse impacts of poorly designed high-risk AI systems used by law enforcement authorities, and how do these systems contribute to these risks?", "reference_answer": "Poorly designed high-risk AI systems, such as those used for assessing crime victim risk or evaluating evidence reliability, can result in unjust surveillance and discrimination, leading to wrongful arrests or loss of liberty. The inherent risks of these systems are amplified when they are not designed with fairness and accountability in mind, potentially exacerbating issues of bias and injustice in law enforcement.", "reference_context": "Document 67: EN\nUnited in diversity\nEN\n(59)\nGiven their role and responsibility, actions by law enforcement authorities involving \ncertain uses of AI systems are characterised by a significant degree of power imbalance \nand may lead to surveillance, arrest or deprivation of a natural person\u2019s liberty as well as \nother adverse impacts on fundamental rights guaranteed in the Charter. In particular, if the \nAI system is not trained with high-quality data, does not meet adequate requirements in \nterms of its performance, its accuracy or robustness, or is not properly designed and tested \nbefore being put on the market or otherwise put into service, it may single out people in a \ndiscriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of \nimportant procedural fundamental rights, such as the right to an effective remedy and to a \nfair trial as well as the right of defence and the presumption of innocence, could be \nhampered, in particular, where such AI systems are not sufficiently transparent, \nexplainable and documented. It is therefore appropriate to classify as high-risk, insofar as \ntheir use is permitted under relevant Union and national law, a number of AI systems \nintended to be used in the law enforcement context where accuracy, reliability and \ntransparency is particularly important to avoid adverse impacts, retain public trust and \nensure accountability and effective redress.\n\nDocument 68: EN\nUnited in diversity\nEN\nIn view of the nature of the activities and the risks relating thereto, those high-risk AI \nsystems should include in particular AI systems intended to be used by or on behalf of law \nenforcement authorities or by Union institutions, bodies, offices, or agencies in support of \nlaw enforcement authorities for assessing the risk of a natural person to become a victim \nof criminal offences, as polygraphs and similar tools, for the evaluation of the reliability \nof evidence in in the course of investigation or prosecution of criminal offences, and, \ninsofar as not prohibited under this Regulation, for assessing the risk of a natural \nperson offending or reoffending not solely on the basis of the profiling of natural persons \nor the assessment of personality traits and characteristics or the past criminal behaviour of \nnatural persons or groups, for profiling in the course of detection, investigation or \nprosecution of criminal offences \u258c . AI systems specifically intended to be used for \nadministrative proceedings by tax and customs authorities as well as by financial \nintelligence units carrying out administrative tasks analysing information pursuant to \nUnion anti-money laundering law should not be classified as high-risk AI systems used \nby law enforcement authorities for the purpose of prevention, detection, investigation and \nprosecution of criminal offences. The use of AI tools by law enforcement and other \nrelevant authorities should not become a factor of inequality, or exclusion. The impact \nof the use of AI tools on the defence rights of suspects should not be ignored, in \nparticular the difficulty in obtaining meaningful information on the functioning of those \nsystems and the resulting difficulty in challenging their results in court, in particular by \nnatural persons under investigation.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What are the potential adverse impacts of poorly designed AI systems used by law enforcement authorities?", "answer": "Poorly designed AI systems may lead to surveillance, arrest, or deprivation of a natural person\u2019s liberty, and can single out people in a discriminatory or unjust manner."}, {"question": "What types of AI systems are classified as high-risk in the law enforcement context?", "answer": "High-risk AI systems include those intended to be used by law enforcement for assessing the risk of a person becoming a victim of crimes, evaluating the reliability of evidence, and profiling in the course of investigations."}], "seed_document_id": 67, "topic": "AI Regulation and Ethics"}}
{"id": "e39fc4b5-3a60-4b40-b54a-c746287652f3", "question": "What actions should the Commission take when objections are raised, and what are the requirements for notified bodies under the EU AI Act in relation to these actions?", "reference_answer": "When objections are raised, the Commission must promptly consult with relevant Member States and conformity assessment bodies to address the issues, while notified bodies are required to meet specific organizational, quality management, and cybersecurity standards to effectively perform their tasks. Together, these processes ensure that both the Commission and notified bodies maintain compliance and address concerns within the framework of the EU AI Act.", "reference_context": "Document 261: EN\nUnited in diversity\nEN\n5.\nWhere objections are raised, the Commission shall, without delay, enter into \nconsultations with the relevant Member States and the conformity assessment body. In \nview thereof, the Commission shall decide whether the authorisation is justified. The \nCommission shall address its decision to the Member State concerned and to the relevant \nconformity assessment body.\n\u258c\nArticle 31\nRequirements relating to notified bodies\n1.\nA notified body shall be established under the national law of a Member State and shall \nhave legal personality.\n2.\nNotified bodies shall satisfy the organisational, quality management, resources and process \nrequirements that are necessary to fulfil their tasks, as well as suitable cybersecurity \nrequirements.\n3.\nThe organisational structure, allocation of responsibilities, reporting lines and operation of \nnotified bodies shall ensure confidence in their performance, and in the results of the \nconformity assessment activities that the notified bodies conduct.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What should the Commission do when objections are raised?", "answer": "The Commission shall, without delay, enter into consultations with the relevant Member States and the conformity assessment body."}, {"question": "What are the requirements for notified bodies under the EU AI Act?", "answer": "Notified bodies shall satisfy the organisational, quality management, resources and process requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity requirements."}], "seed_document_id": 261, "topic": "Notified Bodies Regulations"}}
{"id": "587a84d3-1904-49ad-9673-11cf204b0730", "question": "What is the purpose of the risk management system for high-risk AI systems, and how frequently should it be reviewed and updated to maintain its effectiveness?", "reference_answer": "The purpose of the risk management system for high-risk AI systems is to identify and mitigate risks throughout the system's lifecycle, focusing on health, safety, and fundamental rights. To ensure its effectiveness, this system should be regularly reviewed and updated, reflecting the need for continuous improvement and adaptation to emerging risks.", "reference_context": "Document 211: EN\nUnited in diversity\nEN\n2.\nThe risk management system shall be understood as a continuous iterative process \nplanned and run throughout the entire lifecycle of a high-risk AI system, requiring regular \nsystematic review and updating. It shall comprise the following steps:\n(a)\nthe identification and analysis of the known and the reasonably foreseeable risks \nthat the high-risk AI system can pose to health, safety or fundamental rights when \nthe high-risk AI system is used in accordance with its intended purpose;\n(b)\nthe estimation and evaluation of the risks that may emerge when the high-risk AI \nsystem is used in accordance with its intended purpose, and under conditions of \nreasonably foreseeable misuse;\n(c)\nthe evaluation of other risks possibly arising, based on the analysis of data gathered \nfrom the post-market monitoring system referred to in Article 72;\n(d)\nthe adoption of appropriate and targeted risk management measures designed to \naddress the risks identified pursuant to point (a).\n3.\nThe risks referred to in this Article shall concern only those which may be reasonably \nmitigated or eliminated through the development or design of the high-risk AI system, or \nthe provision of adequate technical information.\n\nDocument 75: EN\nUnited in diversity\nEN\n(65)\nThe risk-management system should consist of a continuous, iterative process that is \nplanned and run throughout the entire lifecycle of a high-risk AI system. That process \nshould be aimed at identifying and mitigating the relevant risks of AI systems on health, \nsafety and fundamental rights. The risk-management system should be regularly \nreviewed and updated to ensure its continuing effectiveness, as well as justification and \ndocumentation of any significant decisions and actions taken subject to this Regulation. \nThis process should ensure that the provider identifies risks or adverse impacts and \nimplements mitigation measures for the known and reasonably foreseeable risks of AI \nsystems to the health, safety and fundamental rights in light of their intended purpose \nand reasonably foreseeable misuse, including the possible risks arising from the \ninteraction between the AI system and the environment within which it operates. The \nrisk-management system should adopt the most appropriate risk-management measures \nin light of the state of the art in AI. When identifying the most appropriate risk-\nmanagement measures, the provider should document and explain the choices made \nand, when relevant, involve experts and external stakeholders. In identifying the \nreasonably foreseeable misuse of high-risk AI systems, the provider should cover uses of \nAI systems which, while not directly covered by the intended purpose and provided for in \nthe instruction for use may nevertheless be reasonably expected to result from readily \npredictable human behaviour in the context of the specific characteristics and use of a \nparticular AI system.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the purpose of the risk management system for high-risk AI systems?", "answer": "The risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high-risk AI system, aimed at identifying and mitigating the relevant risks of AI systems on health, safety, and fundamental rights."}, {"question": "How often should the risk management system be reviewed and updated?", "answer": "The risk-management system should be regularly reviewed and updated to ensure its continuing effectiveness."}], "seed_document_id": 211, "topic": "High-Risk AI Regulation"}}
{"id": "2e140daf-cfa3-4311-acc4-7f0efbee5ac5", "question": "What are the purposes of the Entry/Exit System (EES) and the European Travel Information and Authorisation System (ETIAS), and how do they complement each other in managing third-country nationals at the borders?", "reference_answer": "The Entry/Exit System (EES) is designed to register entry, exit, and refusal of entry data for third-country nationals at external borders, while the European Travel Information and Authorisation System (ETIAS) focuses on enhancing security and simplifying travel authorization for these individuals. Together, they complement each other by providing a comprehensive framework for monitoring and regulating the movement of third-country nationals, ensuring both security and efficient border management.", "reference_context": "Document 466: EN\nUnited in diversity\nEN\n4.\nEntry/Exit System\nRegulation (EU) 2017/2226 of the European Parliament and of the Council of \n30 November 2017 establishing an Entry/Exit System (EES) to register entry and \nexit data and refusal of entry data of third-country nationals crossing the external \nborders of the Member States and determining the conditions for access to the EES \nfor law enforcement purposes, and amending the Convention implementing the \nSchengen Agreement and Regulations (EC) No 767/2008 and (EU) No 1077/2011 \n(OJ L 327, 9.12.2017, p. 20).\n5.\nEuropean Travel Information and Authorisation System\n(a)\nRegulation (EU) 2018/1240 of the European Parliament and of the Council of \n12 September 2018 establishing a European Travel Information and Authorisation \nSystem (ETIAS) and amending Regulations (EU) No 1077/2011, (EU) No 515/2014, \n(EU) 2016/399, (EU) 2016/1624 and (EU) 2017/2226 (OJ L 236, 19.9.2018, p. 1).\n(b)\nRegulation (EU) 2018/1241 of the European Parliament and of the Council of \n12 September 2018 amending Regulation (EU) 2016/794 for the purpose of \nestablishing a European Travel Information and Authorisation System (ETIAS) \n(OJ L 236, 19.9.2018, p. 72).", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the purpose of the Entry/Exit System (EES)?", "answer": "The Entry/Exit System (EES) is established to register entry and exit data and refusal of entry data of third-country nationals crossing the external borders of the Member States."}, {"question": "What does the European Travel Information and Authorisation System (ETIAS) aim to achieve?", "answer": "The European Travel Information and Authorisation System (ETIAS) is aimed at enhancing security and streamlining the travel authorization process for third-country nationals."}], "seed_document_id": 466, "topic": "EU Regulations and Directives"}}
{"id": "c94987ab-606b-4052-9ac8-9b4a58e2d13e", "question": "How does the principle of 'United in diversity' relate to the Commission's consideration of technical and regulatory specificities when amending acts for high-risk AI systems?", "reference_answer": "The principle of 'United in diversity' emphasizes the importance of recognizing and valuing different perspectives and contexts, which aligns with the Commission's need to consider the unique technical and regulatory specificities of each sector when amending acts related to high-risk AI systems. Both concepts highlight the importance of inclusivity and tailored approaches in governance and regulation.", "reference_context": "Document 55: EN\nUnited in diversity\nEN\nappropriate to amend those acts to ensure that the Commission takes into account, on the \nbasis of the technical and regulatory specificities of each sector, and without interfering \nwith existing governance, conformity assessment and enforcement mechanisms and \nauthorities established therein, the mandatory requirements for high-risk AI systems laid \ndown in this Regulation when adopting any relevant delegated or implementing acts on the \nbasis of those acts.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What principle is emphasized in the context provided?", "answer": "The principle of 'United in diversity' is emphasized."}, {"question": "What must the Commission consider when amending acts related to high-risk AI systems?", "answer": "The Commission must take into account the technical and regulatory specificities of each sector."}], "seed_document_id": 55, "topic": "AI Regulation and Compliance"}}
{"id": "3ba4a81f-81fa-4efc-ae9a-f57c41eac26b", "question": "What actions must providers of high-risk AI systems take if they find their system is not in conformity with the Regulation, and how do these actions compare to those required when a high-risk AI system presents a risk?", "reference_answer": "If providers of high-risk AI systems find their system is not in conformity with the Regulation, they must take corrective actions to ensure compliance, which may include withdrawing or recalling the system. In contrast, if a high-risk AI system presents a risk, providers must investigate the causes and inform market surveillance authorities. Both scenarios require immediate action, but the focus differs: one is on compliance and corrective measures, while the other emphasizes risk assessment and communication.", "reference_context": "Document 237: EN\nUnited in diversity\nEN\nArticle 20\nCorrective actions and duty of information\n1.\nProviders of high-risk AI systems which consider or have reason to consider that a high-\nrisk AI system that they have placed on the market or put into service is not in conformity \nwith this Regulation shall immediately take the necessary corrective actions to bring that \nsystem into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They \nshall inform the distributors of the high-risk AI system concerned and, where applicable, \nthe deployers, the authorised representative and importers accordingly.\n2.\nWhere the high-risk AI system presents a risk within the meaning of Article 79(1) and \nthe provider becomes aware of that risk, it shall immediately investigate the causes, in \ncollaboration with the reporting deployer, where applicable, and inform the market \nsurveillance authorities competent for the high-risk AI system concerned and, where \napplicable, the notified body that issued a certificate for that high-risk AI system in \naccordance with Article 44, in particular, of the nature of the non-compliance and of \nany relevant corrective action taken.\n\u258c", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What must providers of high-risk AI systems do if they find their system is not in conformity with the Regulation?", "answer": "Providers of high-risk AI systems shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate."}, {"question": "What should a provider do if a high-risk AI system presents a risk?", "answer": "The provider shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, and inform the market surveillance authorities competent for the high-risk AI system concerned."}], "seed_document_id": 237, "topic": "High-Risk AI Regulation"}}
{"id": "cf6fe4fc-7c45-4611-bfda-4c508edcf8da", "question": "What is the significance of Article 6a of Protocol No 21 regarding Ireland in relation to the binding nature of Article 5(1) and its implications for AI systems?", "reference_answer": "Article 6a of Protocol No 21 highlights that Ireland is not bound by Article 5(1) concerning biometric categorization systems in police and judicial cooperation, which also extends to specific provisions regarding AI systems, particularly the first subparagraph, point (d). This indicates that Ireland retains autonomy over these regulations, emphasizing its unique position within the framework of EU laws.", "reference_context": "Document 42: EN\nUnited in diversity\nEN\n(40)\nIn accordance with Article 6a of Protocol No 21 on the position of the United Kingdom \nand Ireland in respect of the area of freedom, security and justice, as annexed to the TEU \nand to the TFEU, Ireland is not bound by the rules laid down in Article 5(1), first \nsubparagraph, point (g), to the extent it applies to the use of biometric categorisation \nsystems for activities in the field of police cooperation and judicial cooperation in \ncriminal matters, Article 5(1), first subparagraph, point (d), to the extent it applies to the \nuse of AI systems covered by that provision, Article 5(1), first subparagraph, point (h), \nArticle 5(2) to (6) and Article 26(10) of this Regulation adopted on the basis of Article 16 \nTFEU which relate to the processing of personal data by the Member States when carrying \nout activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of \nthe TFEU, where Ireland is not bound by the rules governing the forms of judicial \ncooperation in criminal matters or police cooperation which require compliance with the \nprovisions laid down on the basis of Article 16 TFEU.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the significance of Article 6a of Protocol No 21 regarding Ireland?", "answer": "Ireland is not bound by the rules laid down in Article 5(1) concerning the use of biometric categorisation systems for activities in the field of police cooperation and judicial cooperation in criminal matters."}, {"question": "Which articles are mentioned as not binding for Ireland in relation to AI systems?", "answer": "Ireland is not bound by Article 5(1), first subparagraph, point (d), to the extent it applies to the use of AI systems covered by that provision."}], "seed_document_id": 42, "topic": "Others"}}
{"id": "ab6e96a3-9a9d-46de-9d85-dd3a42969b19", "question": "What is the deadline for AI systems to comply with the new Regulation, and how do the referenced legal acts relate to the compliance of large-scale IT systems?", "reference_answer": "AI systems placed on the market before 36 months from the Regulation's entry into force must comply by 31 December 2030. The compliance requirements outlined in this Regulation also apply to the evaluation of large-scale IT systems as specified in the legal acts listed in Annex X, indicating a broader framework for ensuring compliance across various technological domains.", "reference_context": "Document 422: EN\nUnited in diversity\nEN\nArticle 111\nAI systems already placed on the market or put into service and general-purpose AI models already \nplaced on the marked\n1.\nWithout prejudice to the application of Article 5 as referred to in Article 113(3), point \n(a), AI systems which are components of the large-scale IT systems established by the \nlegal acts listed in Annex X that have been placed on the market or put into service before \n\u258c\u2026 [36 months from the date of entry into force of this Regulation] shall be brought into \ncompliance with this Regulation by 31 December 2030.\nThe requirements laid down in this Regulation shall be taken into account \u258c in the \nevaluation of each large-scale IT system established by the legal acts listed in Annex X to \nbe undertaken as provided for in those legal acts and where those legal acts are replaced \nor amended.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the deadline for AI systems already placed on the market to comply with the new Regulation?", "answer": "AI systems that have been placed on the market or put into service before 36 months from the date of entry into force of this Regulation shall be brought into compliance by 31 December 2030."}, {"question": "Which legal acts are referenced in relation to large-scale IT systems and AI compliance?", "answer": "The requirements laid down in this Regulation shall be taken into account in the evaluation of each large-scale IT system established by the legal acts listed in Annex X."}], "seed_document_id": 422, "topic": "AI Regulation and Compliance"}}
{"id": "765b9da9-a0c3-4b7b-a8d4-12f45a5827ad", "question": "What does Article 87 pertain to in terms of reporting infringements, and which directive specifically applies to this regulation?", "reference_answer": "Article 87 pertains to the reporting of infringements and the protection of reporting persons, while Directive (EU) 2019/1937 specifically applies to the reporting of infringements under this regulation. Both emphasize the importance of safeguarding individuals who report violations, with the directive providing a legal framework for such protections.", "reference_context": "Document 387: EN\nUnited in diversity\nEN\nArticle 87\nReporting of infringements and protection of reporting persons\nDirective (EU) 2019/1937 shall apply to the reporting of infringements of this Regulation and the \nprotection of persons reporting such infringements.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What does Article 87 pertain to?", "answer": "Article 87 pertains to the reporting of infringements and the protection of reporting persons."}, {"question": "Which directive applies to the reporting of infringements under this regulation?", "answer": "Directive (EU) 2019/1937 shall apply to the reporting of infringements of this Regulation."}], "seed_document_id": 387, "topic": "AI Regulation and Compliance"}}
{"id": "e27aa44f-8f7f-488b-bb5d-33b8d67151b1", "question": "What are the requirements for using 'real-time' remote biometric identification systems in public spaces for law enforcement, and what are the consequences if the required authorisation is rejected?", "reference_answer": "The use of 'real-time' remote biometric identification systems for law enforcement requires express and specific authorisation from a judicial or independent administrative authority. If this authorisation is rejected, the use must cease immediately, and all associated data and results must be discarded and deleted.", "reference_context": "Document 199: EN\nUnited in diversity\nEN\n3.\nFor the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each \u258c use \nfor the purposes of law enforcement of a \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be subject to a prior authorisation granted by a judicial \nauthority or \u258c an independent administrative authority whose decision is binding of the \nMember State in which the use is to take place, issued upon a reasoned request and in \naccordance with the detailed rules of national law referred to in paragraph 5. However, in a \nduly justified situation of urgency, the use of such system may be commenced without an \nauthorisation provided that such authorisation is requested without undue delay, at the \nlatest within 24 hours. If such authorisation is rejected, the use shall be stopped with \nimmediate effect and all the data, as well as the results and outputs of that use shall be \nimmediately discarded and deleted.\nThe competent judicial authority or an independent administrative authority whose \ndecision is binding shall grant the authorisation only where it is satisfied, on the basis of \nobjective evidence or clear indications presented to it, that the use of the \u2018real-time\u2019 remote \nbiometric identification system concerned is necessary for, and proportionate to, achieving \none of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in \nthe request and, in particular, remains limited to what is strictly necessary concerning \nthe period of time as well as the geographic and personal scope. In deciding on the \nrequest, that authority shall take into account the elements referred to in paragraph 2. No \ndecision that produces an adverse legal effect on a person may be taken based solely on \nthe output of the \u2018real-time\u2019 remote biometric identification system.\n\nDocument 38: EN\nUnited in diversity\nEN\n(35)\nEach use of a \u2018real-time\u2019 remote biometric identification system in publicly accessible \nspaces for the purpose of law enforcement should be subject to an express and specific \nauthorisation by a judicial authority or by an independent administrative authority of a \nMember State whose decision is binding. Such authorisation should, in principle, be \nobtained prior to the use of the AI system with a view to identifying a person or persons. \nExceptions to that rule should be allowed in duly justified situations on grounds of \nurgency, namely in situations where the need to use the systems concerned is such as to \nmake it effectively and objectively impossible to obtain an authorisation before \ncommencing the use of the AI system. In such situations of urgency, the use of the AI \nsystem should be restricted to the absolute minimum necessary and should be subject to \nappropriate safeguards and conditions, as determined in national law and specified in the \ncontext of each individual urgent use case by the law enforcement authority itself. In \naddition, the law enforcement authority should in such situations request such \nauthorisation \u258c while providing the reasons for not having been able to request it earlier, \nwithout undue delay and at the latest within 24 hours. If such an authorisation is \nrejected, the use of real-time biometric identification systems linked to that authorisation \nshould cease with immediate effect and all the data related to such use should be \ndiscarded and deleted. Such data includes input data directly acquired by an AI system \nin the course of the use of such system as well as the results and outputs of the use \nlinked to that authorisation. It should not include input that is legally acquired in \naccordance with another Union or national law. In any case, no decision producing an \nadverse legal effect on a person should be taken based solely on the output of the remote \nbiometric identification system.\n\nDocument 198: EN\nUnited in diversity\nEN\n2.\nThe use of \u2018real-time\u2019 remote biometric identification systems in publicly accessible spaces \nfor the purposes of law enforcement for any of the objectives referred to in paragraph 1, \nfirst subparagraph, point (h), shall be deployed for the purposes set out in that point only \nto confirm the identity of the specifically targeted individual, and it shall take into \naccount the following elements:\n(a)\nthe nature of the situation giving rise to the possible use, in particular the seriousness, \nprobability and scale of the harm that would be caused if the system were not used;\n(b)\nthe consequences of the use of the system for the rights and freedoms of all persons \nconcerned, in particular the seriousness, probability and scale of those consequences.\nIn addition, the use of \u2018real-time\u2019 remote biometric identification systems in publicly \naccessible spaces for the purposes of law enforcement for any of the objectives referred to \nin paragraph 1, first subparagraph, point (h), of this Article shall comply with necessary \nand proportionate safeguards and conditions in relation to the use in accordance with the \nnational law authorising the use thereof, in particular as regards the temporal, geographic \nand personal limitations. The use of the \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be authorised only if the law enforcement authority \nhas completed a fundamental rights impact assessment as provided for in Article 27 and \nhas registered the system in the EU database according to Article 49. However, in duly \njustified cases of urgency, the use of such systems may be commenced without the \nregistration in the EU database, provided that such registration is completed without \nundue delay.\n\nDocument 200: EN\nUnited in diversity\nEN\n4.\nWithout prejudice to paragraph 3, each use of a \u2018real-time\u2019 remote biometric \nidentification system in publicly accessible spaces for law enforcement purposes shall be \nnotified to the relevant market surveillance authority and the national data protection \nauthority in accordance with the national rules referred to in paragraph 5. The \nnotification shall, as a minimum, contain the information specified under paragraph 6 \nand shall not include sensitive operational data.\n5.\nA Member State may decide to provide for the possibility to fully or partially authorise the \nuse of \u2018real-time\u2019 remote biometric identification systems in publicly accessible spaces for \nthe purposes of law enforcement within the limits and under the conditions listed in \nparagraph 1, first subparagraph, point (h), and paragraphs 2 and 3. \u258c Member States \nconcerned shall lay down in their national law the necessary detailed rules for the request, \nissuance and exercise of, as well as supervision and reporting relating to, the \nauthorisations referred to in paragraph 3. Those rules shall also specify in respect of which \nof the objectives listed in paragraph 1, first subparagraph, point (h), including which of the \ncriminal offences referred to in point (h)(iii) thereof, the competent authorities may be \nauthorised to use those systems for the purposes of law enforcement. Member States shall \nnotify those rules to the Commission at the latest 30 days following the adoption thereof. \nMember States may introduce, in accordance with Union law, more restrictive laws on \nthe use of remote biometric identification systems.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is required for the use of 'real-time' remote biometric identification systems in publicly accessible spaces for law enforcement purposes?", "answer": "Each use of a 'real-time' remote biometric identification system for law enforcement should be subject to an express and specific authorisation by a judicial authority or by an independent administrative authority of a Member State whose decision is binding."}, {"question": "What happens if the authorisation for using the biometric identification system is rejected?", "answer": "If such an authorisation is rejected, the use shall be stopped with immediate effect and all the data, as well as the results and outputs of that use shall be immediately discarded and deleted."}], "seed_document_id": 199, "topic": "Biometric Data and AI"}}
{"id": "dc62bb67-ce7b-474c-98b8-347a6e3ffce2", "question": "What are the requirements for providers of high-risk AI systems regarding the implementation of a quality management system and the establishment of a post-market monitoring system?", "reference_answer": "Providers of high-risk AI systems must implement a quality management system to ensure compliance with regulations, and in addition, they should also establish a robust post-market monitoring system to continuously assess the performance and safety of their AI systems after they are deployed. Both systems are essential for maintaining high standards and addressing potential issues that may arise during the AI system's lifecycle.", "reference_context": "Document 231: EN\nUnited in diversity\nEN\nArticle 17\nQuality management system\n1.\nProviders of high-risk AI systems shall put a quality management system in place that \nensures compliance with this Regulation. That system shall be documented in a systematic \nand orderly manner in the form of written policies, procedures and instructions, and shall \ninclude at least the following aspects:\n(a)\na strategy for regulatory compliance, including compliance with conformity \nassessment procedures and procedures for the management of modifications to the \nhigh-risk AI system;\n(b)\ntechniques, procedures and systematic actions to be used for the design, design \ncontrol and design verification of the high-risk AI system;\n(c)\ntechniques, procedures and systematic actions to be used for the development, \nquality control and quality assurance of the high-risk AI system;\n(d)\nexamination, test and validation procedures to be carried out before, during and after \nthe development of the high-risk AI system, and the frequency with which they have \nto be carried out;\n\nDocument 89: EN\nUnited in diversity\nEN\n(81)\nThe provider should establish a sound quality management system, ensure the \naccomplishment of the required conformity assessment procedure, draw up the relevant \ndocumentation and establish a robust post-market monitoring system. Providers of high-\nrisk AI systems that are subject to obligations regarding quality management systems \nunder relevant sectoral Union law should have the possibility to include the elements of \nthe quality management system provided for in this Regulation as part of the existing \nquality management system provided for in that other sectoral Union law. The \ncomplementarity between this Regulation and existing sectoral Union law should also be \ntaken into account in future standardisation activities or guidance adopted by the \nCommission. Public authorities which put into service high-risk AI systems for their own \nuse may adopt and implement the rules for the quality management system as part of the \nquality management system adopted at a national or regional level, as appropriate, taking \ninto account the specificities of the sector and the competences and organisation of the \npublic authority concerned.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What must providers of high-risk AI systems implement according to Article 17?", "answer": "Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation."}, {"question": "What should the provider establish in addition to a quality management system?", "answer": "The provider should establish a robust post-market monitoring system."}], "seed_document_id": 231, "topic": "High-Risk AI Regulation"}}
{"id": "147c7177-7471-4595-be22-98aabea9fec6", "question": "What are the conditions that classify an AI system as high-risk, and under what circumstances is a product manufacturer considered the provider of such a system?", "reference_answer": "An AI system is classified as high-risk if it serves as a safety component of a product that requires third-party conformity assessment under Union harmonisation legislation. A product manufacturer is considered the provider of a high-risk AI system if it is marketed under their name or trademark, either alongside the product or after the product's market introduction, indicating a direct relationship between the manufacturer and the high-risk classification of the AI system.", "reference_context": "Document 247: EN\nUnited in diversity\nEN\n3.\nIn the case of high-risk AI systems that are safety components of products covered by the \nUnion harmonisation legislation listed in Section A of Annex I, the product manufacturer \nshall be considered to be the provider of the high-risk AI system, and shall be subject to \nthe obligations under Article 16 under either of the following circumstances:\n(a)\nthe high-risk AI system is placed on the market together with the product under the \nname or trademark of the product manufacturer;\n(b)\nthe high-risk AI system is put into service under the name or trademark of the \nproduct manufacturer after the product has been placed on the market.\n4.\nThe provider of a high-risk AI system and the third party that supplies an AI system, \ntools, services, components, or processes that are used or integrated in a high-risk AI \nsystem shall, by written agreement, specify the necessary information, capabilities, \ntechnical access and other assistance based on the generally acknowledged state of the \nart, in order to enable the provider of the high-risk AI system to fully comply with the \nobligations set out in this Regulation. This paragraph shall not apply to third parties \nmaking accessible to the public tools, services, processes, or components, other than \ngeneral-purpose AI models, under a free and open-source licence.\n\nDocument 91: EN\nUnited in diversity\nEN\n(84)\nTo ensure legal certainty, it is necessary to clarify that, under certain specific conditions, \nany distributor, importer, deployer or other third-party should be considered to be a \nprovider of a high-risk AI system and therefore assume all the relevant obligations. This \nwould be the case if that party puts its name or trademark on a high-risk AI system \nalready placed on the market or put into service, without prejudice to contractual \narrangements stipulating that the obligations are allocated otherwise. This would also be \nthe case if that party makes a substantial modification to a high-risk AI system that has \nalready been placed on the market or has already been put into service in a way that it \nremains a high-risk AI system in accordance with this Regulation, or if it modifies the \nintended purpose of an AI system, including a general-purpose AI system, which has not \nbeen classified as high-risk and has already been placed on the market or put into \nservice, in a way that the AI system becomes a high-risk AI system in accordance with \nthis Regulation. Those provisions should apply without prejudice to more specific \nprovisions established in certain Union harmonisation legislation based on the New \nLegislative Framework, together with which this Regulation should apply. For example, \nArticle 16(2) of Regulation (EU) 2017/745, establishing that certain changes should not \nbe considered to be modifications of a device that could affect its compliance with the \napplicable requirements, should continue to apply to high-risk AI systems that are \nmedical devices within the meaning of that Regulation.\n\nDocument 202: EN\nUnited in diversity\nEN\nCHAPTER III\nHIGH-RISK AI SYSTEMS\nSection 1\nClassification of AI systems as high-risk\nArticle 6\nClassification rules for high-risk AI systems\n1.\nIrrespective of whether an AI system is placed on the market or put into service \nindependently of the products referred to in points (a) and (b), that AI system shall be \nconsidered to be high-risk where both of the following conditions are fulfilled:\n(a)\nthe AI system is intended to be used as a safety component of a product, or the AI \nsystem is itself a product, covered by the Union harmonisation legislation listed in \nAnnex I;\n(b)\nthe product whose safety component pursuant to point (a) is the AI system, or the \nAI system itself as a product, is required to undergo a third-party conformity \nassessment, with a view to the placing on the market or the putting into service of \nthat product pursuant to the Union harmonisation legislation listed in Annex I.\n\nDocument 46: EN\nUnited in diversity\nEN\n(46)\nHigh-risk AI systems should only be placed on the Union market, put into service or used \nif they comply with certain mandatory requirements. Those requirements should ensure \nthat high-risk AI systems available in the Union or whose output is otherwise used in the \nUnion do not pose unacceptable risks to important Union public interests as recognised and \nprotected by Union law. On the basis of the New Legislative Framework, as clarified in \nthe Commission notice \u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules \n2022\u201d20, the general rule is that more than one legal act of Union harmonisation \nlegislation, such as Regulations (EU) 2017/74521 and (EU) 2017/74622 of the European \nParliament and of the Council or Directive 2006/42/EC of the European Parliament and \nof the Council23, may be applicable to one product, since the making available or putting \ninto service can take place only when the product complies with all applicable Union \nharmonisation legislation. To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "Under what circumstances is a product manufacturer considered the provider of a high-risk AI system?", "answer": "The product manufacturer shall be considered to be the provider of the high-risk AI system if the high-risk AI system is placed on the market together with the product under the name or trademark of the product manufacturer, or if the high-risk AI system is put into service under the name or trademark of the product manufacturer after the product has been placed on the market."}, {"question": "What conditions classify an AI system as high-risk?", "answer": "An AI system shall be considered high-risk if it is intended to be used as a safety component of a product covered by Union harmonisation legislation and if that product is required to undergo a third-party conformity assessment."}], "seed_document_id": 247, "topic": "High-Risk AI Regulation"}}
{"id": "3f381e47-307a-4a36-bbf7-2a1dfe7f2269", "question": "How does the purpose of the AI Office in developing an evaluation methodology for risk levels compare to the Commission's role in assessing the enforcement of the Regulation seven years after its entry into force?", "reference_answer": "The AI Office focuses on creating a participative methodology for evaluating risk levels, emphasizing objective assessment, while the Commission's role involves reviewing and reporting on the enforcement of the Regulation after seven years. Both aim to enhance governance and accountability in AI, but they target different aspects: the AI Office on risk evaluation and the Commission on regulatory compliance.", "reference_context": "Document 427: EN\nUnited in diversity\nEN\n11.\nTo guide the evaluations and reviews referred to in paragraphs 1 to 7 of this Article, the \nAI Office shall undertake to develop an objective and participative methodology for the \nevaluation of risk levels based on the criteria outlined in the relevant Articles and the \ninclusion of new systems in:\n(a)\nthe list set out in Annex III, including the extension of existing area headings or \nthe addition of new area headings in that Annex;\n(b)\nthe list of prohibited practices set out in Article 5; and\n(c)\nthe list of AI systems requiring additional transparency measures pursuant to \nArticle 50.\n12.\nAny amendment to this Regulation pursuant to paragraph 10, or relevant delegated or \nimplementing acts, which concerns sectoral Union harmonisation legislation listed in \nSection B of Annex I shall take into account the regulatory specificities of each sector, \nand the existing governance, conformity assessment and enforcement mechanisms and \nauthorities established therein.\n13.\nBy \u2026 [seven years from the date of entry into force of this Regulation], the Commission \nshall carry out an assessment of the enforcement of this Regulation and shall report on \nit to the European Parliament, the Council and the European Economic and Social \nCommittee, taking into account the first years of application of this Regulation. On the \nbasis of the findings, that report shall, where appropriate, be accompanied by a proposal \nfor amendment of this Regulation with regard to the structure of enforcement and the \nneed for a Union agency to resolve any identified shortcomings.", "conversation_history": [], "metadata": {"question_type": "compare", "original_questions": [{"question": "What is the purpose of the AI Office as mentioned in the text?", "answer": "The AI Office shall undertake to develop an objective and participative methodology for the evaluation of risk levels based on the criteria outlined in the relevant Articles."}, {"question": "What will the Commission do seven years after the Regulation's entry into force?", "answer": "The Commission shall carry out an assessment of the enforcement of this Regulation and shall report on it to the European Parliament, the Council and the European Economic and Social Committee."}], "seed_document_id": 427, "topic": "AI Regulation"}}
{"id": "f472dd77-4256-4e71-9873-e270c0ece121", "question": "Under what conditions is a distributor considered a provider of a high-risk AI system?", "reference_answer": "A distributor is considered a provider of a high-risk AI system if they put their name or trademark on a high-risk AI system already placed on the market or put into service, or if they make a substantial modification to a high-risk AI system that has already been placed on the market or put into service.", "reference_context": "Document 91: EN\nUnited in diversity\nEN\n(84)\nTo ensure legal certainty, it is necessary to clarify that, under certain specific conditions, \nany distributor, importer, deployer or other third-party should be considered to be a \nprovider of a high-risk AI system and therefore assume all the relevant obligations. This \nwould be the case if that party puts its name or trademark on a high-risk AI system \nalready placed on the market or put into service, without prejudice to contractual \narrangements stipulating that the obligations are allocated otherwise. This would also be \nthe case if that party makes a substantial modification to a high-risk AI system that has \nalready been placed on the market or has already been put into service in a way that it \nremains a high-risk AI system in accordance with this Regulation, or if it modifies the \nintended purpose of an AI system, including a general-purpose AI system, which has not \nbeen classified as high-risk and has already been placed on the market or put into \nservice, in a way that the AI system becomes a high-risk AI system in accordance with \nthis Regulation. Those provisions should apply without prejudice to more specific \nprovisions established in certain Union harmonisation legislation based on the New \nLegislative Framework, together with which this Regulation should apply. For example, \nArticle 16(2) of Regulation (EU) 2017/745, establishing that certain changes should not \nbe considered to be modifications of a device that could affect its compliance with the \napplicable requirements, should continue to apply to high-risk AI systems that are \nmedical devices within the meaning of that Regulation.\n\nDocument 46: EN\nUnited in diversity\nEN\n(46)\nHigh-risk AI systems should only be placed on the Union market, put into service or used \nif they comply with certain mandatory requirements. Those requirements should ensure \nthat high-risk AI systems available in the Union or whose output is otherwise used in the \nUnion do not pose unacceptable risks to important Union public interests as recognised and \nprotected by Union law. On the basis of the New Legislative Framework, as clarified in \nthe Commission notice \u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules \n2022\u201d20, the general rule is that more than one legal act of Union harmonisation \nlegislation, such as Regulations (EU) 2017/74521 and (EU) 2017/74622 of the European \nParliament and of the Council or Directive 2006/42/EC of the European Parliament and \nof the Council23, may be applicable to one product, since the making available or putting \ninto service can take place only when the product complies with all applicable Union \nharmonisation legislation. To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.\n\nDocument 247: EN\nUnited in diversity\nEN\n3.\nIn the case of high-risk AI systems that are safety components of products covered by the \nUnion harmonisation legislation listed in Section A of Annex I, the product manufacturer \nshall be considered to be the provider of the high-risk AI system, and shall be subject to \nthe obligations under Article 16 under either of the following circumstances:\n(a)\nthe high-risk AI system is placed on the market together with the product under the \nname or trademark of the product manufacturer;\n(b)\nthe high-risk AI system is put into service under the name or trademark of the \nproduct manufacturer after the product has been placed on the market.\n4.\nThe provider of a high-risk AI system and the third party that supplies an AI system, \ntools, services, components, or processes that are used or integrated in a high-risk AI \nsystem shall, by written agreement, specify the necessary information, capabilities, \ntechnical access and other assistance based on the generally acknowledged state of the \nart, in order to enable the provider of the high-risk AI system to fully comply with the \nobligations set out in this Regulation. This paragraph shall not apply to third parties \nmaking accessible to the public tools, services, processes, or components, other than \ngeneral-purpose AI models, under a free and open-source licence.\n\nDocument 245: EN\nUnited in diversity\nEN\n5.\nUpon a reasoned request from a relevant competent authority, distributors of a high-risk \nAI system shall provide that authority with all the information and documentation \nregarding their actions pursuant to paragraphs 1 to 4 necessary to demonstrate the \nconformity of that system with the requirements set out in Section 2. \u258c\n6.\nDistributors shall cooperate with the relevant competent authorities in any action those \nauthorities take in relation to a high-risk AI system made available on the market by the \ndistributors, in particular to reduce or mitigate the risk posed by it.\nArticle 25\nResponsibilities along the AI value chain\n1.\nAny distributor, importer, deployer or other third-party shall be considered to be a provider \nof a high-risk AI system for the purposes of this Regulation and shall be subject to the \nobligations of the provider under Article 16, in any of the following circumstances:\n(a)\nthey put their name or trademark on a high-risk AI system already placed on the \nmarket or put into service, without prejudice to contractual arrangements \nstipulating that the obligations are otherwise allocated;\n(b)\nthey make a substantial modification to a high-risk AI system that has already been \nplaced on the market or has already been put into service in such a way that it \nremains a high-risk AI system pursuant to Article 6;", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 91, "topic": "High-Risk AI Systems"}}
{"id": "3f24d716-b804-447d-a4f5-b7daf860265b", "question": "What must the technical documentation of a high-risk AI system demonstrate before it is placed on the market?", "reference_answer": "The technical documentation must demonstrate that the high-risk AI system complies with the requirements set out in this Section.", "reference_context": "Document 218: EN\nUnited in diversity\nEN\n6.\n\u258cFor the development of high-risk AI systems not using techniques involving the training \nof AI models, paragraphs 2 to 5 apply only to the testing data sets.\nArticle 11\nTechnical documentation\n1.\nThe technical documentation of a high-risk AI system shall be drawn up before that system \nis placed on the market or put into service and shall be kept up-to date.\nThe technical documentation shall be drawn up in such a way as to demonstrate that the \nhigh-risk AI system complies with the requirements set out in this Section and to provide \nnational competent authorities and notified bodies with the necessary information in a \nclear and comprehensive form to assess the compliance of the AI system with those \nrequirements. It shall contain, at a minimum, the elements set out in Annex IV. SMEs, \nincluding start-ups, may provide the elements of the technical documentation specified \nin Annex IV in a simplified manner. To that end, the Commission shall establish a \nsimplified technical documentation form targeted at the needs of small and \nmicroenterprises. Where an SME, including a start-up, opts to provide the information \nrequired in Annex IV in a simplified manner, it shall use the form referred to in this \nparagraph. Notified bodies shall accept the form for the purposes of the conformity \nassessment.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 218, "topic": "High-Risk AI Regulation"}}
{"id": "9ad938c7-906e-4c35-b54b-3c629a0fe75e", "question": "What should be taken into account when adopting implementing acts concerning Artificial Intelligence systems that are safety components?", "reference_answer": "The requirements set out in Chapter III, Section 2, of Regulation (EU) 2024/\u2026 shall be taken into account.", "reference_context": "Document 419: EN\nUnited in diversity\nEN\n(3)\nin Article 43, the following paragraph is added:\n\u20184.\nWhen adopting implementing acts pursuant to paragraph 1 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n(4)\nin Article 47, the following paragraph is added:\n\u20183.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n(5)\nin Article 57, the following subparagraph is added:\n\u2018When adopting those implementing acts concerning Artificial Intelligence systems which \nare safety components within the meaning of Regulation (EU) 2024/\u2026+, the requirements \nset out in Chapter III, Section 2, of that Regulation shall be taken into account.\u2019;\n+\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n\nDocument 420: EN\nUnited in diversity\nEN\n(6)\nin Article 58, the following paragraph is added:\n\u20183.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019.\nArticle 109\nAmendment to Regulation (EU) 2019/2144\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\u20183.\nWhen adopting the implementing acts pursuant to paragraph 2, concerning artificial \nintelligence systems which are safety components within the meaning of Regulation (EU) \n2024/\u2026 of the European Parliament and of the Council*++, the requirements set out in \nChapter III, Section 2, of that Regulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n++\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 418: EN\nUnited in diversity\nEN\nArticle 108\nAmendments to Regulation (EU) 2018/1139\nRegulation (EU) 2018/1139 is amended as follows:\n(1)\nin Article 17, the following paragraph is added:\n\u20183.\nWithout prejudice to paragraph 2, when adopting implementing acts pursuant to \nparagraph 1 concerning Artificial Intelligence systems which are safety components \nwithin the meaning of Regulation (EU) 2024/\u2026 of the European Parliament and of \nthe Council*+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying \ndown harmonised rules on artificial intelligence and amending Regulations (EC) No \n300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 \nand (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) \n2020/1828 (Artificial Intelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019;\n(2)\nin Article 19, the following paragraph is added:\n\u20184.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026++, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n++\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n\nDocument 416: EN\nUnited in diversity\nEN\nArticle 106\nAmendment to Directive (EU) 2016/797\nIn Article 5 of Directive (EU) 2016/797, the following paragraph is added:\n\u201812.\nWhen adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to \nparagraph 11 concerning Artificial Intelligence systems which are safety components \nwithin the meaning of Regulation (EU) 2024/\u2026 of the European Parliament and of the \nCouncil*+, the requirements set out in Chapter III, Section 2, of that Regulation shall be \ntaken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 419, "topic": "Artificial Intelligence Regulation"}}
{"id": "4762aace-bf83-460b-bd76-f9fe9eb65c58", "question": "What conditions must be fulfilled for the Commission to adopt implementing acts establishing common specifications?", "reference_answer": "The Commission may adopt implementing acts establishing common specifications if it has requested one or more European standardisation organisations to draft a harmonised standard and the request has not been accepted, the harmonised standards are not delivered on time, the standards insufficiently address fundamental rights concerns, or the standards do not comply with the request.", "reference_context": "Document 276: EN\nUnited in diversity\nEN\nArticle 41\nCommon specifications\n1.\nThe Commission may adopt, implementing acts establishing common specifications for \nthe requirements set out in Section 2 of this Chapter or, as applicable, for the obligations \nset out in Sections 2 and 3 of Chapter V where the following conditions have been \nfulfilled:\n(a)\nthe Commission has requested, pursuant to Article 10(1) of Regulation (EU) \nNo 1025/2012, one or more European standardisation organisations to draft a \nharmonised standard for the requirements set out in Section 2 of this Chapter, or, \nas applicable, for the obligations set out in Sections 2 and 3 of Chapter V, and:\n(i)\nthe request has not been accepted by any of the European standardisation \norganisations; or\n(ii)\nthe harmonised standards addressing that request are not delivered within \nthe deadline set in accordance with Article 10(1) of Regulation (EU) \nNo 1025/2012; or\n(iii)\nthe relevant harmonised standards insufficiently address fundamental rights \nconcerns; or\n(iv)\nthe harmonised standards do not comply with the request; and", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 276, "topic": "AI Regulation"}}
{"id": "d667320c-d042-48fc-a28f-857a48318e57", "question": "What actions must a provider take if a serious incident is identified during real world testing?", "reference_answer": "The provider or prospective provider shall adopt immediate mitigation measures or, failing that, shall suspend the testing in real world conditions until such mitigation takes place, or otherwise terminate it.", "reference_context": "Document 334: EN\nUnited in diversity\nEN\n7.\nAny serious incident identified in the course of the testing in real world conditions shall \nbe reported to the national market surveillance authority in accordance with Article 73. \nThe provider or prospective provider shall adopt immediate mitigation measures or, \nfailing that, shall suspend the testing in real world conditions until such mitigation takes \nplace, or otherwise terminate it. The provider or prospective provider shall establish a \nprocedure for the prompt recall of the AI system upon such termination of the testing in \nreal world conditions.\n8.\nProviders or prospective providers shall notify the national market surveillance authority \nin the Member State where the testing in real world conditions is to be conducted of the \nsuspension or termination of the testing in real world conditions and of the final \noutcomes.\n9.\nThe provider or prospective provider shall be liable under applicable Union and national \nliability law for any damage caused in the course of their testing in real world \nconditions.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 334, "topic": "AI System Testing Regulations"}}
{"id": "c0dcb107-c0e4-427c-83f0-a62b37151e78", "question": "What is the purpose of the codes of practice encouraged by the AI Office?", "reference_answer": "The codes of practice should cover obligations for providers of general-purpose AI models and help to establish a risk taxonomy of the type and nature of the systemic risks at Union level, including their sources.", "reference_context": "Document 310: EN\nUnited in diversity\nEN\n(d)\nthe measures, procedures and modalities for the assessment and management of \nthe systemic risks at Union level, including the documentation thereof, which shall \nbe proportionate to the risks, take into consideration their severity and probability \nand take into account the specific challenges of tackling those risks in light of the \npossible ways in which such risks may emerge and materialise along the AI value \nchain.\n3.\nThe AI Office may invite all providers of general-purpose AI models, as well as relevant \nnational competent authorities, to participate in the drawing-up of codes of practice. \nCivil society organisations, industry, academia and other relevant stakeholders, such as \ndownstream providers and independent experts, may support the process.\n4.\nThe AI Office and the Board shall aim to ensure that the codes of practice clearly set out \ntheir specific objectives and contain commitments or measures, including key \nperformance indicators as appropriate, to ensure the achievement of those objectives, \nand that they take due account of the needs and interests of all interested parties, \nincluding affected persons, at Union level.\n\nDocument 116: EN\nUnited in diversity\nEN\n(116)\nThe AI Office should encourage and facilitate the drawing up, review and adaptation of \ncodes of practice, taking into account international approaches. All providers of general-\npurpose AI models could be invited to participate. To ensure that the codes of practice \nreflect the state of the art and duly take into account a diverse set of perspectives, the AI \nOffice should collaborate with relevant national competent authorities, and could, where \nappropriate, consult with civil society organisations and other relevant stakeholders and \nexperts, including the Scientific Panel, for the drawing up of such codes. Codes of \npractice should cover obligations for providers of general-purpose AI models and of \ngeneral-purpose AI models presenting systemic risks. In addition, as regards systemic \nrisks, codes of practice should help to establish a risk taxonomy of the type and nature of \nthe systemic risks at Union level, including their sources. Codes of practice should also \nbe focused on specific risk assessment and mitigation measures.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 310, "topic": "AI Regulation and Governance"}}
{"id": "adc5b58c-2d92-4c43-97bd-a14e2465dae2", "question": "What is the role of national competent authorities in establishing AI regulatory sandboxes?", "reference_answer": "National competent authorities shall ensure that they allocate sufficient resources to comply with the regulations effectively and in a timely manner, and they should cooperate with other relevant authorities and allow for the involvement of other actors within the AI ecosystem.", "reference_context": "Document 314: EN\nUnited in diversity\nEN\n2.\nAdditional AI regulatory sandboxes at regional or local level, or established jointly with \nthe competent authorities of other Member States may also be established.\n3.\nThe European Data Protection Supervisor may also establish an AI regulatory sandbox \nfor Union institutions, bodies, offices and agencies, and may exercise the roles and the \ntasks of national competent authorities in accordance with this Chapter.\n4.\nMember States shall ensure that the competent authorities referred to in paragraphs 1 \nand 2 allocate sufficient resources to comply with this Article effectively and in a timely \nmanner. Where appropriate, national competent authorities shall cooperate with other \nrelevant authorities, and may allow for the involvement of other actors within the AI \necosystem. This Article shall not affect other regulatory sandboxes established under \nUnion or national law. Member States shall ensure an appropriate level of cooperation \nbetween the authorities supervising those other sandboxes and the national competent \nauthorities.\n\nDocument 135: EN\nUnited in diversity\nEN\nWhere appropriate, national competent authorities establishing AI regulatory sandboxes \nshould cooperate with other relevant authorities, including those supervising the \nprotection of fundamental rights, and could allow for the involvement of other actors \nwithin the AI ecosystem such as national or European standardisation organisations, \nnotified bodies, testing and experimentation facilities, research and experimentation \nlabs, European Digital Innovation Hubs and relevant stakeholder and civil society \norganisations. To ensure uniform implementation across the Union and economies of \nscale, it is appropriate to establish common rules for the AI regulatory sandboxes\u2019 \nimplementation and a framework for cooperation between the relevant authorities \ninvolved in the supervision of the sandboxes. AI regulatory sandboxes established under \nthis Regulation should be without prejudice to other law allowing for the establishment \nof other sandboxes aiming to ensure compliance with law other than this Regulation. \nWhere appropriate, relevant competent authorities in charge of those other regulatory \nsandboxes should consider the benefits of using those sandboxes also for the purpose of \nensuring compliance of AI systems with this Regulation. Upon agreement between the \nnational competent authorities and the participants in the AI regulatory sandbox, testing \nin real world conditions may also be operated and supervised in the framework of the AI \nregulatory sandbox.\n\nDocument 319: EN\nUnited in diversity\nEN\n13.\nThe AI regulatory sandboxes shall be designed and implemented in such a way that, \nwhere relevant, they facilitate cross-border cooperation between national competent \nauthorities.\n14.\nNational competent authorities \u258c shall coordinate their activities and cooperate within the \nframework of the \u258c Board. \u258c\n15.\nNational competent authorities shall inform the AI Office and the Board of the \nestablishment of a sandbox, and may ask them for support and guidance. The AI Office \nshall make publicly available a list of planned and existing sandboxes and keep it up to \ndate in order to encourage more interaction in the AI regulatory sandboxes and cross-\nborder cooperation.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 314, "topic": "AI Regulatory Sandboxes"}}
{"id": "7ed4c029-d963-422c-97c5-e36aa6bb13fc", "question": "What obligations do providers of very large online platforms have regarding AI systems under this Regulation?", "reference_answer": "Providers of very large online platforms are obliged to assess potential systemic risks stemming from the design, functioning, and use of their services, including how the design of algorithmic systems used in the service may contribute to such risks, and are also obliged to take appropriate mitigating measures in observance of fundamental rights.", "reference_context": "Document 119: EN\nUnited in diversity\nEN\n(119)\nConsidering the quick pace of innovation and the technological evolution of digital \nservices in scope of different instruments of Union law in particular having in mind the \nusage and the perception of their recipients, the AI systems subject to this Regulation \nmay be provided as intermediary services or parts thereof within the meaning of \nRegulation (EU) 2022/2065, which should be interpreted in a technology-neutral \nmanner. For example, AI systems may be used to provide online search engines, in \nparticular, to the extent that an AI system such as an online chatbot performs searches \nof, in principle, all websites, then incorporates the results into its existing knowledge and \nuses the updated knowledge to generate a single output that combines different sources \nof information.\n(120)\nFurthermore, obligations placed on providers and deployers of certain AI systems in this \nRegulation to enable the detection and disclosure that the outputs of those systems are \nartificially generated or manipulated are particularly relevant to facilitate the effective \nimplementation of Regulation (EU) 2022/2065. This applies in particular as regards the \nobligations of providers of very large online platforms or very large online search \nengines to identify and mitigate systemic risks that may arise from the dissemination of \ncontent that has been artificially generated or manipulated, in particular risk of the \nactual or foreseeable negative effects on democratic processes, civic discourse and \nelectoral processes, including through disinformation.\n\nDocument 132: EN\nUnited in diversity\nEN\n(136)\nThe obligations placed on providers and deployers of certain AI systems in this \nRegulation to enable the detection and disclosure that the outputs of those systems are \nartificially generated or manipulated are particularly relevant to facilitate the effective \nimplementation of Regulation (EU) 2022/2065. This applies in particular as regards the \nobligations of providers of very large online platforms or very large online search \nengines to identify and mitigate systemic risks that may arise from the dissemination of \ncontent that has been artificially generated or manipulated, in particular the risk of the \nactual or foreseeable negative effects on democratic processes, civic discourse and \nelectoral processes, including through disinformation. The requirement to label content \ngenerated by AI systems under this Regulation is without prejudice to the obligation in \nArticle 16(6) of Regulation (EU) 2022/2065 for providers of hosting services to process \nnotices on illegal content received pursuant to Article 16(1) of that Regulation and \nshould not influence the assessment and the decision on the illegality of the specific \ncontent. That assessment should be performed solely with reference to the rules \ngoverning the legality of the content.\n(137)\nCompliance with the transparency obligations for the AI systems covered by this \nRegulation should not be interpreted as indicating that the use of the AI system or its \noutput is lawful under this Regulation or other Union and Member State law and should \nbe without prejudice to other transparency obligations for deployers of AI systems laid \ndown in Union or national law.\n\nDocument 118: EN\nUnited in diversity\nEN\n(118)\nThis Regulation regulates AI systems and AI models by imposing certain requirements \nand obligations for relevant market actors that are placing them on the market, putting \ninto service or use in the Union, thereby complementing obligations for providers of \nintermediary services that embed such systems or models into their services regulated by \nRegulation (EU) 2022/2065. To the extent that such systems or models are embedded \ninto designated very large online platforms or very large online search engines, they are \nsubject to the risk-management framework provided for in Regulation (EU) 2022/2065. \nConsequently, the corresponding obligations of this Regulation should be presumed to \nbe fulfilled, unless significant systemic risks not covered by Regulation (EU) 2022/2065 \nemerge and are identified in such models. Within this framework, providers of very large \nonline platforms and very large online search engines are obliged to assess potential \nsystemic risks stemming from the design, functioning and use of their services, including \nhow the design of algorithmic systems used in the service may contribute to such risks, \nas well as systemic risks stemming from potential misuses. Those providers are also \nobliged to take appropriate mitigating measures in observance of fundamental rights.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 119, "topic": "AI Regulation in the EU"}}
{"id": "c019b4f1-0cd3-476a-8e63-95a0f008eb01", "question": "What should providers of general-purpose AI models ensure regarding copyright compliance?", "reference_answer": "Providers that place general-purpose AI models on the Union market should ensure compliance with the relevant obligations in this Regulation and put in place a policy to comply with Union law on copyright and related rights.", "reference_context": "Document 109: EN\nUnited in diversity\nEN\n(109)\nCompliance with the obligations applicable to the providers of general-purpose AI \nmodels should be commensurate and proportionate to the type of model provider, \nexcluding the need for compliance for persons who develop or use models for non-\nprofessional or scientific research purposes, who should nevertheless be encouraged to \nvoluntarily comply with these requirements. Without prejudice to Union copyright law, \ncompliance with those obligations should take due account of the size of the provider \nand allow simplified ways of compliance for SMEs, including start-ups, that should not \nrepresent an excessive cost and not discourage the use of such models. In the case of a \nmodification or fine-tuning of a model, the obligations for providers of general-purpose \nAI models should be limited to that modification or fine-tuning, for example by \ncomplementing the already existing technical documentation with information on the \nmodifications, including new training data sources, as a means to comply with the value \nchain obligations provided in this Regulation.\n\nDocument 107: EN\nUnited in diversity\nEN\n(106)\nProviders that place general-purpose AI models on the Union market should ensure \ncompliance with the relevant obligations in this Regulation. To that end, providers of \ngeneral-purpose AI models should put in place a policy to comply with Union law on \ncopyright and related rights, in particular to identify and comply with the reservation of \nrights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790. \nAny provider placing a general-purpose AI model on the Union market should comply \nwith this obligation, regardless of the jurisdiction in which the copyright-relevant acts \nunderpinning the training of those general-purpose AI models take place. This is \nnecessary to ensure a level playing field among providers of general-purpose AI models \nwhere no provider should be able to gain a competitive advantage in the Union market \nby applying lower copyright standards than those provided in the Union.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 109, "topic": "AI Regulation and Governance"}}
{"id": "ed92ea14-d96d-4d7f-9308-37ec1f48ef81", "question": "What action can the Commission take if a notified body no longer meets the requirements for its notification?", "reference_answer": "The Commission shall inform the notifying Member State accordingly and request it to take the necessary corrective measures, including the suspension or withdrawal of the notification if necessary.", "reference_context": "Document 272: EN\nUnited in diversity\nEN\nArticle 37\nChallenge to the competence of notified bodies\n1.\nThe Commission shall, where necessary, investigate all cases where there are reasons to \ndoubt the competence of a notified body or the continued fulfilment by a notified body of \nthe requirements laid down in Article 31 and of its applicable responsibilities.\n2.\nThe notifying authority shall provide the Commission, on request, with all relevant \ninformation relating to the notification or the maintenance of the competence of the \nnotified body concerned.\n3.\nThe Commission shall ensure that all sensitive information obtained in the course of its \ninvestigations pursuant to this Article is treated confidentially in accordance with \nArticle 78.\n4.\nWhere the Commission ascertains that a notified body does not meet or no longer meets \nthe requirements for its notification, it shall inform the notifying Member State \naccordingly and request it to take the necessary corrective measures, including the \nsuspension or withdrawal of the notification if necessary. Where the Member State fails \nto take the necessary corrective measures, the Commission may, by means of an \nimplementing act, suspend, restrict or withdraw the designation. That implementing act \nshall be adopted in accordance with the examination procedure referred to in Article 98(2).", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 272, "topic": "Notified Bodies Regulations"}}
{"id": "ba9f70cc-9b49-4879-ac69-f0d6f81e01f4", "question": "What are some of the fundamental rights protected by the Charter that are relevant when classifying an AI system as high risk?", "reference_answer": "Some of the fundamental rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, the right to non-discrimination, the right to education, consumer protection, workers\u2019 rights, the rights of persons with disabilities, gender equality, intellectual property rights, the right to an effective remedy and to a fair trial, the right of defence and the presumption of innocence, and the right to good administration.", "reference_context": "Document 49: EN\nUnited in diversity\nEN\n(48)\nThe extent of the adverse impact caused by the AI system on the fundamental rights \nprotected by the Charter is of particular relevance when classifying an AI system as high \nrisk. Those rights include the right to human dignity, respect for private and family life, \nprotection of personal data, freedom of expression and information, freedom of \nassembly and of association, the right to non-discrimination, the right to education, \nconsumer protection, workers\u2019 rights, the rights of persons with disabilities, gender \nequality, intellectual property rights, the right to an effective remedy and to a fair trial, \nthe right of defence and the presumption of innocence, and the right to good \nadministration. In addition to those rights, it is important to highlight the fact that \nchildren have specific rights as enshrined in Article 24 of the Charter and in the United \nNations Convention on the Rights of the Child, further developed in the UNCRC \nGeneral Comment No 25 as regards the digital environment, both of which require \nconsideration of the children\u2019s vulnerabilities and provision of such protection and care \nas necessary for their well-being. The fundamental right to a high level of environmental \nprotection enshrined in the Charter and implemented in Union policies should also be \nconsidered when assessing the severity of the harm that an AI system can cause, \nincluding in relation to the health and safety of persons.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 49, "topic": "AI Regulation and Ethics"}}
{"id": "24093f56-c439-474d-93f5-4d03ea0550a1", "question": "What is the purpose of Regulation (EU) 2018/1860?", "reference_answer": "The purpose of Regulation (EU) 2018/1860 is on the use of the Schengen Information System for the return of illegally staying third-country nationals.", "reference_context": "Document 463: EN\nUnited in diversity\nEN\nANNEX X\nUnion legislative acts on large-scale IT systems in the area of Freedom, Security and Justice\n1.\nSchengen Information System\n(a)\nRegulation (EU) 2018/1860 of the European Parliament and of the Council of \n28 November 2018 on the use of the Schengen Information System for the return of \nillegally staying third-country nationals (OJ L 312, 7.12.2018, p. 1).\n(b)\nRegulation (EU) 2018/1861 of the European Parliament and of the Council of \n28 November 2018 on the establishment, operation and use of the Schengen \nInformation System (SIS) in the field of border checks, and amending the \nConvention implementing the Schengen Agreement, and amending and repealing \nRegulation (EC) No 1987/2006 (OJ L 312, 7.12.2018, p. 14).\n(c)\nRegulation (EU) 2018/1862 of the European Parliament and of the Council of \n28 November 2018 on the establishment, operation and use of the Schengen \nInformation System (SIS) in the field of police cooperation and judicial cooperation \nin criminal matters, amending and repealing Council Decision 2007/533/JHA, and \nrepealing Regulation (EC) No 1986/2006 of the European Parliament and of the \nCouncil and Commission Decision 2010/261/EU (OJ L 312, 7.12.2018, p. 56).", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 463, "topic": "EU Regulations and Directives"}}
{"id": "fa022c2a-14d0-4a05-867a-b4fb6da2d6ed", "question": "How long is the term of office for members of the advisory forum?", "reference_answer": "The term of office of the members of the advisory forum shall be two years, which may be extended by up to no more than four years.", "reference_context": "Document 347: EN\nUnited in diversity\nEN\n4.\nThe term of office of the members of the advisory forum shall be two years, which may \nbe extended by up to no more than four years.\n5.\nThe Fundamental Rights Agency, ENISA, the European Committee for Standardization \n(CEN), the European Committee for Electrotechnical Standardization (CENELEC), and \nthe European Telecommunications Standards Institute (ETSI) shall be permanent \nmembers of the advisory forum.\n6.\nThe advisory forum shall draw up its rules of procedure. It shall elect two co-chairs from \namong its members, in accordance with criteria set out in paragraph 2. The term of \noffice of the co-chairs shall be two years, renewable once.\n7.\nThe advisory forum shall hold meetings at least twice a year. The advisory forum may \ninvite experts and other stakeholders to its meetings.\n8.\nThe advisory forum may prepare opinions, recommendations and written contributions \nat the request of the Board or the Commission.\n9.\nThe advisory forum may establish standing or temporary sub-groups as appropriate for \nthe purpose of examining specific questions related to the objectives of this Regulation.\n10.\nThe advisory forum shall prepare an annual report on its activities. That report shall be \nmade publicly available.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 347, "topic": "AI Regulation"}}
{"id": "1baac891-5488-4591-8095-1897584195c6", "question": "When should the prohibitions and general provisions of the Regulation start to apply?", "reference_answer": "The prohibitions as well as the general provisions of this Regulation should already apply from six months from the date of entry into force of this Regulation.", "reference_context": "Document 170: EN\nUnited in diversity\nEN\n(179)\nThis Regulation should apply from \u2026 [two years from the date of entry into force of this \nRegulation]. However, taking into account the unacceptable risk associated with the use \nof AI in certain ways, the prohibitions as well as the general provisions of this \nRegulation should already apply from \u2026 [six months from the date of entry into force of \nthis Regulation]. While the full effect of those prohibitions follows with the \nestablishment of the governance and enforcement of this Regulation, anticipating the \napplication of the prohibitions is important to take account of unacceptable risks and to \nhave an effect on other procedures, such as in civil law. Moreover, the infrastructure \nrelated to the governance and the conformity assessment system should be operational \nbefore\u2026 [two years from the date of entry into force of this Regulation], therefore the \nprovisions on notified bodies and governance structure should apply from \u2026 [ 12 months \nfrom the date of entry into force of this Regulation]. Given the rapid pace of \ntechnological advancements and adoption of general-purpose AI models, obligations for \nproviders of general-purpose AI models should apply from ... [12 months from the date \nof entry into force of this Regulation]. Codes of practice should be ready by... [9 months \nfrom the date of entry into force of this Regulation] in view of enabling providers to \ndemonstrate compliance on time. The AI Office should ensure that classification rules \nand procedures are up to date in light of technological developments. In addition, \nMember States should lay down and notify to the Commission the rules on penalties, \nincluding administrative fines, and ensure that they are properly and effectively \nimplemented by the date of application of this Regulation. Therefore the provisions on \npenalties should apply from \u2026 [12 months from the date of entry into force of this \nRegulation].", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 170, "topic": "AI Regulation"}}
{"id": "097a96bb-42d2-488c-bc0a-c6d811996722", "question": "What requirements must be taken into account when adopting implementing acts concerning Artificial Intelligence systems that are safety components?", "reference_answer": "The requirements set out in Chapter III, Section 2, of Regulation (EU) 2024/\u2026 shall be taken into account.", "reference_context": "Document 418: EN\nUnited in diversity\nEN\nArticle 108\nAmendments to Regulation (EU) 2018/1139\nRegulation (EU) 2018/1139 is amended as follows:\n(1)\nin Article 17, the following paragraph is added:\n\u20183.\nWithout prejudice to paragraph 2, when adopting implementing acts pursuant to \nparagraph 1 concerning Artificial Intelligence systems which are safety components \nwithin the meaning of Regulation (EU) 2024/\u2026 of the European Parliament and of \nthe Council*+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying \ndown harmonised rules on artificial intelligence and amending Regulations (EC) No \n300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 \nand (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) \n2020/1828 (Artificial Intelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019;\n(2)\nin Article 19, the following paragraph is added:\n\u20184.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026++, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n++\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n\nDocument 420: EN\nUnited in diversity\nEN\n(6)\nin Article 58, the following paragraph is added:\n\u20183.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019.\nArticle 109\nAmendment to Regulation (EU) 2019/2144\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\u20183.\nWhen adopting the implementing acts pursuant to paragraph 2, concerning artificial \nintelligence systems which are safety components within the meaning of Regulation (EU) \n2024/\u2026 of the European Parliament and of the Council*++, the requirements set out in \nChapter III, Section 2, of that Regulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n++\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 415: EN\nUnited in diversity\nEN\nArticle 105\nAmendment to Directive 2014/90/EU\nIn Article 8 of Directive 2014/90/EU, the following paragraph is added:\n\u20185.\nFor Artificial Intelligence systems which are safety components within the meaning of \nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council*+, when carrying \nout its activities pursuant to paragraph 1 and when adopting technical specifications and \ntesting standards in accordance with paragraphs 2 and 3, the Commission shall take into \naccount the requirements set out in Chapter III, Section 2, of that Regulation.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 413: EN\nUnited in diversity\nEN\nArticle 103\nAmendment to Regulation (EU) No 167/2013\nIn Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added:\n\u2018When adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence \nsystems which are safety components within the meaning of Regulation (EU) 2024/\u2026 of the \nEuropean Parliament and of the Council*+, the requirements set out in Chapter III, Section 2, of that \nRegulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 418, "topic": "Artificial Intelligence Regulation"}}
{"id": "029bf5d7-9547-4444-85d2-db6a4912afb1", "question": "What must high-risk AI systems comply with before being placed on the Union market?", "reference_answer": "High-risk AI systems should only be placed on the Union market, put into service or used if they comply with certain mandatory requirements that ensure they do not pose unacceptable risks to important Union public interests as recognised and protected by Union law.", "reference_context": "Document 74: EN\nUnited in diversity\nEN\nThis calls for a simultaneous and complementary application of the various legislative \nacts. To ensure consistency and to avoid an unnecessary administrative burden and \nunnecessary costs, providers of a product that contains one or more high-risk AI system, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nbased on the New Legislative Framework and listed in an annex to this Regulation \napply, should have flexibility with regard to operational decisions on how to ensure \ncompliance of a product that contains one or more AI systems with all the applicable \nrequirements of that Union harmonised legislation in an optimal manner. That \nflexibility could mean, for example a decision by the provider to integrate a part of the \nnecessary testing and reporting processes, information and documentation required \nunder this Regulation into already existing documentation and procedures required \nunder existing Union harmonisation legislation based on the New Legislative \nFramework and listed in an annex to this Regulation. This should not, in any way, \nundermine the obligation of the provider to comply with all the applicable requirements.\n\nDocument 46: EN\nUnited in diversity\nEN\n(46)\nHigh-risk AI systems should only be placed on the Union market, put into service or used \nif they comply with certain mandatory requirements. Those requirements should ensure \nthat high-risk AI systems available in the Union or whose output is otherwise used in the \nUnion do not pose unacceptable risks to important Union public interests as recognised and \nprotected by Union law. On the basis of the New Legislative Framework, as clarified in \nthe Commission notice \u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules \n2022\u201d20, the general rule is that more than one legal act of Union harmonisation \nlegislation, such as Regulations (EU) 2017/74521 and (EU) 2017/74622 of the European \nParliament and of the Council or Directive 2006/42/EC of the European Parliament and \nof the Council23, may be applicable to one product, since the making available or putting \ninto service can take place only when the product complies with all applicable Union \nharmonisation legislation. To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.\n\nDocument 73: EN\nUnited in diversity\nEN\n(64)\nTo mitigate the risks from high-risk AI systems placed on the market or put into service \nand to ensure a high level of trustworthiness, certain mandatory requirements should \napply to high-risk AI systems, taking into account the intended purpose and the context of \nuse of the AI system and according to the risk-management system to be established by the \nprovider. The measures adopted by the providers to comply with the mandatory \nrequirements of this Regulation should take into account the generally acknowledged \nstate of the art on AI, be proportionate and effective to meet the objectives of this \nRegulation. Based on the New Legislative Framework, as clarified in Commission notice \n\u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules 2022\u201d, the general rule is \nthat more than one legal act of Union harmonisation legislation may be applicable to \none product, since the making available or putting into service can take place only when \nthe product complies with all applicable Union harmonisation legislation. The hazards \nof AI systems covered by the requirements of this Regulation concern different aspects \nthan the existing Union harmonisation legislation and therefore the requirements of this \nRegulation would complement the existing body of the Union harmonisation legislation. \nFor example, machinery or medical devices products incorporating an AI system might \npresent risks not addressed by the essential health and safety requirements set out in the \nrelevant Union harmonised legislation, as that sectoral law does not deal with risks \nspecific to AI systems.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 74, "topic": "AI Regulation and Ethics"}}
{"id": "323f0339-adaf-4c78-b404-e2488b2a1869", "question": "What information is required about the data used for training, testing, and validation of a general-purpose AI model?", "reference_answer": "Information on the data used for training, testing, and validation, where applicable, including the type and provenance of data and curation methodologies.", "reference_context": "Document 472: EN\nUnited in diversity\nEN\n(f)\nthe architecture and number of parameters;\n(g)\nthe modality (e.g., text, image) and format of inputs and outputs;\n(h)\nthe licence for the model.\n2.\nA description of the elements of the model and of the process for its development, \nincluding:\n(a)\nthe technical means (e.g., instructions for use, infrastructure, tools) required for \nthe general-purpose AI model to be integrated into AI systems;\n(b)\nthe modality (e.g., text, image, etc.) and format of the inputs and outputs and their \nmaximum size (e.g., context window length, etc.);\n(c)\ninformation on the data used for training, testing and validation, where applicable, \nincluding the type and provenance of data and curation methodologies.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 472, "topic": "Others"}}
{"id": "ed3fa948-cc3d-4e8d-8093-f3d38ee231c7", "question": "What is the basis for conformity assessment according to the document?", "reference_answer": "Conformity based on an assessment of the quality management system and an assessment of the technical documentation.", "reference_context": "Document 451: EN\nUnited in diversity\nEN\nANNEX VII\nConformity based on an assessment of the quality management system \nand an assessment of the technical documentation\n1.\nIntroduction\nConformity based on an assessment of the quality management system and an assessment \nof the technical documentation is the conformity assessment procedure based on points 2 \nto 5.\n2.\nOverview\nThe approved quality management system for the design, development and testing of AI \nsystems pursuant to Article 17 shall be examined in accordance with point 3 and shall be \nsubject to surveillance as specified in point 5. The technical documentation of the AI \nsystem shall be examined in accordance with point 4.\n3.\nQuality management system\n3.1.\nThe application of the provider shall include:\n(a)\nthe name and address of the provider and, if the application is lodged by an \nauthorised representative, also their name and address;", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 451, "topic": "Others"}}
{"id": "31177f47-44e2-4023-aa22-cb8cad0080e0", "question": "What must the Commission and other authorities respect in relation to confidentiality according to Article 78?", "reference_answer": "The Commission, market surveillance authorities and notified bodies must respect the confidentiality of information and data obtained in carrying out their tasks and activities to protect, in particular, the intellectual property rights and confidential business information or trade secrets of a natural or legal person.", "reference_context": "Document 372: EN\nUnited in diversity\nEN\nArticle 78\nConfidentiality\n1.\nThe Commission, market surveillance authorities and notified bodies and any other \nnatural or legal person involved in the application of this Regulation shall, in accordance \nwith Union or national law, respect the confidentiality of information and data obtained in \ncarrying out their tasks and activities in such a manner as to protect, in particular:\n(a)\nthe intellectual property rights and confidential business information or trade secrets \nof a natural or legal person, including source code, except in the cases referred to in \nArticle 5 of Directive (EU) 2016/943 of the European Parliament and of the \nCouncil57 ;\n57\nDirective (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on \nthe protection of undisclosed know-how and business information (trade secrets) against \ntheir unlawful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 372, "topic": "AI Regulation and Compliance"}}
{"id": "c1735d84-1beb-400e-9d24-64452590c33f", "question": "What responsibilities should deployers have regarding high-risk AI systems?", "reference_answer": "Deployers should take appropriate technical and organisational measures to ensure they use high-risk AI systems in accordance with the instructions of use, monitor the functioning of the AI systems, and maintain record-keeping. They should also ensure that persons assigned to implement the instructions have the necessary competence, training, and authority.", "reference_context": "Document 95: EN\nUnited in diversity\nEN\n(91)\nGiven the nature of AI systems and the risks to safety and fundamental rights possibly \nassociated with their use, including as regards the need to ensure proper monitoring of the \nperformance of an AI system in a real-life setting, it is appropriate to set specific \nresponsibilities for deployers. Deployers should in particular take appropriate technical \nand organisational measures to ensure they use high-risk AI systems in accordance with \nthe instructions of use and certain other obligations should be provided for with regard to \nmonitoring of the functioning of the AI systems and with regard to record-keeping, as \nappropriate. Furthermore, deployers should ensure that the persons assigned to \nimplement the instructions for use and human oversight as set out in this Regulation \nhave the necessary competence, in particular an adequate level of AI literacy, training \nand authority to properly fulfil those tasks. Those obligations should be without \nprejudice to other deployer obligations in relation to high-risk AI systems under Union \nor national law.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 95, "topic": "High-Risk AI Systems"}}
{"id": "e4316d57-1ec1-4ddd-86c8-f5527a05e4c8", "question": "What rights and remedies are unaffected by the Regulation concerning AI systems?", "reference_answer": "All rights and remedies provided for by Union law to consumers and other persons on whom AI systems may have a negative impact, including as regards the compensation of possible damages pursuant to Council Directive 85/374/EEC, remain unaffected and fully applicable.", "reference_context": "Document 8: EN\nUnited in diversity\nEN\nAs a consequence, all rights and remedies provided for by such Union law to consumers, \nand other persons on whom AI systems may have a negative impact, including as \nregards the compensation of possible damages pursuant to Council Directive \n85/374/EEC10 remain unaffected and fully applicable. Furthermore, in the context of \nemployment and protection of workers, this Regulation should therefore not affect \nUnion law on social policy and national labour law, in compliance with Union law, \nconcerning employment and working conditions, including health and safety at work \nand the relationship between employers and workers. This Regulation should also not \naffect the exercise of fundamental rights as recognised in the Member States and at \nUnion level, including the right or freedom to strike or to take other action covered by \nthe specific industrial relations systems in Member States as well as the right to \nnegotiate, to conclude and enforce collective agreements or to take collective action in \naccordance with national law. \n10\nCouncil Directive 85/374/EEC of 25 July 1985 on the approximation of the laws, \nregulations and administrative provisions of the Member States concerning liability for \ndefective products (OJ L 210, 7.8.1985, p. 29).", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 8, "topic": "AI Regulation and Compliance"}}
{"id": "50558770-7ab2-432a-82ab-184c6e431158", "question": "What criteria determine whether a space is considered publicly accessible according to the Regulation?", "reference_answer": "A space is considered publicly accessible if it is accessible to an undetermined number of natural persons, regardless of ownership or activity. Access may be subject to predetermined conditions, such as ticket purchase or registration. In contrast, spaces limited to specific persons through law or authority, or where access is restricted by signs, are not publicly accessible.", "reference_context": "Document 19: EN\nUnited in diversity\nEN\n(19)\nFor the purposes of this Regulation the notion of \u2018publicly accessible space\u2019 should be \nunderstood as referring to any physical space that is accessible to an undetermined \nnumber of natural persons, and irrespective of whether the space in question is privately \nor publicly owned, irrespective of the activity for which the space may be used, such as \nfor commerce, for example, shops, restaurants, caf\u00e9s; for services, for example, banks, \nprofessional activities, hospitality; for sport, for example, swimming pools, gyms, \nstadiums; for transport, for example, bus, metro and railway stations, airports, means of \ntransport; for entertainment, for example, cinemas, theatres, museums, concert and \nconference halls; or for leisure or otherwise, for example, public roads and squares, \nparks, forests, playgrounds. A space should also be classified as being publicly accessible \nif, regardless of potential capacity or security restrictions, access is subject to certain \npredetermined conditions which can be fulfilled by an undetermined number of persons, \nsuch as the purchase of a ticket or title of transport, prior registration or having a certain \nage. In contrast, a space should not be considered to be publicly accessible if access is \nlimited to specific and defined natural persons through either Union or national law \ndirectly related to public safety or security or through the clear manifestation of will by \nthe person having the relevant authority over the space. The factual possibility of access \nalone, such as an unlocked door or an open gate in a fence, does not imply that the space \nis publicly accessible in the presence of indications or circumstances suggesting the \ncontrary, such as. signs prohibiting or restricting access. Company and factory premises, \nas well as offices and workplaces that are intended to be accessed only by relevant \nemployees and service providers, are spaces that are not publicly accessible. Publicly \naccessible spaces should not include prisons or border control. Some other spaces may \ncomprise both publicly accessible and non-publicly accessible spaces, such as the \nhallway of a private residential building necessary to access a doctor's office or an \nairport. Online spaces are not covered, as they are not physical spaces.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 19, "topic": "Biometric Data and AI"}}
{"id": "308f607e-a24a-4f5c-b62d-3d98fa268264", "question": "What should providers of high-risk AI systems have in place to improve their systems based on user experience?", "reference_answer": "All providers should have a post-market monitoring system in place.", "reference_context": "Document 357: EN\nUnited in diversity\nEN\nCHAPTER IX\nPOST-MARKET MONITORING, INFORMATION SHARING AND \nMARKET SURVEILLANCE\nSection 1\nPost-market monitoring\nArticle 72\nPost-market monitoring by providers and post-market monitoring plan for high-risk AI systems\n1.\nProviders shall establish and document a post-market monitoring system in a manner that \nis proportionate to the nature of the AI technologies and the risks of the high-risk AI \nsystem.\n2.\nThe post-market monitoring system shall actively and systematically collect, document and \nanalyse relevant data which may be provided by deployers or which may be collected \nthrough other sources on the performance of high-risk AI systems throughout their \nlifetime, and which allow the provider to evaluate the continuous compliance of AI \nsystems with the requirements set out in Chapter III, Section 2. Where relevant, post-\nmarket monitoring shall include an analysis of the interaction with other AI systems. \nThis obligation shall not cover sensitive operational data of deployers which are law-\nenforcement authorities.\n\nDocument 149: EN\nUnited in diversity\nEN\n(155)\nIn order to ensure that providers of high-risk AI systems can take into account the \nexperience on the use of high-risk AI systems for improving their systems and the design \nand development process or can take any possible corrective action in a timely manner, all \nproviders should have a post-market monitoring system in place. Where relevant, post-\nmarket monitoring should include an analysis of the interaction with other AI systems \nincluding other devices and software. Post-market monitoring should not cover sensitive \noperational data of deployers which are law enforcement authorities. This system is also \nkey to ensure that the possible risks emerging from AI systems which continue to \u2018learn\u2019 \nafter being placed on the market or put into service can be more efficiently and timely \naddressed. In this context, providers should also be required to have a system in place to \nreport to the relevant authorities any serious incidents resulting from the use of their AI \nsystems, meaning incident or malfunctioning leading to death or serious damage to \nhealth, serious and irreversible disruption of the management and operation of critical \ninfrastructure, infringements of obligations under Union law intended to protect \nfundamental rights or serious damage to property or the environment.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 357, "topic": "AI Regulation and Compliance"}}
{"id": "e3fec2e5-2029-4047-82ab-439be4a0bb1c", "question": "What is the purpose of establishing uniform obligations for operators of AI systems within the EU?", "reference_answer": "The purpose is to ensure a consistent and high level of protection throughout the Union, achieve trustworthy AI, and prevent divergences that may hamper the free circulation, innovation, deployment, and uptake of AI systems and related products and services within the internal market.", "reference_context": "Document 3: EN\nUnited in diversity\nEN\n(3)\n\u258c AI systems \u258c can be easily deployed in a large variety of sectors of the economy and \nmany parts of society, including across borders, and can easily circulate throughout the \nUnion. Certain Member States have already explored the adoption of national rules to \nensure that AI is trustworthy and safe and is developed and used in accordance with \nfundamental rights obligations. Diverging national rules may lead to the fragmentation of \nthe internal market and may decrease legal certainty for operators that develop, import or \nuse AI systems. A consistent and high level of protection throughout the Union should \ntherefore be ensured in order to achieve trustworthy AI, while divergences hampering the \nfree circulation, innovation, deployment and the uptake of AI systems and related \nproducts and services within the internal market should be prevented by laying down \nuniform obligations for operators and guaranteeing the uniform protection of overriding \nreasons of public interest and of rights of persons throughout the internal market on the \nbasis of Article 114 of the Treaty on the Functioning of the European Union (TFEU). To \nthe extent that this Regulation contains specific rules on the protection of individuals with \nregard to the processing of personal data concerning restrictions of the use of AI systems \nfor remote biometric identification for the purpose of law enforcement, of the use of AI \nsystems for risk assessments of natural persons for the purpose of law enforcement and \nof the use of AI systems of biometric categorisation for the purpose of law enforcement, it \nis appropriate to base this Regulation, in so far as those specific rules are concerned, on \nArticle 16 TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is \nappropriate to consult the European Data Protection Board.\n\nDocument 22: EN\nUnited in diversity\nEN\n(21)\nIn order to ensure a level playing field and an effective protection of rights and freedoms of \nindividuals across the Union, the rules established by this Regulation should apply to \nproviders of AI systems in a non-discriminatory manner, irrespective of whether they are \nestablished within the Union or in a third country, and to deployers of AI systems \nestablished within the Union.\n(22)\nIn light of their digital nature, certain AI systems should fall within the scope of this \nRegulation even when they are not placed on the market, put into service, or used in the \nUnion. This is the case, for example, where an operator established in the Union contracts \ncertain services to an operator established in a third country in relation to an activity to be \nperformed by an AI system that would qualify as high-risk \u258c . In those circumstances, the \nAI system used in a third country by the operator could process data lawfully collected in \nand transferred from the Union, and provide to the contracting operator in the Union the \noutput of that AI system resulting from that processing, without that AI system being \nplaced on the market, put into service or used in the Union. To prevent the circumvention \nof this Regulation and to ensure an effective protection of natural persons located in the \nUnion, this Regulation should also apply to providers and deployers of AI systems that are \nestablished in a third country, to the extent the output produced by those systems is \nintended to be used in the Union.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 3, "topic": "AI Regulation in the EU"}}
{"id": "969f05a9-774d-4833-98a2-64b0196cf6c4", "question": "What should a provider do if their general-purpose AI model meets the threshold for high-impact capabilities?", "reference_answer": "The provider should notify the AI Office at the latest two weeks after the requirements are met or it becomes known that a general-purpose AI model will meet the requirements that lead to the presumption.", "reference_context": "Document 111: EN\nUnited in diversity\nEN\n(111)\nIt is appropriate to establish a methodology for the classification of general-purpose AI \nmodels as general-purpose AI model with systemic risks. Since systemic risks result from \nparticularly high capabilities, a general-purpose AI model should be considered to \npresent systemic risks if it has high-impact capabilities, evaluated on the basis of \nappropriate technical tools and methodologies, or significant impact on the internal \nmarket due to its reach. High-impact capabilities in general-purpose AI models means \ncapabilities that match or exceed the capabilities recorded in the most advanced general-\npurpose AI models. The full range of capabilities in a model could be better understood \nafter its placing on the market or when deployers interact with the model. According to \nthe state of the art at the time of entry into force of this Regulation, the cumulative \namount of computation used for the training of the general-purpose AI model measured \nin floating point operations is one of the relevant approximations for model capabilities. \nThe cumulative amount of computation used for training includes the computation used \nacross the activities and methods that are intended to enhance the capabilities of the \nmodel prior to deployment, such as pre-training, synthetic data generation and fine-\ntuning. Therefore, an initial threshold of floating point operations should be set, which, \nif met by a general-purpose AI model, leads to a presumption that the model is a \ngeneral-purpose AI model with systemic risks. This threshold should be adjusted over \ntime to reflect technological and industrial changes, such as algorithmic improvements \nor increased hardware efficiency, and should be supplemented with benchmarks and \nindicators for model capability.\n\nDocument 113: EN\nUnited in diversity\nEN\n(112)\nIt is also necessary to clarify a procedure for the classification of a general-purpose AI \nmodel with systemic risks. A general-purpose AI model that meets the applicable \nthreshold for high-impact capabilities should be presumed to be a general-purpose AI \nmodels with systemic risk. The provider should notify the AI Office at the latest two \nweeks after the requirements are met or it becomes known that a general-purpose AI \nmodel will meet the requirements that lead to the presumption. This is especially relevant \nin relation to the threshold of floating point operations because training of general-\npurpose AI models takes considerable planning which includes the upfront allocation of \ncompute resources and, therefore, providers of general-purpose AI models are able to \nknow if their model would meet the threshold before the training is completed. In the \ncontext of that notification, the provider should be able to demonstrate that, because of \nits specific characteristics, a general-purpose AI model exceptionally does not present \nsystemic risks, and that it thus should not be classified as a general-purpose AI model \nwith systemic risks. That information is valuable for the AI Office to anticipate the \nplacing on the market of general-purpose AI models with systemic risks and the \nproviders can start to engage with the AI Office early on. That information is especially \nimportant with regard to general-purpose AI models that are planned to be released as \nopen-source, given that, after the open-source model release, necessary measures to \nensure compliance with the obligations under this Regulation may be more difficult to \nimplement.\n\nDocument 112: EN\nUnited in diversity\nEN\nTo inform this, the AI Office should engage with the scientific community, industry, civil \nsociety and other experts. Thresholds, as well as tools and benchmarks for the \nassessment of high-impact capabilities, should be strong predictors of generality, its \ncapabilities and associated systemic risk of general-purpose AI models, and could take \ninto account the way the model will be placed on the market or the number of users it \nmay affect. To complement this system, there should be a possibility for the Commission \nto take individual decisions designating a general-purpose AI model as a general-\npurpose AI model with systemic risk if it is found that such model has capabilities or an \nimpact equivalent to those captured by the set threshold. That decision should be taken \non the basis of an overall assessment of the criteria for the designation of a general-\npurpose AI model with systemic risk set out in an annex to this Regulation, such as \nquality or size of the training data set, number of business and end users, its input and \noutput modalities, its level of autonomy and scalability, or the tools it has access to. \nUpon a reasoned request of a provider whose model has been designated as a general-\npurpose AI model with systemic risk, the Commission should take the request into \naccount and may decide to reassess whether the general-purpose AI model can still be \nconsidered to present systemic risks.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 111, "topic": "Systemic Risks in AI"}}
{"id": "d2f5718e-030b-4df5-9e15-dfa014ff1111", "question": "What is the maximum validity period for certificates of AI systems covered by Annex I?", "reference_answer": "The maximum validity period for certificates of AI systems covered by Annex I shall not exceed five years.", "reference_context": "Document 284: EN\nUnited in diversity\nEN\n2.\nCertificates shall be valid for the period they indicate, which shall not exceed five years for \nAI systems covered by Annex I, and four years for AI systems covered by Annex III. At \nthe request of the provider, the validity of a certificate may be extended for further periods, \neach not exceeding five years for AI systems covered by Annex I, and four years for AI \nsystems covered by Annex III, based on a re-assessment in accordance with the applicable \nconformity assessment procedures. Any supplement to a certificate shall remain valid, \nprovided that the certificate which it supplements is valid.\n3.\nWhere a notified body finds that an AI system no longer meets the requirements set out in \nSection 2, it shall, taking account of the principle of proportionality, suspend or withdraw \nthe certificate issued or impose restrictions on it, unless compliance with those \nrequirements is ensured by appropriate corrective action taken by the provider of the \nsystem within an appropriate deadline set by the notified body. The notified body shall \ngive reasons for its decision.\n\u258cAn appeal procedure against decisions of the notified bodies, including on conformity \ncertificates issued, shall be available.", "conversation_history": [], "metadata": {"question_type": "simple", "seed_document_id": 284, "topic": "Others"}}
{"id": "e18a75a7-8759-4301-9d28-8c02e7fae7bd", "question": "Imagine a scenario where Clara receives a decision from a company based on the output of a high-risk AI system that significantly impacts her employment status. Given this situation, does Clara have the right to obtain a clear and meaningful explanation of the AI system's role in the decision-making process under Article 86 of the EU AI Act?", "reference_answer": "Yes, Clara has the right to obtain a clear and meaningful explanation of the AI system's role in the decision-making process, as her situation falls within the scope of Article 86, which applies to decisions that produce legal effects or significantly affect individuals, unless exceptions or restrictions apply under Union or national law.", "reference_context": "Document 386: EN\nUnited in diversity\nEN\nArticle 86\nRight to explanation of individual decision-making\n1.\nAny affected person subject to a decision which is taken by the deployer on the basis of \nthe output from a high-risk AI system listed in Annex III, with the exception of systems \nlisted under point 2 thereof, and which produces legal effects or similarly significantly \naffects that person in a way that they consider to have an adverse impact on their health, \nsafety or fundamental rights shall have the right to obtain from the deployer clear and \nmeaningful explanations of the role of the AI system in the decision-making procedure \nand the main elements of the decision taken.\n2.\nParagraph 1 shall not apply to the use of AI systems for which exceptions from, or \nrestrictions to, the obligation under that paragraph follow from Union or national law in \ncompliance with Union law.\n3.\nThis Article shall apply only to the extent that the right referred to in paragraph 1 is not \notherwise provided for under Union law.\n\nDocument 165: EN\nUnited in diversity\nEN\n(171)\nAffected persons should have the right to obtain an explanation where a deployer\u2019s \ndecision is based mainly upon the output from certain high-risk AI systems that fall \nwithin the scope of this Regulation and where that decision produces legal effects or \nsimilarly significantly affects those persons in a way that they consider to have an \nadverse impact on their health, safety or fundamental rights. That explanation should be \nclear and meaningful and should provide a basis on which the affected persons are able \nto exercise their rights. The right to obtain an explanation should not apply to the use of \nAI systems for which exceptions or restrictions follow from Union or national law and \nshould apply only to the extent this right is not already provided for under Union law.\n(172)\nPersons acting as whistleblowers on the infringements of this Regulation should be \nprotected under the Union law. Directive (EU) 2019/1937 of the European Parliament \nand of the Council54 should therefore apply to the reporting of infringements of this \nRegulation and the protection of persons reporting such infringements.\n54\nDirective (EU) 2019/1937 of the European Parliament and of the Council of 23 October \n2019 on the protection of persons who report breaches of Union law (OJ L 305, 26.11.2019, \np. 17).", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 386, "topic": "AI Regulation and Ethics"}}
{"id": "4a3bf0cc-992e-427a-9df5-56fdf8575ed5", "question": "Imagine a company developing an AI system that profiles individuals to predict the likelihood of them committing a crime based solely on their personality traits. Would this AI system be compliant with the EU AI Act given the specific prohibitions outlined in the context?", "reference_answer": "No, the AI system would not be compliant with the EU AI Act, as it is prohibited to use AI systems for making risk assessments of natural persons based solely on profiling or assessing their personality traits.", "reference_context": "Document 195: EN\nUnited in diversity\nEN\n(d)\nthe placing on the market, the putting into service for this specific purpose, or the \nuse of an AI system for making risk assessments of natural persons in order to \nassess or predict the risk of a natural person committing a criminal offence, based \nsolely on the profiling of a natural person or on assessing their personality traits \nand characteristics; this prohibition shall not apply to AI systems used to support \nthe human assessment of the involvement of a person in a criminal activity, which \nis already based on objective and verifiable facts directly linked to a criminal \nactivity;\n(e)\nthe placing on the market, the putting into service for this specific purpose, or the \nuse of AI systems that create or expand facial recognition databases through the \nuntargeted scraping of facial images from the internet or CCTV footage;\n(f)\nthe placing on the market, the putting into service for this specific purpose, or the \nuse of AI systems to infer emotions of a natural person in the areas of workplace \nand education institutions, except where the use of the AI system is intended to be \nput in place or into the market for medical or safety reasons;", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 195, "topic": "Biometric Data and AI"}}
{"id": "9b1d4f1c-55e3-443e-9908-ed14b8f0b80c", "question": "Suppose the European Commission is assessing the effectiveness of voluntary codes of conduct for AI systems that are not classified as high-risk. Given this scenario, what are the mandated intervals for the Commission to conduct such evaluations under the Regulation?", "reference_answer": "The Commission is required to evaluate the impact and effectiveness of voluntary codes of conduct every three years, starting four years from the date of entry into force of this Regulation.", "reference_context": "Document 426: EN\nUnited in diversity\nEN\n7.\nBy \u2026 [four years from the date of entry into force of this Regulation] and every three \nyears thereafter, the Commission shall evaluate the impact and effectiveness of voluntary \ncodes of conduct to foster the application of the requirements set out in Chapter III, Section \n2 for AI systems other than high-risk AI systems and possibly other additional \nrequirements for AI systems other than high-risk AI systems, including as regards \nenvironmental sustainability.\n8.\nFor the purposes of paragraphs 1 to 7, the Board, the Member States and national \ncompetent authorities shall provide the Commission with information upon its request and \nwithout undue delay.\n9.\nIn carrying out the evaluations and reviews referred to in paragraphs 1 to 7, the \nCommission shall take into account the positions and findings of the Board, of the \nEuropean Parliament, of the Council, and of other relevant bodies or sources.\n10.\nThe Commission shall, if necessary, submit appropriate proposals to amend this \nRegulation, in particular taking into account developments in technology, the effect of AI \nsystems on health and safety, and on fundamental rights, and in light of the state of \nprogress in the information society.\n\nDocument 167: EN\nUnited in diversity\nEN\n(174)\nGiven the rapid technological developments and the technical expertise required to \neffectively apply this Regulation, the Commission should evaluate and review this \nRegulation by ... [five years from the date of entry into force of this Regulation] and \nevery four years thereafter and report to the European Parliament and the Council. In \naddition, taking into account the implications for the scope of this Regulation, the \nCommission should carry out an assessment of the need to amend the list of high-risk AI \nsystems and the list of prohibited practices once a year. Moreover, by ... [four years from \nthe date of entry into force of this Regulation] and every four years thereafter, the \nCommission should evaluate and report to the European Parliament and to the Council \non the need to amend the list of high-risk areas headings in the annex to this \nRegulation, the AI systems within the scope of the transparency obligations, the \neffectiveness of the supervision and governance system and the progress on the \ndevelopment of standardisation deliverables on energy efficient development of general-\npurpose AI models, including the need for further measures or actions. Finally, by ... \n[four years from the date of entry into force of this Regulation] and every three years \nthereafter, the Commission should evaluate the impact and effectiveness of voluntary \ncodes of conduct to foster the application of the requirements provided for high-risk AI \nsystems in the case of AI systems other than high-risk AI systems and possibly other \nadditional requirements for such AI systems.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 426, "topic": "AI Regulation"}}
{"id": "28b5b2f9-5b8d-489b-b82c-7b9a955aa37f", "question": "Imagine a scenario where an AI system used in a children's educational app inadvertently exposes sensitive personal data, leading to significant privacy violations. In this case, would the assessment of the AI system's risk level prioritize the right to privacy over the right to an effective remedy, according to the EU Charter of Fundamental Rights?", "reference_answer": "The assessment of the AI system's risk level would prioritize the extent of the adverse impact on the fundamental rights protected by the Charter, including the right to privacy. However, the right to an effective remedy is also crucial, and both rights should be considered in the overall evaluation of the AI system's impact.", "reference_context": "Document 49: EN\nUnited in diversity\nEN\n(48)\nThe extent of the adverse impact caused by the AI system on the fundamental rights \nprotected by the Charter is of particular relevance when classifying an AI system as high \nrisk. Those rights include the right to human dignity, respect for private and family life, \nprotection of personal data, freedom of expression and information, freedom of \nassembly and of association, the right to non-discrimination, the right to education, \nconsumer protection, workers\u2019 rights, the rights of persons with disabilities, gender \nequality, intellectual property rights, the right to an effective remedy and to a fair trial, \nthe right of defence and the presumption of innocence, and the right to good \nadministration. In addition to those rights, it is important to highlight the fact that \nchildren have specific rights as enshrined in Article 24 of the Charter and in the United \nNations Convention on the Rights of the Child, further developed in the UNCRC \nGeneral Comment No 25 as regards the digital environment, both of which require \nconsideration of the children\u2019s vulnerabilities and provision of such protection and care \nas necessary for their well-being. The fundamental right to a high level of environmental \nprotection enshrined in the Charter and implemented in Union policies should also be \nconsidered when assessing the severity of the harm that an AI system can cause, \nincluding in relation to the health and safety of persons.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 49, "topic": "AI Regulation and Ethics"}}
{"id": "e0ec29a0-930b-4f87-ab67-1d95513be4eb", "question": "Imagine a company is developing an AI system intended for use in medical devices that could potentially harm users if not properly regulated. Given this scenario, should the company adhere to the requirements of the EU AI Act regarding high-risk AI systems, or would the existing medical device regulations be sufficient for compliance?", "reference_answer": "The company should adhere to the requirements of the EU AI Act regarding high-risk AI systems, as the regulation applies to products containing high-risk AI systems that can significantly impact health, safety, and fundamental rights.", "reference_context": "Document 47: To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.\n21\nRegulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 \non medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and \nRegulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and \n93/42/EEC (OJ L 117, 5.5.2017, p. 1).\n22\nRegulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 \non in vitro diagnostic medical devices and repealing Directive 98/79/EC and Commission \nDecision 2010/227/EU (OJ L 117, 5.5.2017, p. 176).\n23\nDirective 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on \nmachinery, and amending Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24).", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 47, "topic": "High-Risk AI Systems"}}
{"id": "45bd7896-0cf9-4185-a4cc-ec7d02971c79", "question": "If a tech company wishes to test a high-risk AI system in real-world conditions without participating in an AI regulatory sandbox, what specific legal requirements must they fulfill to ensure compliance with the EU AI Act?", "reference_answer": "The tech company must ensure that they request informed consent from natural persons to participate in the testing. However, in the case of law enforcement, seeking informed consent is not required if it would prevent the AI system from being tested.", "reference_context": "Document 137: EN\nUnited in diversity\nEN\n(141)\nIn order to accelerate the process of development and the placing on the market of the \nhigh-risk AI systems listed in an annex to this Regulation, it is important that providers \nor prospective providers of such systems may also benefit from a specific regime for \ntesting those systems in real world conditions, without participating in an AI regulatory \nsandbox. However, in such cases, taking into account the possible consequences of such \ntesting on individuals, it should be ensured that appropriate and sufficient guarantees \nand conditions are introduced by this Regulation for providers or prospective providers. \nSuch guarantees should include, inter alia, requesting informed consent of natural \npersons to participate in testing in real world conditions, with the exception of law \nenforcement where the seeking of informed consent would prevent the AI system from \nbeing tested. Consent of subjects to participate in such testing under this Regulation is \ndistinct from, and without prejudice to, consent of data subjects for the processing of \ntheir personal data under the relevant data protection law.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 137, "topic": "AI Regulatory Sandboxes"}}
{"id": "4ef51815-e422-405c-ad32-64f623ec0a32", "question": "If a notified body discovers that it has issued a quality management system approval that is later found to be non-compliant with the requirements of Annex VII, what are its obligations under Article 45 regarding informing the notifying authority?", "reference_answer": "The notified body is obligated to inform the notifying authority of any refusal, restriction, suspension, or withdrawal of a Union technical documentation assessment certificate or a quality management system approval issued in accordance with the requirements of Annex VII.", "reference_context": "Document 285: EN\nUnited in diversity\nEN\nArticle 45\nInformation obligations of notified bodies\n1.\nNotified bodies shall inform the notifying authority of the following: \n(a)\nany Union technical documentation assessment certificates, any supplements to those \ncertificates, and any quality management system approvals issued in accordance with \nthe requirements of Annex VII;\n(b)\nany refusal, restriction, suspension or withdrawal of a Union technical \ndocumentation assessment certificate or a quality management system approval \nissued in accordance with the requirements of Annex VII;\n(c)\nany circumstances affecting the scope of or conditions for notification;\n(d)\nany request for information which they have received from market surveillance \nauthorities regarding conformity assessment activities;\n(e)\non request, conformity assessment activities performed within the scope of their \nnotification and any other activity performed, including cross-border activities and \nsubcontracting.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 285, "topic": "Notified Bodies Regulations"}}
{"id": "040a2205-54d2-48ec-a170-4de8ad38499d", "question": "Suppose a third-country national attempts to cross the external borders of an EU Member State and is refused entry; can the Entry/Exit System (EES) be used to register this refusal, and what regulations govern this process?", "reference_answer": "Yes, the Entry/Exit System (EES) can be used to register the refusal of entry data of third-country nationals crossing the external borders. This process is governed by Regulation (EU) 2017/2226.", "reference_context": "Document 466: EN\nUnited in diversity\nEN\n4.\nEntry/Exit System\nRegulation (EU) 2017/2226 of the European Parliament and of the Council of \n30 November 2017 establishing an Entry/Exit System (EES) to register entry and \nexit data and refusal of entry data of third-country nationals crossing the external \nborders of the Member States and determining the conditions for access to the EES \nfor law enforcement purposes, and amending the Convention implementing the \nSchengen Agreement and Regulations (EC) No 767/2008 and (EU) No 1077/2011 \n(OJ L 327, 9.12.2017, p. 20).\n5.\nEuropean Travel Information and Authorisation System\n(a)\nRegulation (EU) 2018/1240 of the European Parliament and of the Council of \n12 September 2018 establishing a European Travel Information and Authorisation \nSystem (ETIAS) and amending Regulations (EU) No 1077/2011, (EU) No 515/2014, \n(EU) 2016/399, (EU) 2016/1624 and (EU) 2017/2226 (OJ L 236, 19.9.2018, p. 1).\n(b)\nRegulation (EU) 2018/1241 of the European Parliament and of the Council of \n12 September 2018 amending Regulation (EU) 2016/794 for the purpose of \nestablishing a European Travel Information and Authorisation System (ETIAS) \n(OJ L 236, 19.9.2018, p. 72).", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 466, "topic": "EU Regulations and Directives"}}
{"id": "9cd31285-6990-4cf3-ace2-a52aca995379", "question": "If a company develops a large generative AI model that can produce various types of content and decides to integrate it into an AI system for commercial use, does the company need to adhere to the obligations set forth for general-purpose AI models under the Regulation?", "reference_answer": "Yes, the company must adhere to the obligations for general-purpose AI models under the Regulation, as the model is considered to be placed on the market when integrated into its AI system.", "reference_context": "Document 101: EN\nUnited in diversity\nEN\n(97)\nThe notion of general-purpose AI models should be clearly defined and set apart from \nthe notion of AI systems to enable legal certainty. The definition should be based on the \nkey functional characteristics of a general-purpose AI model, in particular the generality \nand the capability to competently perform a wide range of distinct tasks. These models \nare typically trained on large amounts of data, through various methods, such as self-\nsupervised, unsupervised or reinforcement learning. General-purpose AI models may be \nplaced on the market in various ways, including through libraries, application \nprogramming interfaces (APIs), as direct download, or as physical copy. These models \nmay be further modified or fine-tuned into new models. Although AI models are \nessential components of AI systems, they do not constitute AI systems on their own. AI \nmodels require the addition of further components, such as for example a user interface, \nto become AI systems. AI models are typically integrated into and form part of AI \nsystems. This Regulation provides specific rules for general-purpose AI models and for \ngeneral-purpose AI models that pose systemic risks, which should apply also when these \nmodels are integrated or form part of an AI system. It should be understood that the \nobligations for the providers of general-purpose AI models should apply once the \ngeneral-purpose AI models are placed on the market.\n\nDocument 102: EN\nUnited in diversity\nEN\nWhen the provider of a general-purpose AI model integrates an own model into its own \nAI system that is made available on the market or put into service, that model should be \nconsidered to be placed on the market and, therefore, the obligations in this Regulation \nfor models should continue to apply in addition to those for AI systems. The obligations \nlaid down for models should in any case not apply when an own model is used for purely \ninternal processes that are not essential for providing a product or a service to third \nparties and the rights of natural persons are not affected. Considering their potential \nsignificantly negative effects, the general-purpose AI models with systemic risk should \nalways be subject to the relevant obligations under this Regulation. The definition \nshould not cover AI models used before their placing on the market for the sole purpose \nof research, development and prototyping activities. This is without prejudice to the \nobligation to comply with this Regulation when, following such activities, a model is \nplaced on the market.\n(98)\nWhereas the generality of a model could, inter alia, also be determined by a number of \nparameters, models with at least a billion of parameters and trained with a large amount \nof data using self-supervision at scale should be considered to display significant \ngenerality and to competently perform a wide range of distinctive tasks.\n(99)\nLarge generative AI models are a typical example for a general-purpose AI model, given \nthat they allow for flexible generation of content, such as in the form of text, audio, \nimages or video, that can readily accommodate a wide range of distinctive tasks.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 101, "topic": "Others"}}
{"id": "2c12894c-89fc-40e3-9433-f0b5cd65a0c2", "question": "Imagine a scenario where a company uses an AI system to analyze facial images for security purposes, collecting biometric data to identify individuals in a public space. In this case, does the processing of biometric data fall under the definition of biometric identification or biometric verification according to the provided context?", "reference_answer": "The processing of biometric data in this scenario falls under the definition of biometric identification, as it involves the automated recognition of physical features for the purpose of establishing identity by comparing biometric data to stored data.", "reference_context": "Document 184: EN\nUnited in diversity\nEN\n(33)\n\u2018input data\u2019 means data provided to or directly acquired by an AI system on the basis of \nwhich the system produces an output;\n(34)\n\u2018biometric data\u2019 means personal data resulting from specific technical processing relating \nto the physical, physiological or behavioural characteristics of a natural person, \u258c such as \nfacial images or dactyloscopic data;\n(35)\n\u2018biometric identification\u2019 means the automated recognition of physical, physiological, \nbehavioural, or psychological human features for the purpose of establishing the identity \nof a natural person by comparing biometric data of that individual to biometric data of \nindividuals stored in a database;\n(36)\n\u2018biometric verification\u2019 means the automated, one-to-one verification, including \nauthentication, of the identity of natural persons by comparing their biometric data to \npreviously provided biometric data ;\n(37)\n\u2018special categories of personal data\u2019 means the categories of personal data referred to in \nArticle 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and \nArticle 10(1) of Regulation (EU) 2018/1725;\n(38)\n\u2018sensitive operational data\u2019 means operational data related to activities of prevention, \ndetection, investigation or prosecution of criminal offences, the disclosure of which \ncould jeopardise the integrity of criminal proceedings;", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 184, "topic": "Biometric Data and AI"}}
{"id": "d9641f0a-3452-44cc-9b4c-17855c270d44", "question": "If a filmmaker uses an AI system to create a deep fake of a historical figure for a satirical film, does the filmmaker need to disclose that the content is artificially generated, and how does the law apply in this artistic context?", "reference_answer": "In this scenario, the filmmaker is required to disclose that the content has been artificially generated, but the transparency obligation is limited to disclosure of the existence of such generated content in a manner that does not hamper the display or enjoyment of the work, as it falls under the category of an evidently artistic, creative, satirical, or fictional work.", "reference_context": "Document 296: EN\nUnited in diversity\nEN\n4.\nDeployers of an AI system that generates or manipulates image, audio or video content \nconstituting a deep fake, shall disclose that the content has been artificially generated or \nmanipulated. This obligation shall not apply where the use is authorised by law to detect, \nprevent, investigate or prosecute criminal offence. Where the content forms part of an \nevidently artistic, creative, satirical, fictional or analogous work or programme, the \ntransparency obligations set out in this paragraph are limited to disclosure of the \nexistence of such generated or manipulated content in an appropriate manner that does \nnot hamper the display or enjoyment of the work.\nDeployers of an AI system that generates or manipulates text which is published with the \npurpose of informing the public on matters of public interest shall disclose that the text \nhas been artificially generated or manipulated. This obligation shall not apply where the \nuse is authorised by law to detect, prevent, investigate or prosecute criminal offences or \nwhere the AI-generated content has undergone a process of human review or editorial \ncontrol and where a natural or legal person holds editorial responsibility for the \npublication of the content.\n\nDocument 130: EN\nUnited in diversity\nEN\n(134)\nFurther to the technical solutions employed by the providers of the AI system, deployers \nwho use an AI system to generate or manipulate image, audio or video content that \nappreciably resembles existing persons, objects, places, entities or events and would \nfalsely appear to a person to be authentic or truthful (deep fakes), should also clearly \nand distinguishably disclose that the content has been artificially created or manipulated \nby labelling the AI output accordingly and disclosing its artificial origin. Compliance \nwith this transparency obligation should not be interpreted as indicating that the use of \nthe AI system or its output impedes the right to freedom of expression and the right to \nfreedom of the arts and sciences guaranteed in the Charter, in particular where the \ncontent is part of an evidently creative, satirical, artistic, fictional or analogous work or \nprogramme, subject to appropriate safeguards for the rights and freedoms of third \nparties. In those cases, the transparency obligation for deep fakes set out in this \nRegulation is limited to disclosure of the existence of such generated or manipulated \ncontent in an appropriate manner that does not hamper the display or enjoyment of the \nwork, including its normal exploitation and use, while maintaining the utility and \nquality of the work. In addition, it is also appropriate to envisage a similar disclosure \nobligation in relation to AI-generated or manipulated text to the extent it is published \nwith the purpose of informing the public on matters of public interest unless the AI-\ngenerated content has undergone a process of human review or editorial control and a \nnatural or legal person holds editorial responsibility for the publication of the content.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 296, "topic": "AI Regulation in the EU"}}
{"id": "9e07bf41-f765-4cbe-8259-9a91bd43e454", "question": "If a tech company develops a large generative AI model capable of producing various forms of content and decides to place it on the market, should this model be classified as a general-purpose AI model under the Regulation, and what obligations would apply to it in this scenario?", "reference_answer": "Yes, the large generative AI model should be classified as a general-purpose AI model under the Regulation, and the obligations for general-purpose AI models would apply once it is placed on the market.", "reference_context": "Document 101: EN\nUnited in diversity\nEN\n(97)\nThe notion of general-purpose AI models should be clearly defined and set apart from \nthe notion of AI systems to enable legal certainty. The definition should be based on the \nkey functional characteristics of a general-purpose AI model, in particular the generality \nand the capability to competently perform a wide range of distinct tasks. These models \nare typically trained on large amounts of data, through various methods, such as self-\nsupervised, unsupervised or reinforcement learning. General-purpose AI models may be \nplaced on the market in various ways, including through libraries, application \nprogramming interfaces (APIs), as direct download, or as physical copy. These models \nmay be further modified or fine-tuned into new models. Although AI models are \nessential components of AI systems, they do not constitute AI systems on their own. AI \nmodels require the addition of further components, such as for example a user interface, \nto become AI systems. AI models are typically integrated into and form part of AI \nsystems. This Regulation provides specific rules for general-purpose AI models and for \ngeneral-purpose AI models that pose systemic risks, which should apply also when these \nmodels are integrated or form part of an AI system. It should be understood that the \nobligations for the providers of general-purpose AI models should apply once the \ngeneral-purpose AI models are placed on the market.\n\nDocument 102: EN\nUnited in diversity\nEN\nWhen the provider of a general-purpose AI model integrates an own model into its own \nAI system that is made available on the market or put into service, that model should be \nconsidered to be placed on the market and, therefore, the obligations in this Regulation \nfor models should continue to apply in addition to those for AI systems. The obligations \nlaid down for models should in any case not apply when an own model is used for purely \ninternal processes that are not essential for providing a product or a service to third \nparties and the rights of natural persons are not affected. Considering their potential \nsignificantly negative effects, the general-purpose AI models with systemic risk should \nalways be subject to the relevant obligations under this Regulation. The definition \nshould not cover AI models used before their placing on the market for the sole purpose \nof research, development and prototyping activities. This is without prejudice to the \nobligation to comply with this Regulation when, following such activities, a model is \nplaced on the market.\n(98)\nWhereas the generality of a model could, inter alia, also be determined by a number of \nparameters, models with at least a billion of parameters and trained with a large amount \nof data using self-supervision at scale should be considered to display significant \ngenerality and to competently perform a wide range of distinctive tasks.\n(99)\nLarge generative AI models are a typical example for a general-purpose AI model, given \nthat they allow for flexible generation of content, such as in the form of text, audio, \nimages or video, that can readily accommodate a wide range of distinctive tasks.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 101, "topic": "Others"}}
{"id": "7d690309-d929-4764-a14b-3711f97dc8d1", "question": "If a member state believes it can avoid compliance with certain obligations under the EU AI Act by interpreting Paragraph 1 of Article 64 in a restrictive manner, does this interpretation exempt them from fulfilling other requirements laid down in this Regulation?", "reference_answer": "No, the interpretation does not exempt them, as Paragraph 1 of this Article explicitly states that operators must fulfill all other requirements or obligations established in this Regulation.", "reference_context": "Document 339: EN\nUnited in diversity\nEN\n2.\nParagraph 1 of this Article shall not be interpreted as exempting those operators from \nfulfilling any other requirements or obligations laid down in this Regulation, including \nthose established in Articles 9, 10, 11, 12, 13, 14, 15, 72 and 73.\nCHAPTER VII\nGOVERNANCE\nSection 1\nGovernance at Union level\nArticle 64\nAI Office\n1.\nThe Commission shall develop Union expertise and capabilities in the field of AI \nthrough the AI Office.\n2.\nMember States shall facilitate the tasks entrusted to the AI Office, as reflected in this \nRegulation.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 339, "topic": "Others"}}
{"id": "60f65c40-e54a-4b8a-907e-1fb20642efeb", "question": "If a Union institution discovers a data protection infringement and takes immediate action to mitigate the damage, how does this impact the assessment of their responsibility under the EU AI Act?", "reference_answer": "The degree of responsibility of the Union institution is assessed by considering any action taken by them to mitigate the damage suffered by affected persons.", "reference_context": "Document 408: EN\nUnited in diversity\nEN\n(b)\nthe degree of responsibility of the Union institution, body, office or agency, taking \ninto account technical and organisational measures implemented by them;\n(c)\nany action taken by the Union institution, body, office or agency to mitigate the \ndamage suffered by affected persons;\n(d)\nthe degree of cooperation with the European Data Protection Supervisor in order to \nremedy the infringement and mitigate the possible adverse effects of the \ninfringement, including compliance with any of the measures previously ordered by \nthe European Data Protection Supervisor against the Union institution, body, office \nor agency concerned with regard to the same subject matter;\n(e)\nany similar previous infringements by the Union institution, body, office or agency;\n(f)\nthe manner in which the infringement became known to the European Data \nProtection Supervisor, in particular whether, and if so to what extent, the Union \ninstitution, body, office or agency notified the infringement;\n(g)\nthe annual budget of the Union institution, body, office or agency.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 408, "topic": "AI Regulation in the EU"}}
{"id": "53a23ca2-62e9-477f-81c2-0d4a1524e2ae", "question": "Imagine that a tech company is preparing to launch a new AI system in the EU and wonders about the requirements for standardisation requests. Given this scenario, does the company need to align its standards with the existing Union harmonisation legislation as specified by the Commission under the EU AI Act?", "reference_answer": "Yes, the company needs to align its standards with the existing Union harmonisation legislation as specified by the Commission, as the standards must be clear, consistent, and aim to ensure that high-risk AI systems meet the relevant requirements or obligations laid down in this Regulation.", "reference_context": "Document 275: EN\nUnited in diversity\nEN\nWhen issuing a standardisation request to European standardisation organisations, the \nCommission shall specify that standards have to be clear, consistent, including with the \nstandards developed in the various sectors for products covered by the existing Union \nharmonisation legislation listed in Annex I, and aiming to ensure that high-risk AI \nsystems or general-purpose AI models placed on the market or put into service in the \nUnion meet the relevant requirements or obligations laid down in this Regulation.\nThe Commission shall request the European standardisation organisations to provide \nevidence of their best efforts to fulfil the objectives referred to in the first and the second \nsubparagraph of this paragraph in accordance with Article 24 of Regulation (EU) No \n1025/2012.\n3.\nThe participants in the standardisation process shall seek to promote investment and \ninnovation in AI, including through increasing legal certainty, as well as the \ncompetitiveness and growth of the Union market, to contribute to strengthening global \ncooperation on standardisation and taking into account existing international standards \nin the field of AI that are consistent with Union values, fundamental rights and interests, \nand to enhance multi-stakeholder governance ensuring a balanced representation of \ninterests and the effective participation of all relevant stakeholders in accordance with \nArticles 5, 6, and 7 of Regulation (EU) No 1025/2012.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 275, "topic": "AI Regulation in the EU"}}
{"id": "14f5aaf5-fb09-4dc5-b5eb-f75a0c301c0a", "question": "If a technology company has developed a high-risk AI system that is already covered under existing Union harmonisation legislation with a post-market monitoring system, can the company choose to integrate the new post-market monitoring plan requirements from the EU AI Act into its existing system? What legal implications does this choice have regarding compliance with the EU AI Act?", "reference_answer": "Yes, the company can choose to integrate the new post-market monitoring plan requirements into its existing system, provided that it achieves an equivalent level of protection. This choice helps ensure consistency, avoids duplications, and minimizes additional burdens under the EU AI Act.", "reference_context": "Document 358: EN\nUnited in diversity\nEN\n3.\nThe post-market monitoring system shall be based on a post-market monitoring plan. The \npost-market monitoring plan shall be part of the technical documentation referred to in \nAnnex IV. The Commission shall adopt an implementing act laying down detailed \nprovisions establishing a template for the post-market monitoring plan and the list of \nelements to be included in the plan by \u2026 [18 months after the entry into force of this \nRegulation]. That implementing act shall be adopted in accordance with the \nexamination procedure referred to in Article 98(2).\n4.\nFor high-risk AI systems covered by the Union harmonisation legislation listed in Section \nA of Annex I, where a post-market monitoring system and plan are already established \nunder that legislation, in order to ensure consistency, avoid duplications and minimise \nadditional burdens, providers shall have a choice of integrating, as appropriate, the \nnecessary elements described in paragraphs 1, 2 and 3 using the template referred in \nparagraph 3 into systems and plans already existing under that legislation, provided that \nit achieves an equivalent level of protection.\nThe first subparagraph of this paragraph shall also apply \u258cto high-risk AI systems referred \nto in point 5 of Annex III placed on the market or put into service by financial institutions \nthat are subject to requirements under Union financial services law regarding their \ninternal governance, arrangements or processes.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 358, "topic": "High-Risk AI Regulation"}}
{"id": "797e19b1-af12-42c0-9f88-7d6ad721a1c4", "question": "If a company discovers that its AI system was trained on data that does not meet the required standards and intends to apply for a new conformity assessment, must it first undergo re-training of the AI system before submitting its application, and how does this relate to the issuance of the Union technical documentation assessment certificate?", "reference_answer": "Yes, the company must undergo re-training of the AI system prior to applying for a new conformity assessment. If the AI system does not meet the requirement relating to the data used to train it, the notified body will refuse to issue the Union technical documentation assessment certificate and will provide detailed reasons for its refusal.", "reference_context": "Document 455: EN\nUnited in diversity\nEN\n4.6.\nThe decision of the notified body shall be notified to the provider or its authorised \nrepresentative. The notification shall contain the conclusions of the assessment of the \ntechnical documentation and the reasoned assessment decision.\nWhere the AI system is in conformity with the requirements set out in Chapter III, Section \n2, the notified body shall issue a Union technical documentation assessment certificate. \nThe certificate shall indicate the name and address of the provider, the conclusions of the \nexamination, the conditions (if any) for its validity and the data necessary for the \nidentification of the AI system.\nThe certificate and its annexes shall contain all relevant information to allow the \nconformity of the AI system to be evaluated, and to allow for control of the AI system \nwhile in use, where applicable.\nWhere the AI system is not in conformity with the requirements set out in Chapter III, \nSection 2, the notified body shall refuse to issue a Union technical documentation \nassessment certificate and shall inform the applicant accordingly, giving detailed reasons \nfor its refusal.\n\nDocument 456: EN\nUnited in diversity\nEN\nWhere the AI system does not meet the requirement relating to the data used to train it, re-\ntraining of the AI system will be needed prior to the application for a new conformity \nassessment. In this case, the reasoned assessment decision of the notified body refusing to \nissue the Union technical documentation assessment certificate shall contain specific \nconsiderations on the quality data used to train the AI system, in particular on the reasons \nfor non-compliance.\n4.7.\nAny change to the AI system that could affect the compliance of the AI system with the \nrequirements or its intended purpose shall be assessed by the notified body which issued \nthe Union technical documentation assessment certificate. The provider shall inform such \nnotified body of its intention to introduce any of the abovementioned changes, or if it \notherwise becomes aware of the occurrence of such changes. The intended changes shall \nbe assessed by the notified body, which shall decide whether those changes require a new \nconformity assessment in accordance with Article 43(4) or whether they could be \naddressed by means of a supplement to the Union technical documentation assessment \ncertificate. In the latter case, the notified body shall assess the changes, notify the provider \nof its decision and, where the changes are approved, issue to the provider a supplement to \nthe Union technical documentation assessment certificate.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 455, "topic": "AI Quality Management"}}
{"id": "03b5a408-12d2-46a9-8dd1-f9457a272f86", "question": "Imagine a scenario where a startup developing a general-purpose AI model is unsure about how to demonstrate compliance with the obligations set out in the EU AI Act. If they choose not to adhere to an approved code of practice or a European harmonised standard, what alternative compliance methods can they rely on to meet their obligations under the regulation?", "reference_answer": "Providers of general-purpose AI models who do not adhere to an approved code of practice or do not comply with a European harmonised standard shall demonstrate alternative adequate means of compliance for assessment by the Commission.", "reference_context": "Document 117: EN\nUnited in diversity\nEN\n(117)\nThe codes of practice should represent a central tool for the proper compliance with the \nobligations provided for under this Regulation for providers of general-purpose AI \nmodels. Providers should be able to rely on codes of practice to demonstrate compliance \nwith the obligations. By means of implementing acts, the Commission may decide to \napprove a code of practice and give it a general validity within the Union, or, \nalternatively, to provide common rules for the implementation of the relevant \nobligations, if, by the time this Regulation becomes applicable, a code of practice cannot \nbe finalised or is not deemed adequate by the AI Office. Once a harmonised standard is \npublished and assessed as suitable to cover the relevant obligations by the AI Office, \ncompliance with a European harmonised standard should grant providers the \npresumption of conformity. Providers of general-purpose AI models should furthermore \nbe able to demonstrate compliance using alternative adequate means, if codes of practice \nor harmonised standards are not available, or they choose not to rely on those.\n\nDocument 116: EN\nUnited in diversity\nEN\n(116)\nThe AI Office should encourage and facilitate the drawing up, review and adaptation of \ncodes of practice, taking into account international approaches. All providers of general-\npurpose AI models could be invited to participate. To ensure that the codes of practice \nreflect the state of the art and duly take into account a diverse set of perspectives, the AI \nOffice should collaborate with relevant national competent authorities, and could, where \nappropriate, consult with civil society organisations and other relevant stakeholders and \nexperts, including the Scientific Panel, for the drawing up of such codes. Codes of \npractice should cover obligations for providers of general-purpose AI models and of \ngeneral-purpose AI models presenting systemic risks. In addition, as regards systemic \nrisks, codes of practice should help to establish a risk taxonomy of the type and nature of \nthe systemic risks at Union level, including their sources. Codes of practice should also \nbe focused on specific risk assessment and mitigation measures.\n\nDocument 304: EN\nUnited in diversity\nEN\n4.\nProviders of general-purpose AI models may rely on codes of practice within the \nmeaning of Article 56 to demonstrate compliance with the obligations set out in \nparagraph 1 of this Article, until a harmonised standard is published. Compliance with \nEuropean harmonised standards grants providers the presumption of conformity to the \nextent that those standards cover those obligations. Providers of general-purpose AI \nmodels who do not adhere to an approved code of practice or do not comply with a \nEuropean harmonised standard shall demonstrate alternative adequate means of \ncompliance for assessment by the Commission.\n5.\nFor the purpose of facilitating compliance with Annex XI, in particular points 2 (d) and \n(e) thereof, the Commission is empowered to adopt delegated acts in accordance with \nArticle 97 to detail measurement and calculation methodologies with a view to allowing \nfor comparable and verifiable documentation.\n6.\nThe Commission is empowered to adopt delegated acts in accordance with Article 97(2) \nto amend Annexes XI and XII in light of evolving technological developments.\n7.\nAny information or documentation obtained pursuant to this Article, including trade \nsecrets, shall be treated in accordance with the confidentiality obligations set out in \nArticle 78.\n\nDocument 311: EN\nUnited in diversity\nEN\n5.\nThe AI Office shall aim to ensure that participants to the codes of practice report \nregularly to the AI Office on the implementation of the commitments and the measures \ntaken and their outcomes, including as measured against the key performance indicators \nas appropriate. Key performance indicators and reporting commitments shall reflect \ndifferences in size and capacity between various participants.\n6.\nThe AI Office and the Board shall regularly monitor and evaluate the achievement of \nthe objectives of the codes of practice by the participants and their contribution to the \nproper application of this Regulation. The AI Office and the Board shall assess whether \nthe codes of practice cover the obligations provided for in Articles 53 and 55, and shall \nregularly monitor and evaluate the achievement of their objectives. They shall publish \ntheir assessment of the adequacy of the codes of practice. \nThe Commission may, by way of an implementing act, approve a code of practice and \ngive it a general validity within the Union. That implementing act shall be adopted in \naccordance with the examination procedure referred to in Article 98(2).\n7.\nThe AI Office may invite all providers of general-purpose AI models to adhere to the \ncodes of practice. For providers of general-purpose AI models not presenting systemic \nrisks this adherence may be limited to the obligations provided for in Article 53, unless \nthey declare explicitly their interest to join the full code.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 117, "topic": "AI Regulation and Governance"}}
{"id": "fe77a134-5bfd-4503-a2eb-2c46271b5dae", "question": "If a private security company is tasked with investigating a series of thefts in a shopping mall, should it be classified as a law enforcement authority under the definitions provided in the context, and what implications does this classification have for its powers and responsibilities?", "reference_answer": "The private security company would not be classified as a law enforcement authority under the definitions provided, as it is not a public authority or an entity entrusted by Member State law to exercise public authority and public powers for the purposes of law enforcement.", "reference_context": "Document 186: EN\nUnited in diversity\nEN\n(44)\n\u2018publicly accessible space\u2019 means any publicly or privately owned physical place \naccessible to an undetermined number of natural persons, regardless of whether certain \nconditions for access may apply, and regardless of the potential capacity restrictions;\n(45)\n\u2018law enforcement authority\u2019 means:\n(a)\nany public authority competent for the prevention, investigation, detection or \nprosecution of criminal offences or the execution of criminal penalties, including the \nsafeguarding against and the prevention of threats to public security; or\n(b)\nany other body or entity entrusted by Member State law to exercise public authority \nand public powers for the purposes of the prevention, investigation, detection or \nprosecution of criminal offences or the execution of criminal penalties, including the \nsafeguarding against and the prevention of threats to public security;\n(46)\n\u2018law enforcement\u2019 means activities carried out by law enforcement authorities or on their \nbehalf for the prevention, investigation, detection or prosecution of criminal offences or \nthe execution of criminal penalties, including safeguarding against and preventing threats \nto public security;\n(47)\n\u2018AI Office\u2019 means the Commission\u2019s function of contributing to the implementation, \nmonitoring and supervision of AI systems and general-purpose AI models, and AI \ngovernance, provided for in Commission Decision of 24 January 2024; references in this \nRegulation to the AI Office shall be construed as references to the Commission;", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 186, "topic": "Biometric Data and AI"}}
{"id": "3ee12033-f072-4630-8a4f-20d64211846c", "question": "If a tech company develops a general-purpose AI model that meets the floating point operations threshold but argues that its unique design prevents systemic risks, how should the Commission assess the validity of this argument under the EU AI Act?", "reference_answer": "The Commission should assess the argument by determining if it is sufficiently substantiated. If the arguments are not substantiated and the provider cannot demonstrate that the model does not present systemic risks, the model will be classified as a general-purpose AI model with systemic risk.", "reference_context": "Document 299: EN\nUnited in diversity\nEN\n2.\nA general-purpose AI model shall be presumed to have high impact capabilities \npursuant to paragraph 1, point (a), when the cumulative amount of computation used \nfor its training measured in floating point operations is greater than 1025.\n3.\nThe Commission shall adopt delegated acts in accordance with Article 97 to amend the \nthresholds listed in paragraphs 1 and 2 of this Article, as well as to supplement \nbenchmarks and indicators in light of evolving technological developments, such as \nalgorithmic improvements or increased hardware efficiency, when necessary, for these \nthresholds to reflect the state of the art.\nArticle 52\nProcedure\n1.\nWhere a general-purpose AI model meets the condition referred to in Article 51(1), point \n(a), the relevant provider shall notify the Commission without delay and in any event \nwithin two weeks after that requirement is met or it becomes known that it will be met. \nThat notification shall include the information necessary to demonstrate that the \nrelevant requirement has been met. If the Commission becomes aware of a general-\npurpose AI model presenting systemic risks of which it has not been notified, it may \ndecide to designate it as a model with systemic risk.\n\nDocument 300: EN\nUnited in diversity\nEN\n2.\nThe provider of a general-purpose AI model that meets the condition referred to in \nArticle 51(1), point (a), may present, with its notification, sufficiently substantiated \narguments to demonstrate that, exceptionally, although it meets that requirement, the \ngeneral-purpose AI model does not present, due to its specific characteristics, systemic \nrisks and therefore should not be classified as a general-purpose AI model with systemic \nrisk.\n3.\nWhere the Commission concludes that the arguments submitted pursuant to paragraph 2 \nare not sufficiently substantiated and the relevant provider was not able to demonstrate \nthat the general-purpose AI model does not present, due to its specific characteristics, \nsystemic risks, it shall reject those arguments, and the general-purpose AI model shall be \nconsidered to be a general-purpose AI model with systemic risk.\n4.\nThe Commission may designate a general-purpose AI model as presenting systemic \nrisks, ex officio or following a qualified alert from the scientific panel pursuant to \nArticle 90(1), point (a), on the basis of criteria set out in Annex XIII. \nThe Commission is empowered to adopt delegated acts in accordance with Article 97 in \norder to amend Annex XIII by specifying and updating the criteria set out in that Annex.\n\nDocument 473: EN\nUnited in diversity\nEN\nANNEX XIII\nCriteria for the designation of general-purpose AI models \nwith systemic risk referred to in Article 51\nFor the purpose of determining that a general-purpose AI model has capabilities or an \nimpact equivalent to those set out in Article 51(1), point (a), the Commission shall take into \naccount the following criteria:\n(a)\nthe number of parameters of the model;\n(b)\nthe quality or size of the data set, for example measured through tokens;\n(c)\nthe amount of computation used for training the model, measured in floating point \noperations or indicated by a combination of other variables such as estimated cost \nof training, estimated time required for the training, or estimated energy \nconsumption for the training;\n(d)\nthe input and output modalities of the model, such as text to text (large language \nmodels), text to image, multi-modality, and the state of the art thresholds for \ndetermining high-impact capabilities for each modality, and the specific type of \ninputs and outputs (e.g. biological sequences);\n(e)\nthe benchmarks and evaluations of capabilities of the model, including \nconsidering the number of tasks without additional training, adaptability to learn \nnew, distinct tasks, its level of autonomy and scalability, the tools it has access to;\n(f)\nwhether it has a high impact on the internal market due to its reach, which shall \nbe presumed when it has been made available to at least 10 000 registered business \nusers established in the Union;\n(g)\nthe number of registered end-users.\n\nDocument 113: EN\nUnited in diversity\nEN\n(112)\nIt is also necessary to clarify a procedure for the classification of a general-purpose AI \nmodel with systemic risks. A general-purpose AI model that meets the applicable \nthreshold for high-impact capabilities should be presumed to be a general-purpose AI \nmodels with systemic risk. The provider should notify the AI Office at the latest two \nweeks after the requirements are met or it becomes known that a general-purpose AI \nmodel will meet the requirements that lead to the presumption. This is especially relevant \nin relation to the threshold of floating point operations because training of general-\npurpose AI models takes considerable planning which includes the upfront allocation of \ncompute resources and, therefore, providers of general-purpose AI models are able to \nknow if their model would meet the threshold before the training is completed. In the \ncontext of that notification, the provider should be able to demonstrate that, because of \nits specific characteristics, a general-purpose AI model exceptionally does not present \nsystemic risks, and that it thus should not be classified as a general-purpose AI model \nwith systemic risks. That information is valuable for the AI Office to anticipate the \nplacing on the market of general-purpose AI models with systemic risks and the \nproviders can start to engage with the AI Office early on. That information is especially \nimportant with regard to general-purpose AI models that are planned to be released as \nopen-source, given that, after the open-source model release, necessary measures to \nensure compliance with the obligations under this Regulation may be more difficult to \nimplement.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 299, "topic": "Systemic Risks in AI"}}
{"id": "b4a0df56-acf0-43f2-83a9-a1c95075fcef", "question": "If a provider of AI systems is found to be non-compliant during an audit by the notified body, what are the implications under the EU AI Act regarding the provider's approved quality management system?", "reference_answer": "The implications would be that the notified body must ensure that the provider duly complies with the terms and conditions of the approved quality management system, and non-compliance could affect the provider's certification status.", "reference_context": "Document 457: EN\nUnited in diversity\nEN\n5.\nSurveillance of the approved quality management system.\n5.1.\nThe purpose of the surveillance carried out by the notified body referred to in Point 3 is to \nmake sure that the provider duly complies with the terms and conditions of the approved \nquality management system.\n5.2.\nFor assessment purposes, the provider shall allow the notified body to access the premises \nwhere the design, development, testing of the AI systems is taking place. The provider \nshall further share with the notified body all necessary information.\n5.3.\nThe notified body shall carry out periodic audits to make sure that the provider maintains \nand applies the quality management system and shall provide the provider with an audit \nreport. In the context of those audits, the notified body may carry out additional tests of the \nAI systems for which a Union technical documentation assessment certificate was issued.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 457, "topic": "AI Quality Management"}}
{"id": "9f3cf4c6-e215-4716-9ffc-a42e62467687", "question": "If a tech company decides to implement an AI system that prioritizes energy efficiency and includes diverse development teams, how does this align with the requirements set forth by the EU AI Act regarding voluntary codes of conduct for AI systems?", "reference_answer": "The tech company's implementation aligns with the EU AI Act's requirements as it addresses elements such as assessing and minimizing the impact of AI systems on environmental sustainability and facilitating an inclusive and diverse design of AI systems.", "reference_context": "Document 397: EN\nUnited in diversity\nEN\n2.\nThe AI Office and the Member States shall \u258c facilitate the drawing up of codes of conduct \nconcerning the voluntary application, including by deployers, of specific requirements to \nall AI systems, on the basis of clear objectives and key performance indicators to \nmeasure the achievement of those objectives, including elements such as, but not limited \nto:\n(a)\napplicable elements provided for in Union ethical guidelines for trustworthy AI;\n(b)\nassessing and minimising the impact of AI systems on environmental \nsustainability, including as regards energy-efficient programming and techniques \nfor the efficient design, training and use of AI;\n(c)\npromoting AI literacy, in particular that of persons dealing with the development, \noperation and use of AI;\n(d)\nfacilitating an inclusive and diverse design of AI systems, including through the \nestablishment of inclusive and diverse development teams and the promotion of \nstakeholders\u2019 participation in that process;", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 397, "topic": "Others"}}
{"id": "b5e5cabc-df92-43eb-a332-502dd5c518fd", "question": "Imagine a start-up developing an innovative AI system that wishes to test its technology in a real-world environment under the supervision of national competent authorities. In this scenario, are they required to adhere to the specific terms and conditions set by the authorities, or can they operate independently without such oversight according to the Regulation on AI regulatory sandboxes?", "reference_answer": "They are required to adhere to the specific terms and conditions set by the authorities, as these authorities must agree on the terms and conditions of such testing, including appropriate safeguards to protect fundamental rights, health, and safety.", "reference_context": "Document 324: EN\nUnited in diversity\nEN\n3.\nProspective providers in the AI regulatory sandboxes, in particular SMEs and start-ups, \nshall be directed, where relevant, to pre-deployment services such as guidance on the \nimplementation of this Regulation, to other value-adding services such as help with \nstandardisation documents and certification, testing and experimentation facilities, \nEuropean Digital Innovation Hubs and centres of excellence.\n4.\nWhere national competent authorities consider authorising testing in real world \nconditions supervised within the framework of an AI regulatory sandbox to be \nestablished under this Article, they shall specifically agree the terms and conditions of \nsuch testing and, in particular, the appropriate safeguards with the participants, with a \nview to protecting fundamental rights, health and safety. Where appropriate, they shall \ncooperate with other national competent authorities with a view to ensuring consistent \npractices across the Union.\n\nDocument 315: EN\nUnited in diversity\nEN\n5.\nAI regulatory sandboxes established under paragraph 1 shall provide for a controlled \nenvironment that fosters innovation and facilitates the development, training, testing and \nvalidation of innovative AI systems for a limited time before their being placed on the \nmarket or put into service pursuant to a specific sandbox plan agreed between the \nproviders or prospective providers and the competent authority. Such sandboxes may \ninclude testing in real world conditions supervised therein.\n6.\nCompetent authorities shall provide, as appropriate, guidance, supervision and support \nwithin the AI regulatory sandbox with a view to identifying risks, in particular to \nfundamental rights, health and safety, testing, mitigation measures, and their \neffectiveness in relation to the obligations and requirements of this Regulation and, \nwhere relevant, other Union and national law supervised within the sandbox.\n7.\nCompetent authorities shall provide providers and prospective providers participating in \nthe AI regulatory sandbox with guidance on regulatory expectations and how to fulfil the \nrequirements and obligations set out in this Regulation.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 324, "topic": "AI Regulatory Sandboxes"}}
{"id": "b4ad5b49-1c0a-412f-9433-c6582f1261ad", "question": "If a tech company develops an AI system that profiles individuals for targeted advertising but believes it does not pose significant risks to health, safety, or fundamental rights, what documentation must they prepare to justify their assessment under the EU AI Regulation?", "reference_answer": "The tech company must draw up documentation of the assessment before the AI system is placed on the market or put into service and provide that documentation to national competent authorities upon request.", "reference_context": "Document 60: In any case, AI systems used in high-risk use-cases listed in an annex \nto this Regulation should be considered to pose significant risks of harm to the health, \nsafety or fundamental rights if the AI system implies profiling within the meaning of \nArticle 4, point (4) of Regulation (EU) 2016/679 or Article 3, point (4) of Directive (EU) \n2016/680 or Article 3, point (5) of Regulation (EU) 2018/1725. To ensure traceability \nand transparency, a provider who considers that an AI system is not high-risk on the \nbasis of the conditions referred to above should draw up documentation of the \nassessment before that system is placed on the market or put into service and should \nprovide that documentation to national competent authorities upon request. Such a \nprovider should be obliged to register the AI system in the EU database established \nunder this Regulation. With a view to providing further guidance for the practical \nimplementation of the conditions under which the AI systems listed in an annex to this \nRegulation are, on an exceptional basis, non-high-risk, the Commission should, after \nconsulting the Board, provide guidelines specifying that practical implementation, \ncompleted by a comprehensive list of practical examples of use cases of AI systems that \nare high-risk and use cases that are not.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 60, "topic": "High-Risk AI Systems"}}
{"id": "32b89213-67af-45ed-90f8-45a646655ee4", "question": "If a national competent authority discovers a serious incident involving an AI system, but has not yet taken action, how does the notification requirement under Regulation (EU) 2019/1020 apply in this scenario?", "reference_answer": "The national competent authority must immediately notify the Commission of any serious incident, whether or not they have taken action on it, in accordance with Article 20 of Regulation (EU) 2019/1020.", "reference_context": "Document 362: EN\nUnited in diversity\nEN\n11.\nNational competent authorities shall immediately notify the Commission of any serious \nincident, whether or not they have taken action on it, in accordance with Article 20 of \nRegulation (EU) 2019/1020.\nSection 3\nEnforcement\nArticle 74\nMarket surveillance and control of AI systems in the Union market\n1.\nRegulation (EU) 2019/1020 shall apply to AI systems covered by this Regulation. For the \npurposes of the effective enforcement of this Regulation:\n(a)\nany reference to an economic operator under Regulation (EU) 2019/1020 shall be \nunderstood as including all operators identified in Article 2(1) of this Regulation;\n(b)\nany reference to a product under Regulation (EU) 2019/1020 shall be understood as \nincluding all AI systems falling within the scope of this Regulation.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 362, "topic": "AI Regulation and Compliance"}}
{"id": "7753021d-8ee9-4686-bc5d-e5fa000a81e0", "question": "If a start-up named TechInnovate is considering participating in an AI regulatory sandbox to test its new AI system under real-world conditions, what specific legal framework should they expect to comply with in order to ensure the protection of fundamental rights, health, and safety during this testing phase?", "reference_answer": "TechInnovate should expect to comply with the terms and conditions agreed upon with the national competent authorities, which will include appropriate safeguards to protect fundamental rights, health, and safety, as outlined in the AI regulatory sandbox framework.", "reference_context": "Document 324: EN\nUnited in diversity\nEN\n3.\nProspective providers in the AI regulatory sandboxes, in particular SMEs and start-ups, \nshall be directed, where relevant, to pre-deployment services such as guidance on the \nimplementation of this Regulation, to other value-adding services such as help with \nstandardisation documents and certification, testing and experimentation facilities, \nEuropean Digital Innovation Hubs and centres of excellence.\n4.\nWhere national competent authorities consider authorising testing in real world \nconditions supervised within the framework of an AI regulatory sandbox to be \nestablished under this Article, they shall specifically agree the terms and conditions of \nsuch testing and, in particular, the appropriate safeguards with the participants, with a \nview to protecting fundamental rights, health and safety. Where appropriate, they shall \ncooperate with other national competent authorities with a view to ensuring consistent \npractices across the Union.\n\nDocument 315: EN\nUnited in diversity\nEN\n5.\nAI regulatory sandboxes established under paragraph 1 shall provide for a controlled \nenvironment that fosters innovation and facilitates the development, training, testing and \nvalidation of innovative AI systems for a limited time before their being placed on the \nmarket or put into service pursuant to a specific sandbox plan agreed between the \nproviders or prospective providers and the competent authority. Such sandboxes may \ninclude testing in real world conditions supervised therein.\n6.\nCompetent authorities shall provide, as appropriate, guidance, supervision and support \nwithin the AI regulatory sandbox with a view to identifying risks, in particular to \nfundamental rights, health and safety, testing, mitigation measures, and their \neffectiveness in relation to the obligations and requirements of this Regulation and, \nwhere relevant, other Union and national law supervised within the sandbox.\n7.\nCompetent authorities shall provide providers and prospective providers participating in \nthe AI regulatory sandbox with guidance on regulatory expectations and how to fulfil the \nrequirements and obligations set out in this Regulation.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 324, "topic": "AI Regulatory Sandboxes"}}
{"id": "bb3be9b3-f388-4758-b4c5-d9dcee325a29", "question": "If a company receives a suspension on its certification due to concerns about health and safety, and the notifying authority confirms that there is no risk to health, safety, or fundamental rights within one month, can the certificates remain valid under the EU AI Act? What are the specific conditions that must be met for this to apply?", "reference_answer": "Yes, the certificates can remain valid if the notifying authority confirms, within one month of the suspension, that there is no risk to health, safety, or fundamental rights, and outlines a timeline for actions to remedy the suspension or restriction.", "reference_context": "Document 270: EN\nUnited in diversity\nEN\n8.\nWith the exception of certificates unduly issued, and where a designation has been \nsuspended or restricted, the certificates shall remain valid in one of the following \ncircumstances:\n(a)\nthe notifying authority has confirmed, within one month of the suspension or \nrestriction, that there is no risk to health, safety or fundamental rights in relation \nto certificates affected by the suspension or restriction, and the notifying authority \nhas outlined a timeline for actions to remedy the suspension or restriction; or\n(b)\nthe notifying authority has confirmed that no certificates relevant to the \nsuspension will be issued, amended or re-issued during the course of the \nsuspension or restriction, and states whether the notified body has the capability of \ncontinuing to monitor and remain responsible for existing certificates issued for \nthe period of the suspension or restriction; in the event that the notifying authority \ndetermines that the notified body does not have the capability to support existing \ncertificates issued, the provider of the system covered by the certificate shall \nconfirm in writing to the national competent authorities of the Member State in \nwhich it has its registered place of business, within three months of the suspension \nor restriction, that another qualified notified body is temporarily assuming the \nfunctions of the notified body to monitor and remain responsible for the \ncertificates during the period of suspension or restriction.", "conversation_history": [], "metadata": {"question_type": "rule_conclusion", "seed_document_id": 270, "topic": "Notified Bodies Regulations"}}
{"id": "a8606b3e-569b-41b3-9c4e-84dc4a1d7637", "question": "What specific authorisation must be obtained from a judicial or independent administrative authority of a Member State before implementing a real-time remote biometric identification system for law enforcement in publicly accessible spaces, and how does this relate to the competence of notified bodies as outlined in Article 37?", "reference_answer": "An express and specific authorisation by a judicial authority or by an independent administrative authority of a Member State whose decision is binding.", "reference_context": "Document 38: EN\nUnited in diversity\nEN\n(35)\nEach use of a \u2018real-time\u2019 remote biometric identification system in publicly accessible \nspaces for the purpose of law enforcement should be subject to an express and specific \nauthorisation by a judicial authority or by an independent administrative authority of a \nMember State whose decision is binding. Such authorisation should, in principle, be \nobtained prior to the use of the AI system with a view to identifying a person or persons. \nExceptions to that rule should be allowed in duly justified situations on grounds of \nurgency, namely in situations where the need to use the systems concerned is such as to \nmake it effectively and objectively impossible to obtain an authorisation before \ncommencing the use of the AI system. In such situations of urgency, the use of the AI \nsystem should be restricted to the absolute minimum necessary and should be subject to \nappropriate safeguards and conditions, as determined in national law and specified in the \ncontext of each individual urgent use case by the law enforcement authority itself. In \naddition, the law enforcement authority should in such situations request such \nauthorisation \u258c while providing the reasons for not having been able to request it earlier, \nwithout undue delay and at the latest within 24 hours. If such an authorisation is \nrejected, the use of real-time biometric identification systems linked to that authorisation \nshould cease with immediate effect and all the data related to such use should be \ndiscarded and deleted. Such data includes input data directly acquired by an AI system \nin the course of the use of such system as well as the results and outputs of the use \nlinked to that authorisation. It should not include input that is legally acquired in \naccordance with another Union or national law. In any case, no decision producing an \nadverse legal effect on a person should be taken based solely on the output of the remote \nbiometric identification system.\n\nDocument 199: EN\nUnited in diversity\nEN\n3.\nFor the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each \u258c use \nfor the purposes of law enforcement of a \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be subject to a prior authorisation granted by a judicial \nauthority or \u258c an independent administrative authority whose decision is binding of the \nMember State in which the use is to take place, issued upon a reasoned request and in \naccordance with the detailed rules of national law referred to in paragraph 5. However, in a \nduly justified situation of urgency, the use of such system may be commenced without an \nauthorisation provided that such authorisation is requested without undue delay, at the \nlatest within 24 hours. If such authorisation is rejected, the use shall be stopped with \nimmediate effect and all the data, as well as the results and outputs of that use shall be \nimmediately discarded and deleted.\nThe competent judicial authority or an independent administrative authority whose \ndecision is binding shall grant the authorisation only where it is satisfied, on the basis of \nobjective evidence or clear indications presented to it, that the use of the \u2018real-time\u2019 remote \nbiometric identification system concerned is necessary for, and proportionate to, achieving \none of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in \nthe request and, in particular, remains limited to what is strictly necessary concerning \nthe period of time as well as the geographic and personal scope. In deciding on the \nrequest, that authority shall take into account the elements referred to in paragraph 2. No \ndecision that produces an adverse legal effect on a person may be taken based solely on \nthe output of the \u2018real-time\u2019 remote biometric identification system.\n\nDocument 40: EN\nUnited in diversity\nEN\n(38)\nThe use of AI systems for real-time remote biometric identification of natural persons in \npublicly accessible spaces for the purpose of law enforcement necessarily involves the \nprocessing of biometric data. The rules of this Regulation that prohibit, subject to certain \nexceptions, such use, which are based on Article 16 TFEU, should apply as lex specialis in \nrespect of the rules on the processing of biometric data contained in Article 10 of Directive \n(EU) 2016/680, thus regulating such use and the processing of biometric data involved in \nan exhaustive manner. Therefore, such use and processing should be possible only in as far \nas it is compatible with the framework set by this Regulation, without there being scope, \noutside that framework, for the competent authorities, where they act for purpose of law \nenforcement, to use such systems and process such data in connection thereto on the \ngrounds listed in Article 10 of Directive (EU) 2016/680. In that context, this Regulation is \nnot intended to provide the legal basis for the processing of personal data under Article 8 \nof Directive (EU) 2016/680. However, the use of real-time remote biometric identification \nsystems in publicly accessible spaces for purposes other than law enforcement, including \nby competent authorities, should not be covered by the specific framework regarding such \nuse for the purpose of law enforcement set by this Regulation. Such use for purposes other \nthan law enforcement should therefore not be subject to the requirement of an authorisation \nunder this Regulation and the applicable detailed rules of national law that may give effect \nto that authorisation.\n\nDocument 34: EN\nUnited in diversity\nEN\n(32)\nThe use of AI systems for \u2018real-time\u2019 remote biometric identification of natural persons in \npublicly accessible spaces for the purpose of law enforcement is particularly intrusive to \nthe rights and freedoms of the concerned persons, to the extent that it may affect the \nprivate life of a large part of the population, evoke a feeling of constant surveillance and \nindirectly dissuade the exercise of the freedom of assembly and other fundamental rights. \nTechnical inaccuracies of AI systems intended for the remote biometric identification of \nnatural persons can lead to biased results and entail discriminatory effects. Such \npossible biased results and discriminatory effects are particularly relevant with regard to \nage, ethnicity, race, sex or disabilities. In addition, the immediacy of the impact and the \nlimited opportunities for further checks or corrections in relation to the use of such systems \noperating in real-time carry heightened risks for the rights and freedoms of the persons \nconcerned in the context of, or impacted by, law enforcement activities.\n(33)\nThe use of those systems for the purpose of law enforcement should therefore be \nprohibited, except in exhaustively listed and narrowly defined situations, where the use is \nstrictly necessary to achieve a substantial public interest, the importance of which \noutweighs the risks. Those situations involve the search for certain victims of crime \u258c \nincluding missing persons; certain threats to the life or to the physical safety of natural \npersons or of a terrorist attack; and the localisation or identification of perpetrators or \nsuspects of the criminal offences listed in an annex to this Regulation, where those \ncriminal offences are punishable in the Member State concerned by a custodial sentence or \na detention order for a maximum period of at least four years and as they are defined in the \nlaw of that Member State. Such a threshold for the custodial sentence or detention order in \naccordance with national law contributes to ensuring that the offence should be serious \nenough to potentially justify the use of \u2018real-time\u2019 remote biometric identification systems.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 38, "distracting_context": "EN\nUnited in diversity\nEN\nArticle 37\nChallenge to the competence of notified bodies\n1.\nThe Commission shall, where necessary, investigate all cases where there are reasons to \ndoubt the competence of a notified body or the continued fulfilment by a notified body of \nthe requirements laid down in Article 31 and of its applicable responsibilities.\n2.\nThe notifying authority shall provide the Commission, on request, with all relevant \ninformation relating to the notification or the maintenance of the competence of the \nnotified body concerned.\n3.\nThe Commission shall ensure that all sensitive information obtained in the course of its \ninvestigations pursuant to this Article is treated confidentially in accordance with \nArticle 78.\n4.\nWhere the Commission ascertains that a notified body does not meet or no longer meets \nthe requirements for its notification, it shall inform the notifying Member State \naccordingly and request it to take the necessary corrective measures, including the \nsuspension or withdrawal of the notification if necessary. Where the Member State fails \nto take the necessary corrective measures, the Commission may, by means of an \nimplementing act, suspend, restrict or withdraw the designation. That implementing act \nshall be adopted in accordance with the examination procedure referred to in Article 98(2).", "topic": "Biometric Data and AI"}}
{"id": "f6ed05ca-a258-4baa-b2cc-4f2915d446eb", "question": "In the context of Article 100, which entity is authorized to impose administrative fines on Union institutions, bodies, offices, and agencies, particularly when considering the implications of AI literacy and the integration of AI systems?", "reference_answer": "The European Data Protection Supervisor may impose administrative fines on Union institutions, bodies, offices, and agencies falling within the scope of this Regulation.", "reference_context": "Document 407: EN\nUnited in diversity\nEN\n9.\nDepending on the legal system of the Member States, the rules on administrative fines may \nbe applied in such a manner that the fines are imposed by competent national courts or by \nother bodies, as applicable in those Member States. The application of such rules in those \nMember States shall have an equivalent effect.\n10.\nThe exercise of powers under this Article shall be subject to appropriate procedural \nsafeguards in accordance with Union and national law, including effective judicial \nremedies and due process.\n11.\nMember States shall, on an annual basis, report to the Commission about the \nadministrative fines they have issued during that year, in accordance with this Article, \nand about any related litigation or judicial proceedings.\nArticle 100\nAdministrative fines on Union institutions, bodies, offices and agencies \n1.\nThe European Data Protection Supervisor may impose administrative fines on Union \ninstitutions, bodies, offices and agencies falling within the scope of this Regulation. When \ndeciding whether to impose an administrative fine and when deciding on the amount of the \nadministrative fine in each individual case, all relevant circumstances of the specific \nsituation shall be taken into account and due regard shall be given to the following:\n(a)\nthe nature, gravity and duration of the infringement and of its consequences, taking \ninto account the purpose of the AI system concerned, as well as, where \nappropriate, the number of affected persons and the level of damage suffered by \nthem;", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 407, "distracting_context": "EN\nUnited in diversity\nEN\n(66)\n\u2018general-purpose AI system\u2019 means an AI system which is based on a general-purpose \nAI model and which has the capability to serve a variety of purposes, both for direct use \nas well as for integration in other AI systems;\n(67)\n\u2018floating-point operation\u2019 means any mathematical operation or assignment involving \nfloating-point numbers, which are a subset of the real numbers typically represented on \ncomputers by an integer of fixed precision scaled by an integer exponent of a fixed base;\n(68)\n\u2018downstream provider\u2019 means a provider of an AI system, including a general-purpose \nAI system, which integrates an AI model, regardless of whether the AI model is provided \nby themselves and vertically integrated or provided by another entity based on \ncontractual relations.\nArticle 4\nAI literacy\nProviders and deployers of AI systems shall take measures to ensure, to their best extent, a \nsufficient level of AI literacy of their staff and other persons dealing with the operation and use \nof AI systems on their behalf, taking into account their technical knowledge, experience, \neducation and training and the context the AI systems are to be used in, and considering the \npersons or groups of persons on whom the AI systems are to be used.", "topic": "AI Regulation in the EU"}}
{"id": "16694022-7d79-4df7-ab91-c6e3dc091093", "question": "In the context of the Regulation's phased implementation timeline, what specific role does standardisation play in ensuring compliance, particularly in relation to the rapid technological advancements and the establishment of governance structures?", "reference_answer": "Standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation, in line with the state of the art, to promote innovation as well as competitiveness and growth in the single market.", "reference_context": "Document 120: EN\nUnited in diversity\nEN\n(121)\nStandardisation should play a key role to provide technical solutions to providers to ensure \ncompliance with this Regulation, in line with the state of the art, to promote innovation as \nwell as competitiveness and growth in the single market. Compliance with harmonised \nstandards as defined in Article 2, point (1)(c), of Regulation (EU) No 1025/2012 of the \nEuropean Parliament and of the Council41, which are normally expected to reflect the \nstate of the art, should be a means for providers to demonstrate conformity with the \nrequirements of this Regulation. A balanced representation of interests involving all \nrelevant stakeholders in the development of standards, in particular SMEs, consumer \norganisations and environmental and social stakeholders in accordance with Articles 5 \nand 6 of Regulation (EU) No 1025/2012 should therefore be encouraged. In order to \nfacilitate compliance, the standardisation requests should be issued by the Commission \nwithout undue delay. When preparing the standardisation request, the Commission \nshould consult the advisory forum and the Board in order to collect relevant expertise. \nHowever, in the absence of relevant references to harmonised standards, the \nCommission should be able to establish, via implementing acts, and after consultation of \nthe advisory forum, common specifications for certain requirements under this \nRegulation. \n41\nRegulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 \nOctober 2012 on European standardisation, amending Council Directives 89/686/EEC and \n93/15/EEC and Directives 94/9/EC, 94/25/EC, 95/16/EC, 97/23/EC, 98/34/EC, 2004/22/EC, \n2007/23/EC, 2009/23/EC and 2009/105/EC of the European Parliament and of the Council \nand repealing Council Decision 87/95/EEC and Decision No 1673/2006/EC of the European \nParliament and of the Council (OJ L 316, 14.11.2012, p. 12).", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 120, "distracting_context": "EN\nUnited in diversity\nEN\n(179)\nThis Regulation should apply from \u2026 [two years from the date of entry into force of this \nRegulation]. However, taking into account the unacceptable risk associated with the use \nof AI in certain ways, the prohibitions as well as the general provisions of this \nRegulation should already apply from \u2026 [six months from the date of entry into force of \nthis Regulation]. While the full effect of those prohibitions follows with the \nestablishment of the governance and enforcement of this Regulation, anticipating the \napplication of the prohibitions is important to take account of unacceptable risks and to \nhave an effect on other procedures, such as in civil law. Moreover, the infrastructure \nrelated to the governance and the conformity assessment system should be operational \nbefore\u2026 [two years from the date of entry into force of this Regulation], therefore the \nprovisions on notified bodies and governance structure should apply from \u2026 [ 12 months \nfrom the date of entry into force of this Regulation]. Given the rapid pace of \ntechnological advancements and adoption of general-purpose AI models, obligations for \nproviders of general-purpose AI models should apply from ... [12 months from the date \nof entry into force of this Regulation]. Codes of practice should be ready by... [9 months \nfrom the date of entry into force of this Regulation] in view of enabling providers to \ndemonstrate compliance on time. The AI Office should ensure that classification rules \nand procedures are up to date in light of technological developments. In addition, \nMember States should lay down and notify to the Commission the rules on penalties, \nincluding administrative fines, and ensure that they are properly and effectively \nimplemented by the date of application of this Regulation. Therefore the provisions on \npenalties should apply from \u2026 [12 months from the date of entry into force of this \nRegulation].", "topic": "AI Regulation in the EU"}}
{"id": "a40e3bf5-c2ea-4e11-bfdd-e137003c6864", "question": "What specific obligations are imposed on providers of very large online platforms regarding AI systems, particularly in relation to compliance with fundamental rights as outlined in the Treaty on the Functioning of the European Union?", "reference_answer": "Providers of very large online platforms and very large online search engines are obliged to assess potential systemic risks stemming from the design, functioning, and use of their services, and to take appropriate mitigating measures in observance of fundamental rights.", "reference_context": "Document 118: EN\nUnited in diversity\nEN\n(118)\nThis Regulation regulates AI systems and AI models by imposing certain requirements \nand obligations for relevant market actors that are placing them on the market, putting \ninto service or use in the Union, thereby complementing obligations for providers of \nintermediary services that embed such systems or models into their services regulated by \nRegulation (EU) 2022/2065. To the extent that such systems or models are embedded \ninto designated very large online platforms or very large online search engines, they are \nsubject to the risk-management framework provided for in Regulation (EU) 2022/2065. \nConsequently, the corresponding obligations of this Regulation should be presumed to \nbe fulfilled, unless significant systemic risks not covered by Regulation (EU) 2022/2065 \nemerge and are identified in such models. Within this framework, providers of very large \nonline platforms and very large online search engines are obliged to assess potential \nsystemic risks stemming from the design, functioning and use of their services, including \nhow the design of algorithmic systems used in the service may contribute to such risks, \nas well as systemic risks stemming from potential misuses. Those providers are also \nobliged to take appropriate mitigating measures in observance of fundamental rights.\n\nDocument 119: EN\nUnited in diversity\nEN\n(119)\nConsidering the quick pace of innovation and the technological evolution of digital \nservices in scope of different instruments of Union law in particular having in mind the \nusage and the perception of their recipients, the AI systems subject to this Regulation \nmay be provided as intermediary services or parts thereof within the meaning of \nRegulation (EU) 2022/2065, which should be interpreted in a technology-neutral \nmanner. For example, AI systems may be used to provide online search engines, in \nparticular, to the extent that an AI system such as an online chatbot performs searches \nof, in principle, all websites, then incorporates the results into its existing knowledge and \nuses the updated knowledge to generate a single output that combines different sources \nof information.\n(120)\nFurthermore, obligations placed on providers and deployers of certain AI systems in this \nRegulation to enable the detection and disclosure that the outputs of those systems are \nartificially generated or manipulated are particularly relevant to facilitate the effective \nimplementation of Regulation (EU) 2022/2065. This applies in particular as regards the \nobligations of providers of very large online platforms or very large online search \nengines to identify and mitigate systemic risks that may arise from the dissemination of \ncontent that has been artificially generated or manipulated, in particular risk of the \nactual or foreseeable negative effects on democratic processes, civic discourse and \nelectoral processes, including through disinformation.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 118, "distracting_context": "EN\nUnited in diversity\nEN\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 \nand 114 thereof,\nHaving regard to the proposal from the European Commission,\nAfter transmission of the draft legislative act to the national parliaments,\nHaving regard to the opinion of the European Economic and Social Committee1,\nHaving regard to the opinion of the European Central Bank2,\nHaving regard to the opinion of the Committee of the Regions3,\nActing in accordance with the ordinary legislative procedure4,\n1\nOJ C 517, 22.12.2021, p. 56.\n2\n OJ C 115, 11.3.2022, p. 5.\n3\nOJ C 97, 28.2.2022, p. 60.\n4\nPosition of the European Parliament of 13 March 2024.", "topic": "AI Regulation in the EU"}}
{"id": "7acbe1bf-8f79-4734-8a78-406d5867e2d2", "question": "What specific responsibilities must deployers of high-risk AI systems adhere to, considering the principles of transparency, diversity, and non-discrimination as outlined in the regulation?", "reference_answer": "Deployers should take appropriate technical and organisational measures to ensure they use high-risk AI systems in accordance with the instructions of use, monitor the functioning of the AI systems, and keep records as appropriate. They should also ensure that persons assigned to implement the instructions for use and human oversight have the necessary competence, including adequate AI literacy, training, and authority.", "reference_context": "Document 95: EN\nUnited in diversity\nEN\n(91)\nGiven the nature of AI systems and the risks to safety and fundamental rights possibly \nassociated with their use, including as regards the need to ensure proper monitoring of the \nperformance of an AI system in a real-life setting, it is appropriate to set specific \nresponsibilities for deployers. Deployers should in particular take appropriate technical \nand organisational measures to ensure they use high-risk AI systems in accordance with \nthe instructions of use and certain other obligations should be provided for with regard to \nmonitoring of the functioning of the AI systems and with regard to record-keeping, as \nappropriate. Furthermore, deployers should ensure that the persons assigned to \nimplement the instructions for use and human oversight as set out in this Regulation \nhave the necessary competence, in particular an adequate level of AI literacy, training \nand authority to properly fulfil those tasks. Those obligations should be without \nprejudice to other deployer obligations in relation to high-risk AI systems under Union \nor national law.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 95, "distracting_context": "EN\nUnited in diversity\nEN\nTechnical robustness and safety means that AI systems are developed and used in a way \nthat allows robustness in the case of problems and resilience against attempts to alter the \nuse or performance of the AI system so as to allow unlawful use by third parties, and \nminimise unintended harm. Privacy and data governance means that AI systems are \ndeveloped and used in accordance with privacy and data protection rules, while \nprocessing data that meets high standards in terms of quality and integrity. \nTransparency means that AI systems are developed and used in a way that allows \nappropriate traceability and explainability, while making humans aware that they \ncommunicate or interact with an AI system, as well as duly informing deployers of the \ncapabilities and limitations of that AI system and affected persons about their rights. \nDiversity, non-discrimination and fairness means that AI systems are developed and \nused in a way that includes diverse actors and promotes equal access, gender equality \nand cultural diversity, while avoiding discriminatory impacts and unfair biases that are \nprohibited by Union or national law. Social and environmental well-being means that AI \nsystems are developed and used in a sustainable and environmentally friendly manner as \nwell as in a way to benefit all human beings, while monitoring and assessing the long-\nterm impacts on the individual, society and democracy. The application of those \nprinciples should be translated, when possible, in the design and use of AI models. They \nshould in any case serve as a basis for the drafting of codes of conduct under this \nRegulation. All stakeholders, including industry, academia, civil society and \nstandardisation organisations, are encouraged to take into account, as appropriate, the \nethical principles for the development of voluntary best practices and standards.", "topic": "High-Risk AI Systems"}}
{"id": "a1fc2275-03e1-4fb7-ab7d-589179294ce3", "question": "In the context of the EU financial services law, what are some intended applications of AI systems in the realm of education and vocational training that align with regulatory compliance?", "reference_answer": "AI systems intended to be used to determine access or admission to educational institutions, evaluate learning outcomes, assess the appropriate level of education for individuals, and monitor prohibited behavior of students during tests.", "reference_context": "Document 438: EN\nUnited in diversity\nEN\n2.\n\u258cCritical infrastructure: \nAI systems intended to be used as safety components in the \nmanagement and operation of critical digital infrastructure, road traffic, or in the supply \nof water, gas, heating or electricity.\n3.\nEducation and vocational training:\n(a)\nAI systems intended to be used to determine access or admission or to assign \nnatural persons to educational and vocational training institutions at all levels;\n(b)\nAI systems intended to be used to evaluate learning outcomes, including when \nthose outcomes are used to steer the learning process of natural persons in \neducational and vocational training institutions at all levels;\n(c)\nAI systems intended to be used for the purpose of assessing the appropriate level of \neducation that an individual will receive or will be able to access, in the context of \nor within educational and vocational training institutions at all levels;\n(d)\nAI systems intended to be used for monitoring and detecting prohibited behaviour \nof students during tests in the context of or within educational and vocational \ntraining institutions at all levels.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 438, "distracting_context": "EN\nUnited in diversity\nEN\n(158)\nUnion financial services law includes internal governance and risk-management rules and \nrequirements which are applicable to regulated financial institutions in the course of \nprovision of those services, including when they make use of AI systems. In order to \nensure coherent application and enforcement of the obligations under this Regulation and \nrelevant rules and requirements of the Union financial services legal acts, the competent \nauthorities for the supervision and enforcement of those legal acts, in particular competent \nauthorities as defined in Regulation (EU) No 575/2013 of the European Parliament and \nof the Council46 and Directives 2008/48/EC47, 2009/138/EC48, 2013/36/EU49, \n2014/17/EU50 and (EU) 2016/9751of the European Parliament and of the Council, should \nbe designated, within their respective competences, as competent authorities for the \npurpose of supervising the implementation of this Regulation, including for market \nsurveillance activities, as regards AI systems provided or used by regulated and supervised \nfinancial institutions unless Member States decide to designate another authority to fulfil \nthese market surveillance tasks. \n46\nRegulation (EU) No 575/2013 of the European Parliament and of the Council of 26 June \n2013 on prudential requirements for credit institutions and investment firms and amending \nRegulation (EU) No 648/2012 (OJ L 176, 27.6.2013, p. 1).\n47\nDirective 2008/48/EC of the European Parliament and of the Council of 23 April 2008 on \ncredit agreements for consumers and repealing Council Directive 87/102/EEC (OJ L 133, \n22.5.2008, p. 66).\n48\nDirective 2009/138/EC of the European Parliament and of the Council of 25 November \n2009 on the taking-up and pursuit of the business of Insurance and Reinsurance (Solvency \nII) (OJ L 335, 17.12.2009, p. 1).", "topic": "AI Regulation and Ethics"}}
{"id": "f3ab47fe-90bb-41d4-93af-c59729adfdfd", "question": "What specific actions should a provider undertake if their general-purpose AI model meets the threshold for high-impact capabilities, particularly in relation to the information required under Article 49(3) for high-risk AI systems?", "reference_answer": "The provider should notify the AI Office at the latest two weeks after the requirements are met or it becomes known that a general-purpose AI model will meet the requirements that lead to the presumption.", "reference_context": "Document 111: EN\nUnited in diversity\nEN\n(111)\nIt is appropriate to establish a methodology for the classification of general-purpose AI \nmodels as general-purpose AI model with systemic risks. Since systemic risks result from \nparticularly high capabilities, a general-purpose AI model should be considered to \npresent systemic risks if it has high-impact capabilities, evaluated on the basis of \nappropriate technical tools and methodologies, or significant impact on the internal \nmarket due to its reach. High-impact capabilities in general-purpose AI models means \ncapabilities that match or exceed the capabilities recorded in the most advanced general-\npurpose AI models. The full range of capabilities in a model could be better understood \nafter its placing on the market or when deployers interact with the model. According to \nthe state of the art at the time of entry into force of this Regulation, the cumulative \namount of computation used for the training of the general-purpose AI model measured \nin floating point operations is one of the relevant approximations for model capabilities. \nThe cumulative amount of computation used for training includes the computation used \nacross the activities and methods that are intended to enhance the capabilities of the \nmodel prior to deployment, such as pre-training, synthetic data generation and fine-\ntuning. Therefore, an initial threshold of floating point operations should be set, which, \nif met by a general-purpose AI model, leads to a presumption that the model is a \ngeneral-purpose AI model with systemic risks. This threshold should be adjusted over \ntime to reflect technological and industrial changes, such as algorithmic improvements \nor increased hardware efficiency, and should be supplemented with benchmarks and \nindicators for model capability.\n\nDocument 113: EN\nUnited in diversity\nEN\n(112)\nIt is also necessary to clarify a procedure for the classification of a general-purpose AI \nmodel with systemic risks. A general-purpose AI model that meets the applicable \nthreshold for high-impact capabilities should be presumed to be a general-purpose AI \nmodels with systemic risk. The provider should notify the AI Office at the latest two \nweeks after the requirements are met or it becomes known that a general-purpose AI \nmodel will meet the requirements that lead to the presumption. This is especially relevant \nin relation to the threshold of floating point operations because training of general-\npurpose AI models takes considerable planning which includes the upfront allocation of \ncompute resources and, therefore, providers of general-purpose AI models are able to \nknow if their model would meet the threshold before the training is completed. In the \ncontext of that notification, the provider should be able to demonstrate that, because of \nits specific characteristics, a general-purpose AI model exceptionally does not present \nsystemic risks, and that it thus should not be classified as a general-purpose AI model \nwith systemic risks. That information is valuable for the AI Office to anticipate the \nplacing on the market of general-purpose AI models with systemic risks and the \nproviders can start to engage with the AI Office early on. That information is especially \nimportant with regard to general-purpose AI models that are planned to be released as \nopen-source, given that, after the open-source model release, necessary measures to \nensure compliance with the obligations under this Regulation may be more difficult to \nimplement.\n\nDocument 112: EN\nUnited in diversity\nEN\nTo inform this, the AI Office should engage with the scientific community, industry, civil \nsociety and other experts. Thresholds, as well as tools and benchmarks for the \nassessment of high-impact capabilities, should be strong predictors of generality, its \ncapabilities and associated systemic risk of general-purpose AI models, and could take \ninto account the way the model will be placed on the market or the number of users it \nmay affect. To complement this system, there should be a possibility for the Commission \nto take individual decisions designating a general-purpose AI model as a general-\npurpose AI model with systemic risk if it is found that such model has capabilities or an \nimpact equivalent to those captured by the set threshold. That decision should be taken \non the basis of an overall assessment of the criteria for the designation of a general-\npurpose AI model with systemic risk set out in an annex to this Regulation, such as \nquality or size of the training data set, number of business and end users, its input and \noutput modalities, its level of autonomy and scalability, or the tools it has access to. \nUpon a reasoned request of a provider whose model has been designated as a general-\npurpose AI model with systemic risk, the Commission should take the request into \naccount and may decide to reassess whether the general-purpose AI model can still be \nconsidered to present systemic risks.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 111, "distracting_context": "EN\nUnited in diversity\nEN\nSection C - Information to be submitted by deployers of high-risk AI systems in accordance with \nArticle 49(3)\nThe following information shall be provided and thereafter kept up to date with regard to high-\nrisk AI systems to be registered in accordance with Article 49:\n1.\nThe name, address and contact details of the deployer;\n2.\nThe name, address and contact details of the person submitting information on behalf of \nthe deployer;\n3.\nThe URL of the entry of the AI system in the EU database by its provider;\n4.\nA summary of the findings of the fundamental rights impact assessment conducted in \naccordance with Article 27;\n5.\nA summary of the data protection impact assessment carried out in accordance with \nArticle 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680 as \nspecified in Article 26(8) of this Regulation, where applicable.", "topic": "Systemic Risks in AI"}}
{"id": "fdb2e471-e330-4da1-9279-396cdc806bdf", "question": "What additional considerations must the Commission take into account when amending the conditions laid down in paragraph 3, second subparagraph, particularly in relation to the compliance of AI systems with training data requirements?", "reference_answer": "The Commission must ensure that any amendment does not decrease the overall level of protection of health, safety and fundamental rights provided for by this Regulation and ensures consistency with the delegated acts adopted pursuant to Article 7(1), taking account of market and technological developments.", "reference_context": "Document 205: EN\nUnited in diversity\nEN\n7.\nThe Commission shall adopt delegated acts in accordance with Article 97 in order to \namend paragraph 3, second subparagraph, of this Article by deleting any of the \nconditions laid down therein, where there is concrete and reliable evidence that this is \nnecessary to maintain the level of protection of health, safety and fundamental rights \nprovided for by this Regulation.\n8.\nAny amendment to the conditions laid down in paragraph 3, second subparagraph, \nadopted in accordance with paragraphs 6 and 7 of this Article shall not decrease the \noverall level of protection of health, safety and fundamental rights provided for by this \nRegulation and shall ensure consistency with the delegated acts adopted pursuant to \nArticle 7(1), and take account of market and technological developments.\nArticle 7\nAmendments to Annex III\n1.\nThe Commission is empowered to adopt delegated acts in accordance with Article 97 to \namend Annex III by adding or modifying use-cases of high-risk AI systems where both of \nthe following conditions are fulfilled:\n(a)\nthe AI systems are intended to be used in any of the areas listed in Annex III;", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 205, "distracting_context": "EN\nUnited in diversity\nEN\nWhere the AI system does not meet the requirement relating to the data used to train it, re-\ntraining of the AI system will be needed prior to the application for a new conformity \nassessment. In this case, the reasoned assessment decision of the notified body refusing to \nissue the Union technical documentation assessment certificate shall contain specific \nconsiderations on the quality data used to train the AI system, in particular on the reasons \nfor non-compliance.\n4.7.\nAny change to the AI system that could affect the compliance of the AI system with the \nrequirements or its intended purpose shall be assessed by the notified body which issued \nthe Union technical documentation assessment certificate. The provider shall inform such \nnotified body of its intention to introduce any of the abovementioned changes, or if it \notherwise becomes aware of the occurrence of such changes. The intended changes shall \nbe assessed by the notified body, which shall decide whether those changes require a new \nconformity assessment in accordance with Article 43(4) or whether they could be \naddressed by means of a supplement to the Union technical documentation assessment \ncertificate. In the latter case, the notified body shall assess the changes, notify the provider \nof its decision and, where the changes are approved, issue to the provider a supplement to \nthe Union technical documentation assessment certificate.", "topic": "Others"}}
{"id": "95c2178e-7e07-4a04-961a-9063a924edf0", "question": "What steps should an individual take if they suspect a violation of the Regulation on AI systems, particularly in relation to high-risk AI systems that may affect law enforcement or migration processes?", "reference_answer": "Any natural or legal person that has grounds to consider that there has been an infringement of this Regulation should be entitled to lodge a complaint to the relevant market surveillance authority.", "reference_context": "Document 164: EN\nUnited in diversity\nEN\n(169)\nCompliance with the obligations on providers of general-purpose AI models imposed \nunder this Regulation should be enforceable, inter alia, by means of fines. To that end, \nappropriate levels of fines should also be laid down for infringement of those \nobligations, including the failure to comply with measures requested by the Commission \nin accordance with this Regulation, subject to appropriate limitation periods in \naccordance with the principle of proportionality. All decisions taken by the Commission \nunder this Regulation are subject to review by the Court of Justice of the European \nUnion in accordance with the TFEU, including the unlimited jurisdiction of the Court of \nJustice with regard to penalties pursuant to Article 261 TFEU.\n(170)\nUnion and national law already provide effective remedies to natural and legal persons \nwhose rights and freedoms are adversely affected by the use of AI systems. Without \nprejudice to those remedies, any natural or legal person that has grounds to consider \nthat there has been an infringement of this Regulation should be entitled to lodge a \ncomplaint to the relevant market surveillance authority.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 164, "distracting_context": "EN\nUnited in diversity\nEN\nOther deployers should be entitled to do so voluntarily. This section of the EU database \nshould be publicly accessible, free of charge, the information should be easily navigable, \nunderstandable and machine-readable. The EU database should also be user-friendly, \nfor example by providing search functionalities, including through keywords, allowing \nthe general public to find relevant information to be submitted upon the registration of \nhigh-risk AI systems and on the use case of high-risk AI systems, set out in an annex to \nthis Regulation, to which the high-risk AI systems correspond. Any substantial \nmodification of high-risk AI systems should also be registered in the EU database. For \nhigh-risk AI systems in the area of law enforcement, migration, asylum and border \ncontrol management, the registration obligations should be fulfilled in a secure non-\npublic section of the EU database. Access to the secure non-public section should be \nstrictly limited to the Commission as well as to market surveillance authorities with \nregard to their national section of that database. High-risk AI systems in the area of \ncritical infrastructure should only be registered at national level. The Commission should \nbe the controller of the EU database, in accordance with Regulation (EU) 2018/1725. In \norder to ensure the full functionality of the EU database, when deployed, the procedure for \nsetting the database should include the development of functional specifications by the \nCommission and an independent audit report. The Commission should take into account \ncybersecurity risks when carrying out its tasks as data controller on the EU database. In \norder to maximise the availability and use of the EU database by the public, the EU \ndatabase, including the information made available through it, should comply with \nrequirements under the Directive (EU) 2019/882.", "topic": "AI Regulation in the EU"}}
{"id": "3fcab111-7631-4e00-9d90-0ae59b0a35a3", "question": "What specific documentation and compliance measures are mandated for high-risk AI systems to be placed on the Union market, particularly in relation to risk management and post-market performance evaluation?", "reference_answer": "High-risk AI systems should only be placed on the Union market, put into service or used if they comply with certain mandatory requirements that ensure they do not pose unacceptable risks to important Union public interests as recognized and protected by Union law.", "reference_context": "Document 123: EN\nUnited in diversity\nEN\n(124)\nIt is appropriate that, in order to minimise the burden on operators and avoid any possible \nduplication, for high-risk AI systems related to products which are covered by existing \nUnion harmonisation legislation based on the New Legislative Framework, the compliance \nof those AI systems with the requirements of this Regulation should be assessed as part of \nthe conformity assessment already provided for in that law. The applicability of the \nrequirements of this Regulation should thus not affect the specific logic, methodology or \ngeneral structure of conformity assessment under the relevant Union harmonisation \nlegislation. \u258c\n(125)\nGiven the complexity of high-risk AI systems and the risks that are associated with them, \nit is important to develop an adequate conformity assessment procedure for high-risk AI \nsystems involving notified bodies, so-called third party conformity assessment. However, \ngiven the current experience of professional pre-market certifiers in the field of product \nsafety and the different nature of risks involved, it is appropriate to limit, at least in an \ninitial phase of application of this Regulation, the scope of application of third-party \nconformity assessment for high-risk AI systems other than those related to products. \nTherefore, the conformity assessment of such systems should be carried out as a general \nrule by the provider under its own responsibility, with the only exception of AI systems \nintended to be used for biometrics.\n\nDocument 46: EN\nUnited in diversity\nEN\n(46)\nHigh-risk AI systems should only be placed on the Union market, put into service or used \nif they comply with certain mandatory requirements. Those requirements should ensure \nthat high-risk AI systems available in the Union or whose output is otherwise used in the \nUnion do not pose unacceptable risks to important Union public interests as recognised and \nprotected by Union law. On the basis of the New Legislative Framework, as clarified in \nthe Commission notice \u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules \n2022\u201d20, the general rule is that more than one legal act of Union harmonisation \nlegislation, such as Regulations (EU) 2017/74521 and (EU) 2017/74622 of the European \nParliament and of the Council or Directive 2006/42/EC of the European Parliament and \nof the Council23, may be applicable to one product, since the making available or putting \ninto service can take place only when the product complies with all applicable Union \nharmonisation legislation. To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.\n\nDocument 281: EN\nUnited in diversity\nEN\nFor the purposes of the conformity assessment procedure referred to in Annex VII, the \nprovider may choose any of the notified bodies. However, where the high-risk AI system is \nintended to be put into service by law enforcement, immigration or asylum authorities or \nby Union institutions, bodies, offices or agencies, the market surveillance authority \nreferred to in Article 74(8) or (9), as applicable, shall act as a notified body.\n2.\nFor high-risk AI systems referred to in points 2 to 8 of Annex III, \u258c providers shall follow \nthe conformity assessment procedure based on internal control as referred to in Annex VI, \nwhich does not provide for the involvement of a notified body. \u258c\n3.\nFor high-risk AI systems covered by the Union harmonisation legislation listed in Section \nA of Annex I, the provider shall follow the relevant conformity assessment procedure as \nrequired under those legal acts. The requirements set out in Section 2 of this Chapter shall \napply to those high-risk AI systems and shall be part of that assessment. Points 4.3., 4.4., \n4.5. and the fifth paragraph of point 4.6 of Annex VII shall also apply.\nFor the purposes of that assessment, notified bodies which have been notified under those \nlegal acts shall be entitled to control the conformity of the high-risk AI systems with the \nrequirements set out in Section 2, provided that the compliance of those notified bodies \nwith requirements laid down in Article 31(4), (5), (10) and (11) has been assessed in the \ncontext of the notification procedure under those legal acts.\n\nDocument 56: EN\nUnited in diversity\nEN\n(50)\nAs regards AI systems that are safety components of products, or which are themselves \nproducts, falling within the scope of certain Union harmonisation legislation listed in an \nannex to this Regulation, it is appropriate to classify them as high-risk under this \nRegulation if the product concerned undergoes the conformity assessment procedure with a \nthird-party conformity assessment body pursuant to that relevant Union harmonisation \nlegislation. In particular, such products are machinery, toys, lifts, equipment and protective \nsystems intended for use in potentially explosive atmospheres, radio equipment, pressure \nequipment, recreational craft equipment, cableway installations, appliances burning \ngaseous fuels, medical devices, in vitro diagnostic medical devices, automotive and \naviation.\n(51)\nThe classification of an AI system as high-risk pursuant to this Regulation should not \nnecessarily mean that the product whose safety component is the AI system, or the AI \nsystem itself as a product, is considered to be high-risk under the criteria established in the \nrelevant Union harmonisation legislation that applies to the product. This is, in particular, \nthe case for Regulations (EU) 2017/745 and (EU) 2017/746, where a third-party \nconformity assessment is provided for medium-risk and high-risk products.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 123, "distracting_context": "EN\nUnited in diversity\nEN\n5.\nA detailed description of the risk management system in accordance with Article 9;\n6.\nA description of relevant changes made by the provider to the system through its \nlifecycle;\n7.\nA list of the harmonised standards applied in full or in part the references of which have \nbeen published in the Official Journal of the European Union; where no such harmonised \nstandards have been applied, a detailed description of the solutions adopted to meet the \nrequirements set out in Chapter III, Section 2, including a list of other relevant standards \nand technical specifications applied;\n8.\nA copy of the EU declaration of conformity referred to in Article 47;\n9.\nA detailed description of the system in place to evaluate the AI system performance in the \npost-market phase in accordance with Article 72, including the post-market monitoring \nplan referred to in Article 72(3).", "topic": "High-Risk AI Regulation"}}
{"id": "688e376f-6e63-4663-a8c0-b333f6a9c145", "question": "What additional requirements must be fulfilled before placing a high-risk AI system on the market according to Article 49, particularly regarding quality management systems and post-market monitoring?", "reference_answer": "The provider or, where applicable, the authorised representative shall register themselves and their system in the EU database referred to in Article 71.", "reference_context": "Document 292: EN\nUnited in diversity\nEN\nArticle 49\nRegistration\n1.\nBefore placing on the market or putting into service a high-risk AI system listed in \nAnnex III, with the exception of high-risk AI systems referred to in point 2 of Annex III, \nthe provider or, where applicable, the authorised representative shall register themselves \nand their system in the EU database referred to in Article 71.\n2.\nBefore placing on the market or putting into service an AI system for which the provider \nhas concluded that it is not high-risk according to Article 6(3), that provider or, where \napplicable, the authorised representative shall register themselves and that system in the \nEU database referred to in Article 71.\n3.\nBefore putting into service or using a high-risk AI system listed in Annex III, with the \nexception of high-risk AI systems listed in point 2 of Annex III, deployers that are public \nauthorities, Union institutions, bodies, offices or agencies or persons acting on their \nbehalf shall register themselves, select the system and register its use in the EU database \nreferred to in Article 71.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 292, "distracting_context": "EN\nUnited in diversity\nEN\n(81)\nThe provider should establish a sound quality management system, ensure the \naccomplishment of the required conformity assessment procedure, draw up the relevant \ndocumentation and establish a robust post-market monitoring system. Providers of high-\nrisk AI systems that are subject to obligations regarding quality management systems \nunder relevant sectoral Union law should have the possibility to include the elements of \nthe quality management system provided for in this Regulation as part of the existing \nquality management system provided for in that other sectoral Union law. The \ncomplementarity between this Regulation and existing sectoral Union law should also be \ntaken into account in future standardisation activities or guidance adopted by the \nCommission. Public authorities which put into service high-risk AI systems for their own \nuse may adopt and implement the rules for the quality management system as part of the \nquality management system adopted at a national or regional level, as appropriate, taking \ninto account the specificities of the sector and the competences and organisation of the \npublic authority concerned.", "topic": "High-Risk AI Regulation"}}
{"id": "bb788141-6890-40ed-bd36-d4ed7bcf6f44", "question": "What are the justifications for classifying AI systems utilized in education as high-risk, particularly in light of the potential impacts on fundamental rights and the necessity for effective mitigation of identified risks during their development and testing?", "reference_answer": "AI systems used in education should be classified as high-risk because they may determine the educational and professional course of a person\u2019s life and therefore may affect that person\u2019s ability to secure a livelihood.", "reference_context": "Document 63: EN\nUnited in diversity\nEN\n(56)\nThe deployment of AI systems in education is important to promote high-quality digital \neducation and training and to allow all learners and teachers to acquire and share the \nnecessary digital skills and competences, including media literacy, and critical thinking, \nto take an active part in the economy, society, and in democratic processes. However, AI \nsystems used in education or vocational training, in particular for determining access or \nadmission, for assigning persons to educational and vocational training institutions or \nprogrammes at all levels, for evaluating learning outcomes of persons, for assessing the \nappropriate level of education for an individual and materially influencing the level of \neducation and training that individuals will receive or will be able to access or for \nmonitoring and detecting prohibited behaviour of students during tests should be \nclassified as high-risk AI systems, since they may determine the educational and \nprofessional course of a person\u2019s life and therefore may affect that person\u2019s ability to \nsecure a livelihood. When improperly designed and used, such systems may be \nparticularly intrusive and may violate the right to education and training as well as the \nright not to be discriminated against and perpetuate historical patterns of discrimination, \nfor example against women, certain age groups, persons with disabilities, or persons of \ncertain racial or ethnic origins or sexual orientation.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 63, "distracting_context": "EN\nUnited in diversity\nEN\n11.\nThe AI regulatory sandboxes shall not affect the supervisory or corrective powers of the \ncompetent authorities supervising the sandboxes, including at regional or local level. Any \nsignificant risks to health and safety and fundamental rights identified during the \ndevelopment and testing of such AI systems shall result in an adequate mitigation. \nNational competent authorities shall have the power to temporarily or permanently \nsuspend the testing process, or the participation in the sandbox if no effective mitigation \nis possible, and shall inform the AI Office of such decision. National competent \nauthorities shall exercise their supervisory powers within the limits of the relevant law, \nusing their discretionary powers when implementing legal provisions in respect of a \nspecific AI regulatory sandbox project, with the objective of supporting innovation in AI \nin the Union.\n12.\nProviders and prospective providers participating in the AI regulatory sandbox shall \nremain liable under applicable Union and national liability law for any damage inflicted on \nthird parties as a result of the experimentation taking place in the sandbox. However, \nprovided that the prospective providers observe the specific plan and the terms and \nconditions for their participation and follow in good faith the guidance given by the \nnational competent authority, no administrative fines shall be imposed by the authorities \nfor infringements of this Regulation. Where other competent authorities responsible for \nother Union and national law were actively involved in the supervision of the AI system \nin the sandbox and provided guidance for compliance, no administrative fines shall be \nimposed regarding that law.", "topic": "AI Regulation and Ethics"}}
{"id": "a745a8ff-6f5b-43d1-a8ba-41d30ae0d879", "question": "In light of the provisions outlined in Article 90, what specific procedural rights does Article 94 delineate for economic operators associated with general-purpose AI models?", "reference_answer": "Article 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis to the providers of the general-purpose AI model, without prejudice to more specific procedural rights provided for in this Regulation.", "reference_context": "Document 396: EN\nUnited in diversity\nEN\nArticle 94\nProcedural rights of economic operators of the general-purpose AI model\nArticle 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis to the providers of the \ngeneral-purpose AI model, without prejudice to more specific procedural rights provided for in this \nRegulation.\nCHAPTER X\nCODES OF CONDUCT AND GUIDELINES\nArticle 95\nCodes of conduct for voluntary application of specific requirements\n1.\nThe AI Office and the Member States shall encourage and facilitate the drawing up of \ncodes of conduct, including related governance mechanisms, intended to foster the \nvoluntary application to AI systems, other than high-risk AI systems, of some or all of the \nrequirements set out in Chapter III, Section 2 taking into account the available technical \nsolutions and industry best practices allowing for the application of such requirements.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 396, "distracting_context": "EN\nUnited in diversity\nEN\nArticle 90\nAlerts of systemic risks by the scientific panel\n1.\nThe scientific panel may provide a qualified alert to the AI Office where it has reason to \nsuspect that:\n(a)\na general-purpose AI model poses concrete identifiable risk at Union level; or,\n(b)\na general-purpose AI model meets the conditions referred to in Article 51.\n2.\nUpon such qualified alert, the Commission, through the AI Office and after having \ninformed the Board, may exercise the powers laid down in this Section for the purpose of \nassessing the matter. The AI Office shall inform the Board of any measure according to \nArticles 91 to 94.\n3.\nA qualified alert shall be duly reasoned and indicate at least:\n(a)\nthe point of contact of the provider of the general-purpose AI model with systemic \nrisk concerned;", "topic": "AI Regulation and Governance"}}
{"id": "9e3bf945-d683-4637-9899-3e86825c6f11", "question": "What is the role of the fundamental rights impact assessment in evaluating high-risk AI systems, particularly in relation to systemic risks that could affect public health and safety?", "reference_answer": "The purpose of the fundamental rights impact assessment is for the deployer to identify the specific risks to the rights of individuals or groups of individuals likely to be affected, and to identify measures to be taken in the case of a materialisation of those risks.", "reference_context": "Document 255: EN\nUnited in diversity\nEN\nArticle 27\nFundamental rights impact assessment for high-risk AI systems\n1.\nPrior to deploying a high-risk AI system referred to in Article 6(2), with the exception of \nhigh-risk AI systems intended to be used in the area listed in point 2 of Annex III, \ndeployers that are bodies governed by public law, or are private entities providing public \nservices, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of \nAnnex III, shall perform an assessment of the impact on fundamental rights that the use \nof such system may produce. For that purpose, deployers shall perform an assessment \nconsisting of:\n(a)\na description of the deployer\u2019s processes in which the high-risk AI system will be \nused in line with its intended purpose;\n(b)\na description of the period of time within which, and the frequency with which, \neach high-risk AI system is intended to be used;\n(c)\nthe categories of natural persons and groups likely to be affected by its use in the \nspecific context;\n\nDocument 99: EN\nUnited in diversity\nEN\n(96)\nIn order to efficiently ensure that fundamental rights are protected, deployers of high-\nrisk AI systems that are bodies governed by public law, or private entities providing \npublic services and deployers of certain high-risk AI systems listed in an annex to this \nRegulation, such as banking or insurance entities, should carry out a fundamental \nrights impact assessment prior to putting it into use. Services important for individuals \nthat are of public nature may also be provided by private entities. Private entities \nproviding such public services are linked to tasks in the public interest such as in the \nareas of education, healthcare, social services, housing, administration of justice. The \naim of the fundamental rights impact assessment is for the deployer to identify the \nspecific risks to the rights of individuals or groups of individuals likely to be affected, \nidentify measures to be taken in the case of a materialisation of those risks. The impact \nassessment should be performed prior to deploying the high-risk AI system, and should \nbe updated when the deployer considers that any of the relevant factors have changed. \nThe impact assessment should identify the deployer\u2019s relevant processes in which the \nhigh-risk AI system will be used in line with its intended purpose, and should include a \ndescription of the period of time and frequency in which the system is intended to be \nused as well as of specific categories of natural persons and groups who are likely to be \naffected in the specific context of use.\n\nDocument 100: EN\nUnited in diversity\nEN\nThe assessment should also include the identification of specific risks of harm likely to \nhave an impact on the fundamental rights of those persons or groups. While performing \nthis assessment, the deployer should take into account information relevant to a proper \nassessment of the impact, including but not limited to the information given by the \nprovider of the high-risk AI system in the instructions for use. In light of the risks \nidentified, deployers should determine measures to be taken in the case of a \nmaterialisation of those risks, including for example governance arrangements in that \nspecific context of use, such as arrangements for human oversight according to the \ninstructions of use or, complaint handling and redress procedures, as they could be \ninstrumental in mitigating risks to fundamental rights in concrete use-cases. After \nperforming that impact assessment, the deployer should notify the relevant market \nsurveillance authority. Where appropriate, to collect relevant information necessary to \nperform the impact assessment, deployers of high-risk AI system, in particular when AI \nsystems are used in the public sector, could involve relevant stakeholders, including the \nrepresentatives of groups of persons likely to be affected by the AI system, independent \nexperts, and civil society organisations in conducting such impact assessments and \ndesigning measures to be taken in the case of materialisation of the risks. The European \nArtificial Intelligence Office (AI Office) should develop a template for a questionnaire in \norder to facilitate compliance and reduce the administrative burden for deployers.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 255, "distracting_context": "EN\nUnited in diversity\nEN\n(62)\n\u2018critical infrastructure\u2019 means critical infrastructure as defined in Article 2, point (4), of \nDirective (EU) 2022/2557;\n(63)\n\u2018general-purpose AI model\u2019 means an AI model, including where such an AI model is \ntrained with a large amount of data using self-supervision at scale, that displays \nsignificant generality and is capable of competently performing a wide range of distinct \ntasks regardless of the way the model is placed on the market and that can be integrated \ninto a variety of downstream systems or applications, except AI models that are used for \nresearch, development or prototyping activities before they are placed on the market;\n(64)\n\u2018high-impact capabilities\u2019 means capabilities that match or exceed the capabilities \nrecorded in the most advanced general-purpose AI models;\n(65)\n\u2018systemic risk\u2019 means a risk that is specific to the high-impact capabilities of general-\npurpose AI models, having a significant impact on the Union market due to their reach, \nor due to actual or reasonably foreseeable negative effects on public health, safety, \npublic security, fundamental rights, or the society as a whole, that can be propagated at \nscale across the value chain;", "topic": "Others"}}
{"id": "903ea84c-f75b-4457-b322-37127920e573", "question": "In light of Article 53, what specific elements must providers of general-purpose AI models ensure are continuously updated, particularly considering the potential administrative fines related to the nature and gravity of any infringements?", "reference_answer": "Providers of general-purpose AI models shall keep up-to-date the technical documentation of the model, including its training and testing process and the results of its evaluation.", "reference_context": "Document 302: EN\nUnited in diversity\nEN\nSection 2\nObligations for providers of general-purpose AI models\nArticle 53\nObligations for providers of general-purpose AI models\n1.\nProviders of general-purpose AI models shall:\n(a)\ndraw up and keep up-to-date the technical documentation of the model, including \nits training and testing process and the results of its evaluation, which shall \ncontain, at a minimum, the information set out in Annex XI for the purpose of \nproviding it, upon request, to the AI Office and the national competent authorities;\n(b)\ndraw up, keep up-to-date and make available information and documentation to \nproviders of AI systems who intend to integrate the general-purpose AI model into \ntheir AI systems. Without prejudice to the need to observe and protect intellectual \nproperty rights and confidential business information or trade secrets in \naccordance with Union and national law, the information and documentation \nshall:\n(i)\nenable providers of AI systems to have a good understanding of the \ncapabilities and limitations of the general-purpose AI model and to comply \nwith their obligations pursuant to this Regulation; and", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 302, "distracting_context": "EN\nUnited in diversity\nEN\n7.\nWhen deciding whether to impose an administrative fine and when deciding on the \namount of the administrative fine in each individual case, all relevant circumstances of the \nspecific situation shall be taken into account and, as appropriate, regard shall be given to \nthe following:\n(a)\nthe nature, gravity and duration of the infringement and of its consequences, taking \ninto account the purpose of the AI system, as well as, where appropriate, the \nnumber of affected persons and the level of damage suffered by them;\n(b)\nwhether administrative fines have already been applied by other market surveillance \nauthorities to the same operator for the same infringement;\n(c)\nwhether administrative fines have already been applied by other authorities to the \nsame operator for infringements of other Union or national law, when such \ninfringements result from the same activity or omission constituting a relevant \ninfringement of this Regulation;\n(d)\nthe size, the annual turnover and market share of the operator committing the \ninfringement;", "topic": "AI Regulation and Governance"}}
{"id": "72b05e4f-7f1d-403f-92e9-ce952df1c5d2", "question": "What specific documentation must providers of high-risk AI systems submit to competent authorities upon a reasoned request, considering the requirements outlined in Annex V of the EU AI Act?", "reference_answer": "Providers of high-risk AI systems shall provide all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements set out in Section 2.", "reference_context": "Document 238: EN\nUnited in diversity\nEN\nArticle 21\nCooperation with competent authorities\n1.\nProviders of high-risk AI systems shall, upon a reasoned request by a \u258c competent \nauthority, provide that authority \u258c all the information and documentation necessary to \ndemonstrate the conformity of the high-risk AI system with the requirements set out in \nSection 2, in a language which can be easily understood by the authority in one of the \nofficial languages of the institutions of the Union as indicated by the Member State \nconcerned.\n2.\nUpon a reasoned request by a competent authority, providers shall also give the \nrequesting competent authority, as applicable, access to the automatically generated logs \nof the high-risk AI system referred to in Article 12(1), to the extent such logs are under \ntheir control.\n3.\nAny information obtained by a competent authority pursuant to this Article shall be \ntreated in accordance with the confidentiality obligations set out in Article 78.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 238, "distracting_context": "EN\nUnited in diversity\nEN\nANNEX V\nEU declaration of conformity\nThe EU declaration of conformity referred to in Article 47, shall contain all of the following \ninformation:\n1.\nAI system name and type and any additional unambiguous reference allowing the \nidentification and traceability of the AI system;\n2.\nThe name and address of the provider or, where applicable, of their authorised \nrepresentative;\n3.\nA statement that the EU declaration of conformity referred to in Article 47 is issued under \nthe sole responsibility of the provider;\n4.\nA statement that the AI system is in conformity with this Regulation and, if applicable, \nwith any other relevant Union law that provides for the issuing of the EU declaration of \nconformity referred to in Article 47;\n5.\nWhere an AI system involves the processing of personal data, a statement that that AI \nsystem complies with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive \n(EU) 2016/680;\n6.\nReferences to any relevant harmonised standards used or any other common specification \nin relation to which conformity is declared;\n7.\nWhere applicable, the name and identification number of the notified body, a description \nof the conformity assessment procedure performed, and identification of the certificate \nissued;\n8.\nThe place and date of issue of the declaration, the name and function of the person who \nsigned it, as well as an indication for, or on behalf of whom, that person signed, a \nsignature.", "topic": "High-Risk AI Regulation"}}
{"id": "f9fd38d2-8420-4085-91ed-befb79523cba", "question": "What specific investigative and corrective powers are assigned to market surveillance authorities concerning high-risk AI systems, particularly in the context of biometrics, while ensuring compliance with Union law on copyright and related rights?", "reference_answer": "Each market surveillance authority should have effective investigative and corrective powers, including at least the power to obtain access to all personal data that are being processed and to all information necessary for the performance of its tasks.", "reference_context": "Document 366: EN\nUnited in diversity\nEN\n11.\nMarket surveillance authorities and the Commission shall be able to propose joint \nactivities, including joint investigations, to be conducted by either market surveillance \nauthorities or market surveillance authorities jointly with the Commission, that have the \naim of promoting compliance, identifying non-compliance, raising awareness or \nproviding guidance in relation to this Regulation with respect to specific categories of \nhigh-risk AI systems that are found to present a serious risk across two or more Member \nStates in accordance with Article 9 of Regulation (EU) 2019/1020. The AI Office shall \nprovide coordination support for joint investigations.\n12.\nWithout prejudice to the powers provided for under Regulation (EU) 2019/1020, and \nwhere relevant and limited to what is necessary to fulfil their tasks, the market \nsurveillance authorities shall be granted full access by providers to the documentation as \nwell as the training, validation and testing data sets used for the development of high-\nrisk AI systems, including, where appropriate and subject to security safeguards, \nthrough application programming interfaces (API) or other relevant technical means \nand tools enabling remote access.\n\nDocument 156: EN\nUnited in diversity\nEN\n(159)\nEach market surveillance authority for high-risk AI systems in the area of biometrics, as \nlisted in an annex to this Regulation insofar as those systems are used for the purposes \nof law enforcement, migration, asylum and border control management, or the \nadministration of justice and democratic processes, should have effective investigative \nand corrective powers, including at least the power to obtain access to all personal data \nthat are being processed and to all information necessary for the performance of its \ntasks. The market surveillance authorities should be able to exercise their powers by \nacting with complete independence. Any limitations of their access to sensitive \noperational data under this Regulation should be without prejudice to the powers \nconferred to them by Directive (EU) 2016/680. No exclusion on disclosing data to \nnational data protection authorities under this Regulation should affect the current or \nfuture powers of those authorities beyond the scope of this Regulation.\n(160)\nThe market surveillance authorities and the Commission should be able to propose joint \nactivities, including joint investigations, to be conducted by market surveillance \nauthorities or market surveillance authorities jointly with the Commission, that have the \naim of promoting compliance, identifying non-compliance, raising awareness and \nproviding guidance in relation to this Regulation with respect to specific categories of \nhigh-risk AI systems that are found to present a serious risk across two or more Member \nStates. Joint activities to promote compliance should be carried out in accordance with \nArticle 9 of Regulation (EU) 2019/1020. The AI Office should provide coordination \nsupport for joint investigations.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 366, "distracting_context": "EN\nUnited in diversity\nEN\n(ii)\ncontain, at a minimum, the elements set out in Annex XII;\n(c)\nput in place a policy to comply with Union law on copyright and related rights, and \nin particular to identify and comply with, including through state-of-the-art \ntechnologies, a reservation of rights expressed pursuant to Article 4(3) of Directive \n(EU) 2019/790;\n(d)\ndraw up and make publicly available a sufficiently detailed summary about the \ncontent used for training of the general-purpose AI model, according to a template \nprovided by the AI Office.\n2.\nThe obligations set out in paragraph 1, points (a) and (b), shall not apply to providers of \nAI models that are released under a free and open-source licence that allows for the \naccess, usage, modification, and distribution of the model, and whose parameters, \nincluding the weights, the information on the model architecture, and the information \non model usage, are made publicly available. This exception shall not apply to general-\npurpose AI models with systemic risks.\n3.\nProviders of general-purpose AI models shall cooperate as necessary with the \nCommission and the national competent authorities in the exercise of their competences \nand powers pursuant to this Regulation.", "topic": "AI Regulation and Compliance"}}
{"id": "73393f07-f3c9-418a-8df3-5e97d569b3ac", "question": "In light of the obligations outlined in the context, what role do AI regulatory sandboxes play in ensuring compliance with both Union and national laws while facilitating the development of AI systems?", "reference_answer": "AI regulatory sandboxes facilitate the development of tools and infrastructure for testing, benchmarking, assessing and explaining dimensions of AI systems relevant for regulatory learning, such as accuracy, robustness and cybersecurity, as well as measures to mitigate risks to fundamental rights and society at large.", "reference_context": "Document 323: EN\nUnited in diversity\nEN\n(g)\nthat procedures, processes and administrative requirements for application, \nselection, participation and exiting the AI regulatory sandbox are simple, easily \nintelligible, and clearly communicated in order to facilitate the participation of \nSMEs, including start-ups, with limited legal and administrative capacities and are \nstreamlined across the Union, in order to avoid fragmentation and that \nparticipation in an AI regulatory sandbox established by a Member State, or by the \nEuropean Data Protection Supervisor is mutually and uniformly recognised and \ncarries the same legal effects across the Union;\n(h)\nthat participation in the AI regulatory sandbox is limited to a period that is \nappropriate to the complexity and scale of the project and that may be extended by \nthe national competent authority;\n(i)\nthat AI regulatory sandboxes facilitate the development of tools and infrastructure \nfor testing, benchmarking, assessing and explaining dimensions of AI systems \nrelevant for regulatory learning, such as accuracy, robustness and cybersecurity, \nas well as measures to mitigate risks to fundamental rights and society at large.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 323, "distracting_context": "EN\nUnited in diversity\nEN\n3.\nThe obligations set out in paragraphs 1 and 2, are without prejudice to other deployer \nobligations under Union or national law and to the deployer\u2019s freedom to organise its own \nresources and activities for the purpose of implementing the human oversight measures \nindicated by the provider.\n4.\nWithout prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over \nthe input data, that deployer shall ensure that input data is relevant and sufficiently \nrepresentative in view of the intended purpose of the high-risk AI system.", "topic": "AI Regulatory Sandboxes"}}
{"id": "9efe3cc6-4acb-4a79-8da9-0faa8756f0d4", "question": "Under what specific conditions can the implementation of real-time remote biometric identification systems for law enforcement proceed without prior authorization, particularly in the context of compliance with Article 76 regarding market surveillance authorities?", "reference_answer": "In a duly justified situation of urgency, the use of such system may be commenced without an authorisation provided that such authorisation is requested without undue delay, at the latest within 24 hours.", "reference_context": "Document 38: EN\nUnited in diversity\nEN\n(35)\nEach use of a \u2018real-time\u2019 remote biometric identification system in publicly accessible \nspaces for the purpose of law enforcement should be subject to an express and specific \nauthorisation by a judicial authority or by an independent administrative authority of a \nMember State whose decision is binding. Such authorisation should, in principle, be \nobtained prior to the use of the AI system with a view to identifying a person or persons. \nExceptions to that rule should be allowed in duly justified situations on grounds of \nurgency, namely in situations where the need to use the systems concerned is such as to \nmake it effectively and objectively impossible to obtain an authorisation before \ncommencing the use of the AI system. In such situations of urgency, the use of the AI \nsystem should be restricted to the absolute minimum necessary and should be subject to \nappropriate safeguards and conditions, as determined in national law and specified in the \ncontext of each individual urgent use case by the law enforcement authority itself. In \naddition, the law enforcement authority should in such situations request such \nauthorisation \u258c while providing the reasons for not having been able to request it earlier, \nwithout undue delay and at the latest within 24 hours. If such an authorisation is \nrejected, the use of real-time biometric identification systems linked to that authorisation \nshould cease with immediate effect and all the data related to such use should be \ndiscarded and deleted. Such data includes input data directly acquired by an AI system \nin the course of the use of such system as well as the results and outputs of the use \nlinked to that authorisation. It should not include input that is legally acquired in \naccordance with another Union or national law. In any case, no decision producing an \nadverse legal effect on a person should be taken based solely on the output of the remote \nbiometric identification system.\n\nDocument 199: EN\nUnited in diversity\nEN\n3.\nFor the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each \u258c use \nfor the purposes of law enforcement of a \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be subject to a prior authorisation granted by a judicial \nauthority or \u258c an independent administrative authority whose decision is binding of the \nMember State in which the use is to take place, issued upon a reasoned request and in \naccordance with the detailed rules of national law referred to in paragraph 5. However, in a \nduly justified situation of urgency, the use of such system may be commenced without an \nauthorisation provided that such authorisation is requested without undue delay, at the \nlatest within 24 hours. If such authorisation is rejected, the use shall be stopped with \nimmediate effect and all the data, as well as the results and outputs of that use shall be \nimmediately discarded and deleted.\nThe competent judicial authority or an independent administrative authority whose \ndecision is binding shall grant the authorisation only where it is satisfied, on the basis of \nobjective evidence or clear indications presented to it, that the use of the \u2018real-time\u2019 remote \nbiometric identification system concerned is necessary for, and proportionate to, achieving \none of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in \nthe request and, in particular, remains limited to what is strictly necessary concerning \nthe period of time as well as the geographic and personal scope. In deciding on the \nrequest, that authority shall take into account the elements referred to in paragraph 2. No \ndecision that produces an adverse legal effect on a person may be taken based solely on \nthe output of the \u2018real-time\u2019 remote biometric identification system.\n\nDocument 40: EN\nUnited in diversity\nEN\n(38)\nThe use of AI systems for real-time remote biometric identification of natural persons in \npublicly accessible spaces for the purpose of law enforcement necessarily involves the \nprocessing of biometric data. The rules of this Regulation that prohibit, subject to certain \nexceptions, such use, which are based on Article 16 TFEU, should apply as lex specialis in \nrespect of the rules on the processing of biometric data contained in Article 10 of Directive \n(EU) 2016/680, thus regulating such use and the processing of biometric data involved in \nan exhaustive manner. Therefore, such use and processing should be possible only in as far \nas it is compatible with the framework set by this Regulation, without there being scope, \noutside that framework, for the competent authorities, where they act for purpose of law \nenforcement, to use such systems and process such data in connection thereto on the \ngrounds listed in Article 10 of Directive (EU) 2016/680. In that context, this Regulation is \nnot intended to provide the legal basis for the processing of personal data under Article 8 \nof Directive (EU) 2016/680. However, the use of real-time remote biometric identification \nsystems in publicly accessible spaces for purposes other than law enforcement, including \nby competent authorities, should not be covered by the specific framework regarding such \nuse for the purpose of law enforcement set by this Regulation. Such use for purposes other \nthan law enforcement should therefore not be subject to the requirement of an authorisation \nunder this Regulation and the applicable detailed rules of national law that may give effect \nto that authorisation.\n\nDocument 34: EN\nUnited in diversity\nEN\n(32)\nThe use of AI systems for \u2018real-time\u2019 remote biometric identification of natural persons in \npublicly accessible spaces for the purpose of law enforcement is particularly intrusive to \nthe rights and freedoms of the concerned persons, to the extent that it may affect the \nprivate life of a large part of the population, evoke a feeling of constant surveillance and \nindirectly dissuade the exercise of the freedom of assembly and other fundamental rights. \nTechnical inaccuracies of AI systems intended for the remote biometric identification of \nnatural persons can lead to biased results and entail discriminatory effects. Such \npossible biased results and discriminatory effects are particularly relevant with regard to \nage, ethnicity, race, sex or disabilities. In addition, the immediacy of the impact and the \nlimited opportunities for further checks or corrections in relation to the use of such systems \noperating in real-time carry heightened risks for the rights and freedoms of the persons \nconcerned in the context of, or impacted by, law enforcement activities.\n(33)\nThe use of those systems for the purpose of law enforcement should therefore be \nprohibited, except in exhaustively listed and narrowly defined situations, where the use is \nstrictly necessary to achieve a substantial public interest, the importance of which \noutweighs the risks. Those situations involve the search for certain victims of crime \u258c \nincluding missing persons; certain threats to the life or to the physical safety of natural \npersons or of a terrorist attack; and the localisation or identification of perpetrators or \nsuspects of the criminal offences listed in an annex to this Regulation, where those \ncriminal offences are punishable in the Member State concerned by a custodial sentence or \na detention order for a maximum period of at least four years and as they are defined in the \nlaw of that Member State. Such a threshold for the custodial sentence or detention order in \naccordance with national law contributes to ensuring that the offence should be serious \nenough to potentially justify the use of \u2018real-time\u2019 remote biometric identification systems.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 38, "distracting_context": "EN\nUnited in diversity\nEN\nArticle 76\nSupervision of testing in real world conditions by market surveillance authorities\n1.\nMarket surveillance authorities shall have competences and powers to ensure that \ntesting in real world conditions is in accordance with this Regulation.\n2.\nWhere testing in real world conditions is conducted for AI systems that are supervised \nwithin an AI regulatory sandbox under Article 58, the market surveillance authorities \nshall verify the compliance with Article 60 as part of their supervisory role for the AI \nregulatory sandbox. Those authorities may, as appropriate, allow the testing in real \nworld conditions to be conducted by the provider or prospective provider, in derogation \nfrom the conditions set out in Article 60(4), points (f) and (g).\n3.\nWhere a market surveillance authority has been informed by the prospective provider, \nthe provider or any third party of a serious incident or has other grounds for considering \nthat the conditions set out in Articles 60 and 61 are not met, it may take either of the \nfollowing decisions on its territory, as appropriate:\n(a)\nto suspend or terminate the testing in real world conditions;", "topic": "Biometric Data and AI"}}
{"id": "5a25f95e-4cc6-45b3-95c7-d413804f9594", "question": "How is the term 'publicly accessible space' defined in the context of systemic risks associated with general-purpose AI models, particularly in relation to their capabilities and market impact?", "reference_answer": "'Publicly accessible space' means any publicly or privately owned physical place accessible to an undetermined number of natural persons, regardless of whether certain conditions for access may apply, and regardless of the potential capacity restrictions.", "reference_context": "Document 186: EN\nUnited in diversity\nEN\n(44)\n\u2018publicly accessible space\u2019 means any publicly or privately owned physical place \naccessible to an undetermined number of natural persons, regardless of whether certain \nconditions for access may apply, and regardless of the potential capacity restrictions;\n(45)\n\u2018law enforcement authority\u2019 means:\n(a)\nany public authority competent for the prevention, investigation, detection or \nprosecution of criminal offences or the execution of criminal penalties, including the \nsafeguarding against and the prevention of threats to public security; or\n(b)\nany other body or entity entrusted by Member State law to exercise public authority \nand public powers for the purposes of the prevention, investigation, detection or \nprosecution of criminal offences or the execution of criminal penalties, including the \nsafeguarding against and the prevention of threats to public security;\n(46)\n\u2018law enforcement\u2019 means activities carried out by law enforcement authorities or on their \nbehalf for the prevention, investigation, detection or prosecution of criminal offences or \nthe execution of criminal penalties, including safeguarding against and preventing threats \nto public security;\n(47)\n\u2018AI Office\u2019 means the Commission\u2019s function of contributing to the implementation, \nmonitoring and supervision of AI systems and general-purpose AI models, and AI \ngovernance, provided for in Commission Decision of 24 January 2024; references in this \nRegulation to the AI Office shall be construed as references to the Commission;", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 186, "distracting_context": "EN\nUnited in diversity\nEN\n(111)\nIt is appropriate to establish a methodology for the classification of general-purpose AI \nmodels as general-purpose AI model with systemic risks. Since systemic risks result from \nparticularly high capabilities, a general-purpose AI model should be considered to \npresent systemic risks if it has high-impact capabilities, evaluated on the basis of \nappropriate technical tools and methodologies, or significant impact on the internal \nmarket due to its reach. High-impact capabilities in general-purpose AI models means \ncapabilities that match or exceed the capabilities recorded in the most advanced general-\npurpose AI models. The full range of capabilities in a model could be better understood \nafter its placing on the market or when deployers interact with the model. According to \nthe state of the art at the time of entry into force of this Regulation, the cumulative \namount of computation used for the training of the general-purpose AI model measured \nin floating point operations is one of the relevant approximations for model capabilities. \nThe cumulative amount of computation used for training includes the computation used \nacross the activities and methods that are intended to enhance the capabilities of the \nmodel prior to deployment, such as pre-training, synthetic data generation and fine-\ntuning. Therefore, an initial threshold of floating point operations should be set, which, \nif met by a general-purpose AI model, leads to a presumption that the model is a \ngeneral-purpose AI model with systemic risks. This threshold should be adjusted over \ntime to reflect technological and industrial changes, such as algorithmic improvements \nor increased hardware efficiency, and should be supplemented with benchmarks and \nindicators for model capability.", "topic": "Biometric Data and AI"}}
{"id": "c10841c6-a8fe-4e71-b604-308d6246543b", "question": "What considerations should be taken into account when determining the most suitable risk management measures for high-risk AI systems, particularly in the absence of harmonised standards as referenced in the EU regulations?", "reference_answer": "The following shall be ensured: elimination or reduction of risks identified and evaluated, implementation of adequate mitigation and control measures for risks that cannot be eliminated, and provision of information required pursuant to Article 13 and, where appropriate, training to deployers.", "reference_context": "Document 212: EN\nUnited in diversity\nEN\n4.\nThe risk management measures referred to in paragraph 2, point (d), shall give due \nconsideration to the effects and possible interaction resulting from the combined \napplication of the requirements set out in this Section, with a view to minimising risks \nmore effectively while achieving an appropriate balance in implementing the measures \nto fulfil those requirements.\n5.\nThe risk management measures referred to in paragraph 2, point (d), shall be such that the \nrelevant residual risk associated with each hazard, as well as the overall residual risk of the \nhigh-risk AI systems is judged to be acceptable.\nIn identifying the most appropriate risk management measures, the following shall be \nensured:\n(a)\nelimination or reduction of risks identified and evaluated pursuant to paragraph 2 \nin as far as technically feasible through adequate design and development of the \nhigh-risk AI system;\n(b)\nwhere appropriate, implementation of adequate mitigation and control measures \naddressing risks that cannot be eliminated;\n(c)\nprovision of information required pursuant to Article 13 and, where appropriate, \ntraining to deployers. \u258c", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 212, "distracting_context": "EN\nUnited in diversity\nEN\n(b)\nno reference to harmonised standards covering the requirements referred to in \nSection 2 of this Chapter or, as applicable, the obligations referred to in Sections 2 \nand 3 of Chapter V has been published in the Official Journal of the European \nUnion in accordance with Regulation (EU) No 1025/2012, and no such reference \nis expected to be published within a reasonable period.\nWhen drafting the common specifications, the Commission shall consult the advisory \nforum referred to in Article 67.\nThe implementing acts referred to in the first subparagraph of this paragraph shall be \nadopted in accordance with the examination procedure referred to in Article 98(2).\n2.\nBefore preparing a draft implementing act, the Commission shall inform the committee \nreferred to in Article 22 of Regulation (EU) No 1025/2012 that it considers the \nconditions laid down in paragraph 1 of this Article to be fulfilled.", "topic": "High-Risk AI Regulation"}}
{"id": "ad0c070c-c2db-4c99-a1f9-6a85d8095f8d", "question": "What specific responsibilities should deployers assume regarding high-risk AI systems that have been designated as general-purpose AI models with systemic risk, particularly in light of the ongoing reassessment process by the Commission?", "reference_answer": "Deployers should take appropriate technical and organisational measures to ensure they use high-risk AI systems in accordance with the instructions of use, monitor the functioning of the AI systems, and maintain record-keeping as appropriate. They should also ensure that persons assigned to implement the instructions have the necessary competence, including adequate AI literacy, training, and authority.", "reference_context": "Document 95: EN\nUnited in diversity\nEN\n(91)\nGiven the nature of AI systems and the risks to safety and fundamental rights possibly \nassociated with their use, including as regards the need to ensure proper monitoring of the \nperformance of an AI system in a real-life setting, it is appropriate to set specific \nresponsibilities for deployers. Deployers should in particular take appropriate technical \nand organisational measures to ensure they use high-risk AI systems in accordance with \nthe instructions of use and certain other obligations should be provided for with regard to \nmonitoring of the functioning of the AI systems and with regard to record-keeping, as \nappropriate. Furthermore, deployers should ensure that the persons assigned to \nimplement the instructions for use and human oversight as set out in this Regulation \nhave the necessary competence, in particular an adequate level of AI literacy, training \nand authority to properly fulfil those tasks. Those obligations should be without \nprejudice to other deployer obligations in relation to high-risk AI systems under Union \nor national law.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 95, "distracting_context": "EN\nUnited in diversity\nEN\n5.\nUpon a reasoned request of a provider whose model has been designated as a general-\npurpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take \nthe request into account and may decide to reassess whether the general-purpose AI \nmodel can still be considered to present systemic risks on the basis of the criteria set out \nin Annex XIII. Such a request shall contain objective, detailed and new reasons that \nhave arisen since the designation decision. Providers may request reassessment at the \nearliest six months after the designation decision. Where the Commission, following its \nreassessment, decides to maintain the designation as a general-purpose AI model with \nsystemic risk, providers may request reassessment at the earliest six months after that \ndecision.\n6.\nThe Commission shall ensure that a list of general-purpose AI models with systemic risk \nis published and shall keep that list up to date, without prejudice to the need to observe \nand protect intellectual property rights and confidential business information or trade \nsecrets in accordance with Union and national law.", "topic": "High-Risk AI Systems"}}
{"id": "42257c04-fd20-49dc-8843-5025e87c5c94", "question": "What are some examples of systemic risks posed by general-purpose AI models that may arise from their compliance with the EU AI Act, particularly in relation to large-scale IT systems established by legal acts listed in Annex X?", "reference_answer": "Systemic risks include negative effects related to major accidents, disruptions of critical sectors, public health and safety, democratic processes, public and economic security, and the dissemination of illegal, false, or discriminatory content.", "reference_context": "Document 110: EN\nUnited in diversity\nEN\n(110)\nGeneral-purpose AI models could pose systemic risks which include, but are not limited \nto, any actual or reasonably foreseeable negative effects in relation to major accidents, \ndisruptions of critical sectors and serious consequences to public health and safety; any \nactual or reasonably foreseeable negative effects on democratic processes, public and \neconomic security; the dissemination of illegal, false, or discriminatory content. \nSystemic risks should be understood to increase with model capabilities and model \nreach, can arise along the entire lifecycle of the model, and are influenced by conditions \nof misuse, model reliability, model fairness and model security, the level of autonomy of \nthe model, its access to tools, novel or combined modalities, release and distribution \nstrategies, the potential to remove guardrails and other factors. In particular, \ninternational approaches have so far identified the need to pay attention to risks from \npotential intentional misuse or unintended issues of control relating to alignment with \nhuman intent; chemical, biological, radiological, and nuclear risks, such as the ways in \nwhich barriers to entry can be lowered, including for weapons development, design \nacquisition, or use; offensive cyber capabilities, such as the ways in vulnerability \ndiscovery, exploitation, or operational use can be enabled; the effects of interaction and \ntool use, including for example the capacity to control physical systems and interfere \nwith critical infrastructure; risks from models of making copies of themselves or \u2018self-\nreplicating\u2019 or training other models; the ways in which models can give rise to harmful \nbias and discrimination with risks to individuals, communities or societies; the \nfacilitation of disinformation or harming privacy with threats to democratic values and \nhuman rights; risk that a particular event could lead to a chain reaction with \nconsiderable negative effects that could affect up to an entire city, an entire domain \nactivity or an entire community.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 110, "distracting_context": "EN\nUnited in diversity\nEN\nArticle 111\nAI systems already placed on the market or put into service and general-purpose AI models already \nplaced on the marked\n1.\nWithout prejudice to the application of Article 5 as referred to in Article 113(3), point \n(a), AI systems which are components of the large-scale IT systems established by the \nlegal acts listed in Annex X that have been placed on the market or put into service before \n\u258c\u2026 [36 months from the date of entry into force of this Regulation] shall be brought into \ncompliance with this Regulation by 31 December 2030.\nThe requirements laid down in this Regulation shall be taken into account \u258c in the \nevaluation of each large-scale IT system established by the legal acts listed in Annex X to \nbe undertaken as provided for in those legal acts and where those legal acts are replaced \nor amended.", "topic": "Systemic Risks in AI"}}
{"id": "822218d3-5054-44b8-8f67-3dcb18c9492c", "question": "What specific obligations do deployers of high-risk AI systems have under Article 26, particularly in relation to the guidance provided by national competent authorities for testing within AI regulatory sandboxes?", "reference_answer": "Deployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems, and they must assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.", "reference_context": "Document 248: EN\nUnited in diversity\nEN\nThe AI Office may develop and recommend voluntary model terms for contracts between \nproviders of high-risk AI systems and third parties that supply tools, services, \ncomponents or processes that are used for or integrated into high-risk AI systems. When \ndeveloping those voluntary model terms, the AI Office shall take into account possible \ncontractual requirements applicable in specific sectors or business cases. The voluntary \nmodel terms shall be published and be available free of charge in an easily usable \nelectronic format.\n5.\nParagraphs 2 and 3 are without prejudice to the need to observe and protect intellectual \nproperty rights, confidential business information and trade secrets in accordance with \nUnion and national law.\nArticle 26\nObligations of deployers of high-risk AI systems\n1.\nDeployers of high-risk AI systems shall take appropriate technical and organisational \nmeasures to ensure they use such systems in accordance with the instructions for use \naccompanying the systems, pursuant to paragraphs 3 and 6.\n2.\nDeployers shall assign human oversight to natural persons who have the necessary \ncompetence, training and authority, as well as the necessary support.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 248, "distracting_context": "EN\nUnited in diversity\nEN\n3.\nProspective providers in the AI regulatory sandboxes, in particular SMEs and start-ups, \nshall be directed, where relevant, to pre-deployment services such as guidance on the \nimplementation of this Regulation, to other value-adding services such as help with \nstandardisation documents and certification, testing and experimentation facilities, \nEuropean Digital Innovation Hubs and centres of excellence.\n4.\nWhere national competent authorities consider authorising testing in real world \nconditions supervised within the framework of an AI regulatory sandbox to be \nestablished under this Article, they shall specifically agree the terms and conditions of \nsuch testing and, in particular, the appropriate safeguards with the participants, with a \nview to protecting fundamental rights, health and safety. Where appropriate, they shall \ncooperate with other national competent authorities with a view to ensuring consistent \npractices across the Union.", "topic": "High-Risk AI Systems"}}
{"id": "11ef5fb0-b664-4fa7-a14f-4794ef830e5d", "question": "What specific details must the implementing acts include regarding the AI regulatory sandboxes, particularly in relation to ensuring compliance with safeguards for fundamental rights and the processing of special categories of personal data?", "reference_answer": "The implementing acts must specify the detailed arrangements for the establishment, development, implementation, operation, and supervision of the AI regulatory sandboxes, including eligibility and selection criteria, procedures for application and participation, and the terms and conditions applicable to participants.", "reference_context": "Document 321: EN\nUnited in diversity\nEN\nArticle 58\nDetailed arrangements for, and functioning of, AI regulatory sandboxes\n1.\nIn order to avoid fragmentation across the Union, the Commission shall adopt \nimplementing acts specifying the detailed arrangements for the establishment, \ndevelopment, implementation, operation and supervision of the AI regulatory sandboxes. \nThe implementing acts shall include common principles on the following issues:\n(a)\neligibility and selection criteria for participation in the AI regulatory sandbox;\n(b)\nprocedures for the application, participation, monitoring, exiting from and \ntermination of the AI regulatory sandbox, including the sandbox plan and the exit \nreport;\n(c)\nthe terms and conditions applicable to the participants.\nThose implementing acts shall be adopted in accordance with the examination \nprocedure referred to in Article 98(2).\n2.\nThe implementing acts referred to in paragraph 1 shall ensure:\n(a)\nthat AI regulatory sandboxes are open to any applying provider or prospective \nprovider of an AI system who fulfils eligibility and selection criteria, which shall be \ntransparent and fair, and that national competent authorities inform applicants of \ntheir decision within three months of the application;", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 321, "distracting_context": "EN\nUnited in diversity\nEN\n(70)\nIn order to protect the right of others from the discrimination that might result from the \nbias in AI systems, the providers should, exceptionally, to the extent that it is strictly \nnecessary for the purpose of ensuring bias detection and correction in relation to the \nhigh-risk AI systems, subject to appropriate safeguards for the fundamental rights and \nfreedoms of natural persons and following the application of all applicable conditions \nlaid down under this Regulation in addition to the conditions laid down in Regulations \n(EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, be able to process also \nspecial categories of personal data, as a matter of substantial public interest within the \nmeaning of Article 9(2), point (g) of Regulation (EU) 2016/679 and Article 10(2), point \n(g) of Regulation (EU) 2018/1725.\n(71)\nHaving comprehensible information on how high-risk AI systems have been developed \nand how they perform throughout their lifetime is essential to enable traceability of those \nsystems, verify compliance with the requirements under this Regulation, as well as \nmonitoring of their operations and post market monitoring. This requires keeping records \nand the availability of technical documentation, containing information which is necessary \nto assess the compliance of the AI system with the relevant requirements and facilitate \npost market monitoring. Such information should include the general characteristics, \ncapabilities and limitations of the system, algorithms, data, training, testing and validation \nprocesses used as well as documentation on the relevant risk-management system and \ndrawn in a clear and comprehensive form. The technical documentation should be kept \nup to date, appropriately throughout the lifetime of the AI system. Furthermore, high-\nrisk AI systems should technically allow for the automatic recording of events, by means \nof logs, over the duration of the lifetime of the system.", "topic": "AI Regulatory Sandboxes"}}
{"id": "4704a84b-1a5b-4428-99db-520eb622dc4c", "question": "What procedural steps must be taken, including the involvement of national competent authorities, before any confidential information concerning high-risk AI systems is shared with the Commission?", "reference_answer": "Prior consultation of the originating national competent authority and the deployer shall occur before disclosing information.", "reference_context": "Document 374: EN\nUnited in diversity\nEN\n3.\nWithout prejudice to paragraphs 1 and 2, information exchanged on a confidential basis \nbetween the national competent authorities or between national competent authorities and \nthe Commission shall not be disclosed without prior consultation of the originating \nnational competent authority and the deployer when high-risk AI systems referred to in \npoint 1, 6 or 7 of Annex III are used by law enforcement, border control, immigration or \nasylum authorities and when such disclosure would jeopardise public and national security \ninterests. This exchange of information shall not cover sensitive operational data in \nrelation to the activities of law enforcement, border control, immigration or asylum \nauthorities.\nWhen the law enforcement, immigration or asylum authorities are providers of high-risk \nAI systems referred to in point 1, 6 or 7 of Annex III, the technical documentation referred \nto in Annex IV shall remain within the premises of those authorities. Those authorities \nshall ensure that the market surveillance authorities referred to in Article 74(8) and (9), as \napplicable, can, upon request, immediately access the documentation or obtain a copy \nthereof. Only staff of the market surveillance authority holding the appropriate level of \nsecurity clearance shall be allowed to access that documentation or any copy thereof.", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 374, "distracting_context": "EN\nUnited in diversity\nEN\n2.\nMember States shall communicate to the Commission the identity of the notifying \nauthorities and the market surveillance authorities and the tasks of those authorities, as \nwell as any subsequent changes thereto. Member States shall make publicly available \ninformation on how competent authorities and single points of contact can be contacted, \nthrough electronic communication means by\u2026 [12 months from the date of entry into \nforce of this Regulation]. Member States shall designate a market surveillance authority \nto act as the single point of contact for this Regulation, and shall notify the Commission \nof the identity of the single point of contact. The Commission shall make a list of the \nsingle points of contact publicly available.\n3.\nMember States shall ensure that their national competent authorities are provided with \nadequate technical, financial and human resources, and with infrastructure to fulfil their \ntasks effectively under this Regulation. In particular, the national competent authorities \nshall have a sufficient number of personnel permanently available whose competences and \nexpertise shall include an in-depth understanding of AI technologies, data and data \ncomputing, personal data protection, cybersecurity, fundamental rights, health and safety \nrisks and knowledge of existing standards and legal requirements. Member States shall \nassess and, if necessary, update competence and resource requirements referred to in \nthis paragraph on an annual basis.\n4.\nNational competent authorities shall take appropriate measures to ensure an adequate \nlevel of cybersecurity.\n5.\nWhen performing their tasks, the national competent authorities shall act in accordance \nwith the confidentiality obligations set out in Article 78.", "topic": "AI Regulation and Compliance"}}
{"id": "abc49d26-a6f5-4c9f-90e1-fb567653b385", "question": "What additional considerations must providers established in third countries take into account regarding potential harm or adverse impacts on fundamental rights before placing a general-purpose AI model on the Union market?", "reference_answer": "Providers established in third countries must appoint an authorised representative which is established in the Union by written mandate.", "reference_context": "Document 305: EN\nUnited in diversity\nEN\nArticle 54\nAuthorised representatives of providers of general-purpose AI models\n1.\nPrior to placing a general-purpose AI model on the Union market, providers established \nin third countries shall, by written mandate, appoint an authorised representative which \nis established in the Union.\n2.\nThe provider shall enable its authorised representative to perform the tasks specified in \nthe mandate received from the provider.\n3.\nThe authorised representative shall perform the tasks specified in the mandate received \nfrom the provider. It shall provide a copy of the mandate to the AI Office upon request, \nin one of the official languages of the institutions of the Union. For the purposes of this \nRegulation, the mandate shall empower the authorised representative to carry out the \nfollowing tasks:\n(a)\nverify that the technical documentation specified in Annex XI has been drawn up \nand all obligations referred to in Article 53 and, where applicable, Article 55 have \nbeen fulfilled by the provider;\n(b)\nkeep a copy of the technical documentation specified in Annex XI at the disposal \nof the AI Office and national competent authorities, for a period of 10 years after \nthe general-purpose AI model has been placed on the market, and the contact \ndetails of the provider that appointed the authorised representative;", "conversation_history": [], "metadata": {"question_type": "distracting element", "seed_document_id": 305, "distracting_context": "EN\nUnited in diversity\nEN\n(e)\nthe extent to which the use of an AI system has already caused harm to \u258c health and \nsafety, has had an adverse impact on \u258c fundamental rights or has given rise to \nsignificant concerns in relation to the likelihood of such harm or adverse impact, as \ndemonstrated, for example, by reports or documented allegations submitted to \nnational competent authorities or by other reports, as appropriate;\n(f)\nthe potential extent of such harm or such adverse impact, in particular in terms of its \nintensity and its ability to affect multiple persons or to disproportionately affect a \nparticular group of persons;\n(g)\nthe extent to which persons who are potentially harmed or suffer an adverse impact \nare dependent on the outcome produced with an AI system, in particular because for \npractical or legal reasons it is not reasonably possible to opt-out from that outcome;\n(h)\nthe extent to which there is an imbalance of power, or the persons who are \npotentially harmed or suffer an adverse impact are in a vulnerable position in relation \nto the deployer of an AI system, in particular due to status, authority, knowledge, \neconomic or social circumstances, or age;", "topic": "AI Regulation and Governance"}}
{"id": "09a5411f-c3f8-4740-b8a6-174f5a5da506", "question": "What can they see?", "reference_answer": "The notified body shall be granted full access to the training, validation, and testing data sets used, including, where appropriate and subject to security safeguards, through API or other relevant technical means and tools enabling remote access.", "reference_context": "Document 454: EN\nUnited in diversity\nEN\n4.3.\nThe technical documentation shall be examined by the notified body. Where relevant, and \nlimited to what is necessary to fulfil its tasks, the notified body shall be granted full access \nto the training, validation, and testing data sets used, including, where appropriate and \nsubject to security safeguards, through API or other relevant technical means and tools \nenabling remote access.\n4.4.\nIn examining the technical documentation, the notified body may require that the provider \nsupply further evidence or carry out further tests so as to enable a proper assessment of the \nconformity of the AI system with the requirements set out in Chapter III, Section 2. Where \nthe notified body is not satisfied with the tests carried out by the provider, the notified \nbody shall itself directly carry out adequate tests, as appropriate.\n4.5.\nWhere necessary to assess the conformity of the high-risk AI system with the requirements \nset out in Chapter III, Section 2, after all other reasonable means to verify conformity \nhave been exhausted and have proven to be insufficient, and upon a reasoned request, the \nnotified body shall also be granted access to the training and trained models of the AI \nsystem, including its relevant parameters. Such access shall be subject to existing Union \nlaw on the protection of intellectual property and trade secrets.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 454, "topic": "AI Quality Management"}}
{"id": "53af7171-4e3f-498e-be0c-d78ccd54c6b3", "question": "What's the aim of it?", "reference_answer": "The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework for the development, the placing on the market, the putting into service and the use of artificial intelligence systems (AI systems) in the Union, in accordance with Union values, to promote the uptake of human centric and trustworthy artificial intelligence (AI) while ensuring a high level of protection of health, safety, fundamental rights, and to support innovation.", "reference_context": "Document 6: EN\nUnited in diversity\nEN\n(8)\nA Union legal framework laying down harmonised rules on AI is therefore needed to foster \nthe development, use and uptake of AI in the internal market that at the same time meets a \nhigh level of protection of public interests, such as health and safety and the protection of \nfundamental rights, including democracy, the rule of law and environmental protection \nas recognised and protected by Union law. To achieve that objective, rules regulating the \nplacing on the market, the putting into service and the use of certain AI systems should be \nlaid down, thus ensuring the smooth functioning of the internal market and allowing those \nsystems to benefit from the principle of free movement of goods and services. Those rules \nshould be clear and robust in protecting fundamental rights, supportive of new \ninnovative solutions, enabling a European ecosystem of public and private actors \ncreating AI systems in line with Union values and unlocking the potential of the digital \ntransformation across all regions of the Union. By laying down those rules as well as \nmeasures in support of innovation with a particular focus on small and medium \nenterprises (SMEs), including startups, this Regulation supports the objective of \npromoting the European human-centric approach to AI and being a global leader in the \ndevelopment of secure, trustworthy and ethical AI \u258c as stated by the European Council5, \nand it ensures the protection of ethical principles, as specifically requested by the European \nParliament6.\n5\nEuropean Council, Special meeting of the European Council (1 and 2 October 2020) \u2013 \nConclusions, EUCO 13/20, 2020, p. 6.\n6\nEuropean Parliament resolution of 20 October 2020 with recommendations to the \nCommission on a framework of ethical aspects of artificial intelligence, robotics and related \ntechnologies, 2020/2012(INL).\n\nDocument 2: EN\nUnited in diversity\nEN\nWhereas:\n(1)\nThe purpose of this Regulation is to improve the functioning of the internal market by \nlaying down a uniform legal framework in particular for the development, the placing on \nthe market, the putting into service and the use of artificial intelligence systems (AI \nsystems) in the Union, in accordance with Union values, to promote the uptake of human \ncentric and trustworthy artificial intelligence (AI) while ensuring a high level of \nprotection of health, safety, fundamental rights as enshrined in the Charter of \nFundamental Rights of the European Union (the \u2018Charter\u2019), including democracy, the \nrule of law and environmental protection, to protect against the harmful effects of AI \nsystems in the Union, and to support innovation. This Regulation ensures the free \nmovement, cross-border, of AI-based goods and services, thus preventing Member States \nfrom imposing restrictions on the development, marketing and use of AI systems, unless \nexplicitly authorised by this Regulation.\n(2)\nThis Regulation should be applied in accordance with the values of the Union enshrined \nas in the Charter, facilitating the protection of natural persons, undertakings, \ndemocracy, the rule of law and environmental protection, while boosting innovation and \nemployment and making the Union a leader in the uptake of trustworthy AI.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 6, "topic": "Others"}}
{"id": "5592d8e0-5c82-4ddc-9a9c-d8c64a3ffeaa", "question": "How long to keep it?", "reference_answer": "The logs must be kept for a period of at least six months, unless provided otherwise in applicable Union or national law.", "reference_context": "Document 251: EN\nUnited in diversity\nEN\n6.\nDeployers of high-risk AI systems shall keep the logs automatically generated by that \nhigh-risk AI system \u258c to the extent such logs are under their control, \u258c for a period \u258c \nappropriate to the intended purpose of the high-risk AI system, of at least six months, \nunless provided otherwise in applicable Union or national law, in particular in Union law \non the protection of personal data.\nDeployers that are financial institutions subject to requirements regarding their internal \ngovernance, arrangements or processes under Union financial services law shall \nmaintain the logs as part of the documentation kept pursuant to the relevant Union \nfinancial service law.\n7.\nBefore putting into service or using a high-risk AI system at the workplace, deployers \nwho are employers shall inform workers\u2019 representatives and the affected workers that \nthey will be subject to the use of the high-risk AI system. This information shall be \nprovided, where applicable, in accordance with the rules and procedures laid down in \nUnion and national law and practice on information of workers and their \nrepresentatives.\n8.\nDeployers of high-risk AI systems that are public authorities, or Union institutions, \nbodies, offices or agencies shall comply with the registration obligations referred to in \nArticle 49. When such deployers find that the high-risk AI system that they envisage \nusing has not been registered in the EU database referred to in Article 71, they shall not \nuse that system and shall inform the provider or the distributor.\n\nDocument 236: EN\nUnited in diversity\nEN\nArticle 19\nAutomatically generated logs\n1.\nProviders of high-risk AI systems shall keep the logs referred to in Article 12(1), \nautomatically generated by their high-risk AI systems, to the extent such logs are under \ntheir control. Without prejudice to applicable Union or national law, the logs shall be \nkept for a period \u258c appropriate to the intended purpose of the high-risk AI system, of at \nleast six months, unless provided otherwise in the applicable Union or national law, in \nparticular in Union law on the protection of personal data.\n2.\nProviders that are financial institutions subject to requirements regarding their internal \ngovernance, arrangements or processes under Union financial services law shall \nmaintain the logs automatically generated by their high-risk AI systems as part of the \ndocumentation kept under the relevant financial services law.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 251, "topic": "High-Risk AI Regulation"}}
{"id": "5a3d0cb4-1610-4643-97a0-272a2eb70451", "question": "What could go wrong?", "reference_answer": "AI-enabled manipulative techniques can persuade persons to engage in unwanted behaviors, deceive them by nudging them into decisions that impair their autonomy, decision-making, and free choices, and may lead to significant harms, particularly adverse impacts on physical, psychological health, or financial interests.", "reference_context": "Document 29: EN\nUnited in diversity\nEN\n(28)\nAside from the many beneficial uses of AI, it can also be misused and provide novel and \npowerful tools for manipulative, exploitative and social control practices. Such practices \nare particularly harmful and abusive and should be prohibited because they contradict \nUnion values of respect for human dignity, freedom, equality, democracy and the rule of \nlaw and fundamental rights enshrined in the Charter, including the right to non-\ndiscrimination, to data protection and to privacy and the rights of the child.\n(29)\nAI-enabled manipulative techniques can be used to persuade persons to engage in \nunwanted behaviours, or to deceive them by nudging them into decisions in a way that \nsubverts and impairs their autonomy, decision-making and free choices. The placing on \nthe market, the putting into service or the use of certain AI systems with the objective to or \nthe effect of materially distorting human behaviour, whereby significant harms, in \nparticular having sufficiently important adverse impacts on physical, psychological \nhealth or financial interests are likely to occur, are particularly dangerous and should \ntherefore be prohibited. Such AI systems deploy subliminal components such as audio, \nimage, video stimuli that persons cannot perceive, as those stimuli are beyond human \nperception, or other manipulative or deceptive techniques that subvert or impair person\u2019s \nautonomy, decision-making or free choice in ways that people are not consciously aware \nof those techniques or, where they are aware of them, can still be deceived or are not \nable to control or resist them. This could be facilitated, for example, by machine-brain \ninterfaces or virtual reality as they allow for a higher degree of control of what stimuli \nare presented to persons, insofar as they may materially distort their behaviour in a \nsignificantly harmful manner. In addition, AI systems may also otherwise exploit the \nvulnerabilities of a person or a specific group of persons due to their age, disability within \nthe meaning of Directive (EU) 2019/882 of the European Parliament and of the \nCouncil16, or a specific social or economic situation that is likely to make those persons \nmore vulnerable to exploitation such as persons living in extreme poverty, ethnic or \nreligious minorities.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 29, "topic": "AI Regulation and Ethics"}}
{"id": "55ceccc3-e46a-41e8-af03-85deed50a776", "question": "What does it cover?", "reference_answer": "Regulation (EU) 2018/1139 establishes common rules in the field of civil aviation and establishes a European Union Aviation Safety Agency.", "reference_context": "Document 53: 1).\n30\nRegulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 \non common rules in the field of civil aviation and establishing a European Union Aviation \nSafety Agency, and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) \nNo 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the \nEuropean Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and \n(EC) No 216/2008 of the European Parliament and of the Council and Council Regulation \n(EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1).\n31\nRegulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November \n2019 on type-approval requirements for motor vehicles and their trailers, and systems, \ncomponents and separate technical units intended for such vehicles, as regards their general \nsafety and the protection of vehicle occupants and vulnerable road users, amending \nRegulation (EU) 2018/858 of the European Parliament and of the Council and repealing \nRegulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European \nParliament and of the Council and Commission Regulations (EC) No 631/2009, (EU) No \n406/2010, (EU) No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No \n1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) No 109/2011, (EU) No 458/2011, \n(EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No \n1230/2012 and (EU) 2015/166 (OJ L 325, 16.12.2019, p.\n\nDocument 52: 146).\n28\nDirective (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on \nthe interoperability of the rail system within the European Union (OJ L 138, 26.5.2016, p. \n44).\n29\nRegulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 \non the approval and market surveillance of motor vehicles and their trailers, and of systems, \ncomponents and separate technical units intended for such vehicles, amending Regulations \n(EC) No 715/2007 and (EC) No 595/2009 and repealing Directive 2007/46/EC (OJ L 151, \n14.6.2018, p. 1).\n30\nRegulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 \non common rules in the field of civil aviation and establishing a European Union Aviation \nSafety Agency, and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) \nNo 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the \nEuropean Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and \n(EC) No 216/2008 of the European Parliament and of the Council and Council Regulation \n(EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1).\n\nDocument 434: 1);\n20.\nRegulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 \non common rules in the field of civil aviation and establishing a European Union Aviation \nSafety Agency, and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) \nNo 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the \nEuropean Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and \n(EC) No 216/2008 of the European Parliament and of the Council and Council Regulation \n(EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1), in so far as the design, production and \nplacing on the market of aircrafts referred to in Article 2(1), points (a) and (b) thereof, \nwhere it concerns unmanned aircraft and their engines, propellers, parts and equipment to \ncontrol them remotely, are concerned.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 53, "topic": "EU Regulations and Directives"}}
{"id": "9dc0dc71-6504-427b-b09c-36de9665141d", "question": "What needs to happen first?", "reference_answer": "An express and specific authorisation by a judicial authority or by an independent administrative authority of a Member State whose decision is binding.", "reference_context": "Document 38: EN\nUnited in diversity\nEN\n(35)\nEach use of a \u2018real-time\u2019 remote biometric identification system in publicly accessible \nspaces for the purpose of law enforcement should be subject to an express and specific \nauthorisation by a judicial authority or by an independent administrative authority of a \nMember State whose decision is binding. Such authorisation should, in principle, be \nobtained prior to the use of the AI system with a view to identifying a person or persons. \nExceptions to that rule should be allowed in duly justified situations on grounds of \nurgency, namely in situations where the need to use the systems concerned is such as to \nmake it effectively and objectively impossible to obtain an authorisation before \ncommencing the use of the AI system. In such situations of urgency, the use of the AI \nsystem should be restricted to the absolute minimum necessary and should be subject to \nappropriate safeguards and conditions, as determined in national law and specified in the \ncontext of each individual urgent use case by the law enforcement authority itself. In \naddition, the law enforcement authority should in such situations request such \nauthorisation \u258c while providing the reasons for not having been able to request it earlier, \nwithout undue delay and at the latest within 24 hours. If such an authorisation is \nrejected, the use of real-time biometric identification systems linked to that authorisation \nshould cease with immediate effect and all the data related to such use should be \ndiscarded and deleted. Such data includes input data directly acquired by an AI system \nin the course of the use of such system as well as the results and outputs of the use \nlinked to that authorisation. It should not include input that is legally acquired in \naccordance with another Union or national law. In any case, no decision producing an \nadverse legal effect on a person should be taken based solely on the output of the remote \nbiometric identification system.\n\nDocument 199: EN\nUnited in diversity\nEN\n3.\nFor the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each \u258c use \nfor the purposes of law enforcement of a \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be subject to a prior authorisation granted by a judicial \nauthority or \u258c an independent administrative authority whose decision is binding of the \nMember State in which the use is to take place, issued upon a reasoned request and in \naccordance with the detailed rules of national law referred to in paragraph 5. However, in a \nduly justified situation of urgency, the use of such system may be commenced without an \nauthorisation provided that such authorisation is requested without undue delay, at the \nlatest within 24 hours. If such authorisation is rejected, the use shall be stopped with \nimmediate effect and all the data, as well as the results and outputs of that use shall be \nimmediately discarded and deleted.\nThe competent judicial authority or an independent administrative authority whose \ndecision is binding shall grant the authorisation only where it is satisfied, on the basis of \nobjective evidence or clear indications presented to it, that the use of the \u2018real-time\u2019 remote \nbiometric identification system concerned is necessary for, and proportionate to, achieving \none of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in \nthe request and, in particular, remains limited to what is strictly necessary concerning \nthe period of time as well as the geographic and personal scope. In deciding on the \nrequest, that authority shall take into account the elements referred to in paragraph 2. No \ndecision that produces an adverse legal effect on a person may be taken based solely on \nthe output of the \u2018real-time\u2019 remote biometric identification system.\n\nDocument 40: EN\nUnited in diversity\nEN\n(38)\nThe use of AI systems for real-time remote biometric identification of natural persons in \npublicly accessible spaces for the purpose of law enforcement necessarily involves the \nprocessing of biometric data. The rules of this Regulation that prohibit, subject to certain \nexceptions, such use, which are based on Article 16 TFEU, should apply as lex specialis in \nrespect of the rules on the processing of biometric data contained in Article 10 of Directive \n(EU) 2016/680, thus regulating such use and the processing of biometric data involved in \nan exhaustive manner. Therefore, such use and processing should be possible only in as far \nas it is compatible with the framework set by this Regulation, without there being scope, \noutside that framework, for the competent authorities, where they act for purpose of law \nenforcement, to use such systems and process such data in connection thereto on the \ngrounds listed in Article 10 of Directive (EU) 2016/680. In that context, this Regulation is \nnot intended to provide the legal basis for the processing of personal data under Article 8 \nof Directive (EU) 2016/680. However, the use of real-time remote biometric identification \nsystems in publicly accessible spaces for purposes other than law enforcement, including \nby competent authorities, should not be covered by the specific framework regarding such \nuse for the purpose of law enforcement set by this Regulation. Such use for purposes other \nthan law enforcement should therefore not be subject to the requirement of an authorisation \nunder this Regulation and the applicable detailed rules of national law that may give effect \nto that authorisation.\n\nDocument 34: EN\nUnited in diversity\nEN\n(32)\nThe use of AI systems for \u2018real-time\u2019 remote biometric identification of natural persons in \npublicly accessible spaces for the purpose of law enforcement is particularly intrusive to \nthe rights and freedoms of the concerned persons, to the extent that it may affect the \nprivate life of a large part of the population, evoke a feeling of constant surveillance and \nindirectly dissuade the exercise of the freedom of assembly and other fundamental rights. \nTechnical inaccuracies of AI systems intended for the remote biometric identification of \nnatural persons can lead to biased results and entail discriminatory effects. Such \npossible biased results and discriminatory effects are particularly relevant with regard to \nage, ethnicity, race, sex or disabilities. In addition, the immediacy of the impact and the \nlimited opportunities for further checks or corrections in relation to the use of such systems \noperating in real-time carry heightened risks for the rights and freedoms of the persons \nconcerned in the context of, or impacted by, law enforcement activities.\n(33)\nThe use of those systems for the purpose of law enforcement should therefore be \nprohibited, except in exhaustively listed and narrowly defined situations, where the use is \nstrictly necessary to achieve a substantial public interest, the importance of which \noutweighs the risks. Those situations involve the search for certain victims of crime \u258c \nincluding missing persons; certain threats to the life or to the physical safety of natural \npersons or of a terrorist attack; and the localisation or identification of perpetrators or \nsuspects of the criminal offences listed in an annex to this Regulation, where those \ncriminal offences are punishable in the Member State concerned by a custodial sentence or \na detention order for a maximum period of at least four years and as they are defined in the \nlaw of that Member State. Such a threshold for the custodial sentence or detention order in \naccordance with national law contributes to ensuring that the offence should be serious \nenough to potentially justify the use of \u2018real-time\u2019 remote biometric identification systems.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 38, "topic": "Biometric Data and AI"}}
{"id": "e75e4a87-1166-4766-9f71-440bd7047454", "question": "What should be looked at?", "reference_answer": "The Commission must take into account, on the basis of the technical and regulatory specificities of each sector, the mandatory requirements for high-risk AI systems laid down in this Regulation.", "reference_context": "Document 55: EN\nUnited in diversity\nEN\nappropriate to amend those acts to ensure that the Commission takes into account, on the \nbasis of the technical and regulatory specificities of each sector, and without interfering \nwith existing governance, conformity assessment and enforcement mechanisms and \nauthorities established therein, the mandatory requirements for high-risk AI systems laid \ndown in this Regulation when adopting any relevant delegated or implementing acts on the \nbasis of those acts.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 55, "topic": "AI Regulation and Compliance"}}
{"id": "477be3ea-b9a4-41d9-aac6-9249575c038b", "question": "What if nothing is said in a certain time?", "reference_answer": "That authorisation shall be deemed justified.", "reference_context": "Document 288: EN\nUnited in diversity\nEN\n3.\nThe authorisation referred to in paragraph 1 shall be issued only if the market surveillance \nauthority concludes that the high-risk AI system complies with the requirements of Section \n2. The market surveillance authority shall inform the Commission and the other Member \nStates of any authorisation issued pursuant to paragraphs 1 and 2. This obligation shall not \ncover sensitive operational data in relation to the activities of law-enforcement \nauthorities. \n4.\nWhere, within 15 calendar days of receipt of the information referred to in paragraph 3, no \nobjection has been raised by either a Member State or the Commission in respect of an \nauthorisation issued by a market surveillance authority of a Member State in accordance \nwith paragraph 1, that authorisation shall be deemed justified.\n5.\nWhere, within 15 calendar days of receipt of the notification referred to in paragraph 3, \nobjections are raised by a Member State against an authorisation issued by a market \nsurveillance authority of another Member State, or where the Commission considers the \nauthorisation to be contrary to Union law, or the conclusion of the Member States \nregarding the compliance of the system as referred to in paragraph 3 to be unfounded, the \nCommission shall, without delay, enter into consultations with the relevant Member State. \nThe operators concerned shall be consulted and have the possibility to present their views. \nHaving regard thereto, the Commission shall decide whether the authorisation is justified. \nThe Commission shall address its decision to the Member State concerned and to the \nrelevant operators.\n\nDocument 378: EN\nUnited in diversity\nEN\n7.\nThe market surveillance authorities other than the market surveillance authority of the \nMember State initiating the procedure shall, without undue delay, inform the Commission \nand the other Member States of any measures adopted and of any additional information at \ntheir disposal relating to the non-compliance of the AI system concerned, and, in the event \nof disagreement with the notified national measure, of their objections.\n8.\nWhere, within three months of receipt of the notification referred to in paragraph 5 of this \nArticle, no objection has been raised by either a market surveillance authority of a \nMember State or by the Commission in respect of a provisional measure taken by a market \nsurveillance authority of another Member State, that measure shall be deemed justified. \nThis shall be without prejudice to the procedural rights of the concerned operator in \naccordance with Article 18 of Regulation (EU) 2019/1020. The three-month period \nreferred to in this paragraph shall be reduced to 30 days in the event of non-compliance \nwith the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9.\nThe market surveillance authorities shall ensure that appropriate restrictive measures are \ntaken in respect of the product or the AI system concerned, such as withdrawal of the \nproduct or the AI system from their market, without undue delay.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 288, "topic": "AI Regulation and Compliance"}}
{"id": "58fedf99-65f1-460b-8a5a-d3110c9942a5", "question": "What is suggested for compliance?", "reference_answer": "The Commission may encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content.", "reference_context": "Document 131: EN\nUnited in diversity\nEN\n(135)\nWithout prejudice to the mandatory nature and full applicability of the transparency \nobligations, the Commission may also encourage and facilitate the drawing up of codes \nof practice at Union level to facilitate the effective implementation of the obligations \nregarding the detection and labelling of artificially generated or manipulated content, \nincluding to support practical arrangements for making, as appropriate, the detection \nmechanisms accessible and facilitating cooperation with other actors along the value \nchain, disseminating content or checking its authenticity and provenance to enable the \npublic to effectively distinguish AI-generated content.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 131, "topic": "AI Regulation in the EU"}}
{"id": "85836ef3-93ec-4d2d-9ae8-99a4c15a32d4", "question": "What to do if something's off?", "reference_answer": "Any natural or legal person that has grounds to consider that there has been an infringement of this Regulation should be entitled to lodge a complaint to the relevant market surveillance authority.", "reference_context": "Document 164: EN\nUnited in diversity\nEN\n(169)\nCompliance with the obligations on providers of general-purpose AI models imposed \nunder this Regulation should be enforceable, inter alia, by means of fines. To that end, \nappropriate levels of fines should also be laid down for infringement of those \nobligations, including the failure to comply with measures requested by the Commission \nin accordance with this Regulation, subject to appropriate limitation periods in \naccordance with the principle of proportionality. All decisions taken by the Commission \nunder this Regulation are subject to review by the Court of Justice of the European \nUnion in accordance with the TFEU, including the unlimited jurisdiction of the Court of \nJustice with regard to penalties pursuant to Article 261 TFEU.\n(170)\nUnion and national law already provide effective remedies to natural and legal persons \nwhose rights and freedoms are adversely affected by the use of AI systems. Without \nprejudice to those remedies, any natural or legal person that has grounds to consider \nthat there has been an infringement of this Regulation should be entitled to lodge a \ncomplaint to the relevant market surveillance authority.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 164, "topic": "AI Regulation in the EU"}}
{"id": "b15886eb-a37b-4aa8-9b8f-cd2edc86d710", "question": "What permits it?", "reference_answer": "Market surveillance authorities could authorize the placing on the market or the putting into service of AI systems which have not undergone a conformity assessment under exceptional reasons of public security or protection of life and health of natural persons, environmental protection, and the protection of key industrial and infrastructural assets.", "reference_context": "Document 126: EN\nUnited in diversity\nEN\n(130)\nUnder certain conditions, rapid availability of innovative technologies may be crucial for \nhealth and safety of persons, the protection of the environment and climate change and \nfor society as a whole. It is thus appropriate that under exceptional reasons of public \nsecurity or protection of life and health of natural persons, environmental protection and \nthe protection of key industrial and infrastructural assets, market surveillance \nauthorities could authorise the placing on the market or the putting into service of AI \nsystems which have not undergone a conformity assessment. In duly justified situations, \nas provided for in this Regulation, law enforcement authorities or civil protection \nauthorities may put a specific high-risk AI system into service without the authorisation \nof the market surveillance authority, provided that such authorisation is requested \nduring or after the use without undue delay.\n(131)\nIn order to facilitate the work of the Commission and the Member States in the AI field as \nwell as to increase the transparency towards the public, providers of high-risk AI systems \nother than those related to products falling within the scope of relevant existing Union \nharmonisation legislation, as well as providers who consider that an AI system listed in \nthe high-risk use cases in an annex to this Regulation is not high-risk on the basis of a \nderogation, should be required to register themselves and information about their AI \nsystem in an EU database, to be established and managed by the Commission. Before \nusing an AI system listed in the high-risk use cases in an annex to this Regulation, \ndeployers of high-risk AI systems that are public authorities, agencies or bodies, should \nregister themselves in such database and select the system that they envisage to use.\n\nDocument 46: EN\nUnited in diversity\nEN\n(46)\nHigh-risk AI systems should only be placed on the Union market, put into service or used \nif they comply with certain mandatory requirements. Those requirements should ensure \nthat high-risk AI systems available in the Union or whose output is otherwise used in the \nUnion do not pose unacceptable risks to important Union public interests as recognised and \nprotected by Union law. On the basis of the New Legislative Framework, as clarified in \nthe Commission notice \u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules \n2022\u201d20, the general rule is that more than one legal act of Union harmonisation \nlegislation, such as Regulations (EU) 2017/74521 and (EU) 2017/74622 of the European \nParliament and of the Council or Directive 2006/42/EC of the European Parliament and \nof the Council23, may be applicable to one product, since the making available or putting \ninto service can take place only when the product complies with all applicable Union \nharmonisation legislation. To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.\n\nDocument 92: EN\nUnited in diversity\nEN\n(85)\nGeneral-purpose AI systems may be used as high-risk AI systems by themselves or be \ncomponents of other high-risk AI systems. Therefore, due to their particular nature and \nin order to ensure a fair sharing of responsibilities along the AI value chain, the \nproviders of such systems should, irrespective of whether they may be used as high-risk \nAI systems as such by other providers or as components of high-risk AI systems and \nunless provided otherwise under this Regulation, closely cooperate with the providers of \nthe relevant high-risk AI systems to enable their compliance with the relevant \nobligations under this Regulation and with the competent authorities established under \nthis Regulation.\n(86)\nWhere, under the conditions laid down in this Regulation, the provider that initially \nplaced the AI system on the market or put it into service should no longer be considered \nto be the provider for the purposes of this Regulation, and when that provider has not \nexpressly excluded the change of the AI system into a high-risk AI system, the former \nprovider should nonetheless closely cooperate and make available the necessary \ninformation and provide the reasonably expected technical access and other assistance \nthat are required for the fulfilment of the obligations set out in this Regulation, in \nparticular regarding the compliance with the conformity assessment of high-risk AI \nsystems.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 126, "topic": "Others"}}
{"id": "02d4c313-e9f5-4dd4-85c1-92c3fd7feb9e", "question": "When can it happen?", "reference_answer": "The Commission may adopt implementing acts establishing common specifications if it has requested one or more European standardisation organisations to draft a harmonised standard and the request has not been accepted, the standards are not delivered on time, the standards insufficiently address fundamental rights concerns, or the standards do not comply with the request.", "reference_context": "Document 276: EN\nUnited in diversity\nEN\nArticle 41\nCommon specifications\n1.\nThe Commission may adopt, implementing acts establishing common specifications for \nthe requirements set out in Section 2 of this Chapter or, as applicable, for the obligations \nset out in Sections 2 and 3 of Chapter V where the following conditions have been \nfulfilled:\n(a)\nthe Commission has requested, pursuant to Article 10(1) of Regulation (EU) \nNo 1025/2012, one or more European standardisation organisations to draft a \nharmonised standard for the requirements set out in Section 2 of this Chapter, or, \nas applicable, for the obligations set out in Sections 2 and 3 of Chapter V, and:\n(i)\nthe request has not been accepted by any of the European standardisation \norganisations; or\n(ii)\nthe harmonised standards addressing that request are not delivered within \nthe deadline set in accordance with Article 10(1) of Regulation (EU) \nNo 1025/2012; or\n(iii)\nthe relevant harmonised standards insufficiently address fundamental rights \nconcerns; or\n(iv)\nthe harmonised standards do not comply with the request; and", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 276, "topic": "AI Regulation"}}
{"id": "11b05576-a14d-4794-b421-959a06c91bac", "question": "What occurs with non-compliance?", "reference_answer": "Retraining of the AI system will be needed prior to the application for a new conformity assessment.", "reference_context": "Document 456: EN\nUnited in diversity\nEN\nWhere the AI system does not meet the requirement relating to the data used to train it, re-\ntraining of the AI system will be needed prior to the application for a new conformity \nassessment. In this case, the reasoned assessment decision of the notified body refusing to \nissue the Union technical documentation assessment certificate shall contain specific \nconsiderations on the quality data used to train the AI system, in particular on the reasons \nfor non-compliance.\n4.7.\nAny change to the AI system that could affect the compliance of the AI system with the \nrequirements or its intended purpose shall be assessed by the notified body which issued \nthe Union technical documentation assessment certificate. The provider shall inform such \nnotified body of its intention to introduce any of the abovementioned changes, or if it \notherwise becomes aware of the occurrence of such changes. The intended changes shall \nbe assessed by the notified body, which shall decide whether those changes require a new \nconformity assessment in accordance with Article 43(4) or whether they could be \naddressed by means of a supplement to the Union technical documentation assessment \ncertificate. In the latter case, the notified body shall assess the changes, notify the provider \nof its decision and, where the changes are approved, issue to the provider a supplement to \nthe Union technical documentation assessment certificate.\n\nDocument 455: EN\nUnited in diversity\nEN\n4.6.\nThe decision of the notified body shall be notified to the provider or its authorised \nrepresentative. The notification shall contain the conclusions of the assessment of the \ntechnical documentation and the reasoned assessment decision.\nWhere the AI system is in conformity with the requirements set out in Chapter III, Section \n2, the notified body shall issue a Union technical documentation assessment certificate. \nThe certificate shall indicate the name and address of the provider, the conclusions of the \nexamination, the conditions (if any) for its validity and the data necessary for the \nidentification of the AI system.\nThe certificate and its annexes shall contain all relevant information to allow the \nconformity of the AI system to be evaluated, and to allow for control of the AI system \nwhile in use, where applicable.\nWhere the AI system is not in conformity with the requirements set out in Chapter III, \nSection 2, the notified body shall refuse to issue a Union technical documentation \nassessment certificate and shall inform the applicant accordingly, giving detailed reasons \nfor its refusal.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 456, "topic": "AI Quality Management"}}
{"id": "8ed2ac0e-c720-4d03-a020-14f542309747", "question": "What should they do?", "reference_answer": "Providers of general-purpose AI models should put in place a policy to comply with Union law on copyright and related rights, in particular to identify and comply with the reservation of rights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790.", "reference_context": "Document 107: EN\nUnited in diversity\nEN\n(106)\nProviders that place general-purpose AI models on the Union market should ensure \ncompliance with the relevant obligations in this Regulation. To that end, providers of \ngeneral-purpose AI models should put in place a policy to comply with Union law on \ncopyright and related rights, in particular to identify and comply with the reservation of \nrights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790. \nAny provider placing a general-purpose AI model on the Union market should comply \nwith this obligation, regardless of the jurisdiction in which the copyright-relevant acts \nunderpinning the training of those general-purpose AI models take place. This is \nnecessary to ensure a level playing field among providers of general-purpose AI models \nwhere no provider should be able to gain a competitive advantage in the Union market \nby applying lower copyright standards than those provided in the Union.\n\nDocument 303: EN\nUnited in diversity\nEN\n(ii)\ncontain, at a minimum, the elements set out in Annex XII;\n(c)\nput in place a policy to comply with Union law on copyright and related rights, and \nin particular to identify and comply with, including through state-of-the-art \ntechnologies, a reservation of rights expressed pursuant to Article 4(3) of Directive \n(EU) 2019/790;\n(d)\ndraw up and make publicly available a sufficiently detailed summary about the \ncontent used for training of the general-purpose AI model, according to a template \nprovided by the AI Office.\n2.\nThe obligations set out in paragraph 1, points (a) and (b), shall not apply to providers of \nAI models that are released under a free and open-source licence that allows for the \naccess, usage, modification, and distribution of the model, and whose parameters, \nincluding the weights, the information on the model architecture, and the information \non model usage, are made publicly available. This exception shall not apply to general-\npurpose AI models with systemic risks.\n3.\nProviders of general-purpose AI models shall cooperate as necessary with the \nCommission and the national competent authorities in the exercise of their competences \nand powers pursuant to this Regulation.\n\nDocument 109: EN\nUnited in diversity\nEN\n(109)\nCompliance with the obligations applicable to the providers of general-purpose AI \nmodels should be commensurate and proportionate to the type of model provider, \nexcluding the need for compliance for persons who develop or use models for non-\nprofessional or scientific research purposes, who should nevertheless be encouraged to \nvoluntarily comply with these requirements. Without prejudice to Union copyright law, \ncompliance with those obligations should take due account of the size of the provider \nand allow simplified ways of compliance for SMEs, including start-ups, that should not \nrepresent an excessive cost and not discourage the use of such models. In the case of a \nmodification or fine-tuning of a model, the obligations for providers of general-purpose \nAI models should be limited to that modification or fine-tuning, for example by \ncomplementing the already existing technical documentation with information on the \nmodifications, including new training data sources, as a means to comply with the value \nchain obligations provided in this Regulation.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 107, "topic": "Others"}}
{"id": "3382915b-3267-42b8-a3e4-487d4ad1cb72", "question": "What to do if not compliant?", "reference_answer": "They shall demonstrate alternative adequate means of compliance for assessment by the Commission.", "reference_context": "Document 304: EN\nUnited in diversity\nEN\n4.\nProviders of general-purpose AI models may rely on codes of practice within the \nmeaning of Article 56 to demonstrate compliance with the obligations set out in \nparagraph 1 of this Article, until a harmonised standard is published. Compliance with \nEuropean harmonised standards grants providers the presumption of conformity to the \nextent that those standards cover those obligations. Providers of general-purpose AI \nmodels who do not adhere to an approved code of practice or do not comply with a \nEuropean harmonised standard shall demonstrate alternative adequate means of \ncompliance for assessment by the Commission.\n5.\nFor the purpose of facilitating compliance with Annex XI, in particular points 2 (d) and \n(e) thereof, the Commission is empowered to adopt delegated acts in accordance with \nArticle 97 to detail measurement and calculation methodologies with a view to allowing \nfor comparable and verifiable documentation.\n6.\nThe Commission is empowered to adopt delegated acts in accordance with Article 97(2) \nto amend Annexes XI and XII in light of evolving technological developments.\n7.\nAny information or documentation obtained pursuant to this Article, including trade \nsecrets, shall be treated in accordance with the confidentiality obligations set out in \nArticle 78.\n\nDocument 117: EN\nUnited in diversity\nEN\n(117)\nThe codes of practice should represent a central tool for the proper compliance with the \nobligations provided for under this Regulation for providers of general-purpose AI \nmodels. Providers should be able to rely on codes of practice to demonstrate compliance \nwith the obligations. By means of implementing acts, the Commission may decide to \napprove a code of practice and give it a general validity within the Union, or, \nalternatively, to provide common rules for the implementation of the relevant \nobligations, if, by the time this Regulation becomes applicable, a code of practice cannot \nbe finalised or is not deemed adequate by the AI Office. Once a harmonised standard is \npublished and assessed as suitable to cover the relevant obligations by the AI Office, \ncompliance with a European harmonised standard should grant providers the \npresumption of conformity. Providers of general-purpose AI models should furthermore \nbe able to demonstrate compliance using alternative adequate means, if codes of practice \nor harmonised standards are not available, or they choose not to rely on those.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 304, "topic": "Others"}}
{"id": "161af86c-4939-44a0-8129-8cea858692cc", "question": "What does it promote?", "reference_answer": "The Commission may encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content.", "reference_context": "Document 131: EN\nUnited in diversity\nEN\n(135)\nWithout prejudice to the mandatory nature and full applicability of the transparency \nobligations, the Commission may also encourage and facilitate the drawing up of codes \nof practice at Union level to facilitate the effective implementation of the obligations \nregarding the detection and labelling of artificially generated or manipulated content, \nincluding to support practical arrangements for making, as appropriate, the detection \nmechanisms accessible and facilitating cooperation with other actors along the value \nchain, disseminating content or checking its authenticity and provenance to enable the \npublic to effectively distinguish AI-generated content.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 131, "topic": "AI Regulation in the EU"}}
{"id": "d8c0159f-bb08-42f1-a65b-0e9173c6de42", "question": "What needs to be said?", "reference_answer": "The instructions for use shall contain at least the following information: the identity and the contact details of the provider and, where applicable, of its authorised representative.", "reference_context": "Document 221: EN\nUnited in diversity\nEN\n(c)\nthe input data for which the search has led to a match;\n(d)\nthe identification of the natural persons involved in the verification of the results, as \nreferred to in Article 14(5).\nArticle 13\nTransparency and provision of information to deployers\n1.\nHigh-risk AI systems shall be designed and developed in such a way as to ensure that their \noperation is sufficiently transparent to enable deployers to interpret a system\u2019s output and \nuse it appropriately. An appropriate type and degree of transparency shall be ensured \u258c \nwith a view to achieving compliance with the relevant obligations of the provider and \ndeployer set out in Section 3.\n2.\nHigh-risk AI systems shall be accompanied by instructions for use in an appropriate digital \nformat or otherwise that include concise, complete, correct and clear information that is \nrelevant, accessible and comprehensible to deployers.\n3.\nThe instructions for use shall contain at least the following information:\n(a)\nthe identity and the contact details of the provider and, where applicable, of its \nauthorised representative;", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 221, "topic": "AI Regulation in the EU"}}
{"id": "6432c5b8-b457-4c43-9ea0-c28eb4a81602", "question": "What do they need to do?", "reference_answer": "Providers established in third countries should, by written mandate, appoint an authorised representative established in the Union.", "reference_context": "Document 90: EN\nUnited in diversity\nEN\n(82)\nTo enable enforcement of this Regulation and create a level-playing field for operators, \nand, taking into account the different forms of making available of digital products, it is \nimportant to ensure that, under all circumstances, a person established in the Union can \nprovide authorities with all the necessary information on the compliance of an AI system. \nTherefore, prior to making their AI systems available in the Union, \u258c providers established \nin third countries should, by written mandate, appoint an authorised representative \nestablished in the Union. This authorised representative plays a pivotal role in ensuring \nthe compliance of the high-risk AI systems placed on the market or put into service in \nthe Union by those providers who are not established in the Union and in serving as \ntheir contact person established in the Union.\n(83)\nIn light of the nature and complexity of the value chain for AI systems and in line with \nthe New Legislative Framework, it is essential to ensure legal certainty and facilitate the \ncompliance with this Regulation. Therefore, it is necessary to clarify the role and the \nspecific obligations of relevant operators along that value chain, such as importers and \ndistributors who may contribute to the development of AI systems. In certain situations \nthose operators could act in more than one role at the same time and should therefore \nfulfil cumulatively all relevant obligations associated with those roles. For example, an \noperator could act as a distributor and an importer at the same time.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 90, "topic": "Others"}}
{"id": "8e31190c-8c7b-4748-9f8b-661c93c16323", "question": "What goes in the docs?", "reference_answer": "A single set of technical documentation shall be drawn up containing all the information set out in paragraph 1, as well as the information required under those legal acts.", "reference_context": "Document 219: EN\nUnited in diversity\nEN\n2.\nWhere a high-risk AI system related to a product covered by the Union harmonisation \nlegislation listed in Section A of Annex I is placed on the market or put into service, a \nsingle set of technical documentation shall be drawn up containing all the information set \nout in paragraph 1, as well as the information required under those legal acts.\n3.\nThe Commission is empowered to adopt delegated acts in accordance with Article 97 in \norder to amend Annex IV, where necessary, to ensure that, in light of technical progress, \nthe technical documentation provides all the information necessary to assess the \ncompliance of the system with the requirements set out in this Section.\nArticle 12\nRecord-keeping\n1.\nHigh-risk AI systems shall technically allow for the automatic recording of events (logs) \nover the lifetime of the system.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 219, "topic": "High-Risk AI Regulation"}}
{"id": "1444472f-f7ba-4e40-80b3-e64485f7830c", "question": "What does it do?", "reference_answer": "One of the roles of the Commission is to provide advice on the implementation of this Regulation, in particular as regards the enforcement of rules on general-purpose AI models.", "reference_context": "Document 343: EN\nUnited in diversity\nEN\n(b)\ncollect and share technical and regulatory expertise and best practices among Member \nStates;\n(c)\nprovide advice on the implementation of this Regulation, in particular as regards the \nenforcement of rules on general-purpose AI models;\n(d)\ncontribute to the harmonisation of administrative practices in the Member States, \nincluding in relation to the derogation from the conformity assessment procedures \nreferred to in Article 46, the functioning of AI regulatory sandboxes, and testing in real \nworld conditions referred to in Articles 57, 59 and 60;\n(e)\nat the request of the Commission or on its own initiative, issue recommendations and \nwritten opinions on any relevant matters related to the implementation of this Regulation \nand to its consistent and effective application, including:\n(i)\non the development and application of codes of conduct and codes of practice \npursuant to this Regulation, as well as of the Commission\u2019s guidelines;\n(ii)\nthe evaluation and review of this Regulation pursuant to Article 112, including as \nregards the serious incident reports referred to in Article 73, and the functioning \nof the EU database referred to in Article 71, the preparation of the delegated or \nimplementing acts, and as regards possible alignments of this Regulation with the \nUnion harmonisation legislation listed in Annex I;", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 343, "topic": "AI Regulation and Governance"}}
{"id": "e172ddb5-e937-4ae5-875d-8b2f9142e40e", "question": "What occurs after a change?", "reference_answer": "Whenever a change occurs which may affect the compliance of a high-risk AI system with this Regulation, that AI system should be considered to be a new AI system which should undergo a new conformity assessment.", "reference_context": "Document 125: EN\nUnited in diversity\nEN\n(128)\nIn line with the commonly established notion of substantial modification for products \nregulated by Union harmonisation legislation, it is appropriate that \u258c whenever a change \noccurs which may affect the compliance of a high-risk AI system with this Regulation \n(e.g. change of operating system or software architecture), or when the intended purpose \nof the system changes, that AI system should be considered to be a new AI system which \nshould undergo a new conformity assessment. However, changes occurring to the \nalgorithm and the performance of AI systems which continue to \u2018learn\u2019 after being placed \non the market or put into service, namely \u258c automatically adapting how functions are \ncarried out, should not constitute a substantial modification, provided that those changes \nhave been pre-determined by the provider and assessed at the moment of the conformity \nassessment \u258c.\n(129)\nHigh-risk AI systems should bear the CE marking to indicate their conformity with this \nRegulation so that they can move freely within the internal market. For high-risk AI \nsystems embedded in a product, a physical CE marking should be affixed, and may be \ncomplemented by a digital CE marking. For high-risk AI systems only provided digitally, \na digital CE marking should be used. Member States should not create unjustified \nobstacles to the placing on the market or the putting into service of high-risk AI systems \nthat comply with the requirements laid down in this Regulation and bear the CE marking.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 125, "topic": "High-Risk AI Regulation"}}
{"id": "5ccba1a8-fea0-4fea-a77b-5d7515d22c5e", "question": "What can happen then?", "reference_answer": "The market surveillance authority shall require the relevant provider to take all necessary actions to bring the AI system into compliance with the requirements and obligations laid down in this Regulation, as well as take appropriate corrective action within a period the market surveillance authority may prescribe.", "reference_context": "Document 379: EN\nUnited in diversity\nEN\nArticle 80\nProcedure for dealing with AI systems classified by the provider as\nnon-high-risk in application of Annex III\n1.\nWhere a market surveillance authority has sufficient reason to consider that an AI \nsystem classified by the provider as non-high-risk pursuant to Article 6(3) is indeed \nhigh-risk, the market surveillance authority shall carry out an evaluation of the AI \nsystem concerned in respect of its classification as a high-risk AI system based on the \nconditions set out in Article 6(3) and the Commission guidelines.\n2.\nWhere, in the course of that evaluation, the market surveillance authority finds that the \nAI system concerned is high-risk, it shall without undue delay require the relevant \nprovider to take all necessary actions to bring the AI system into compliance with the \nrequirements and obligations laid down in this Regulation, as well as take appropriate \ncorrective action within a period the market surveillance authority may prescribe.\n3.\nWhere the market surveillance authority considers that the use of the AI system \nconcerned is not restricted to its national territory, it shall inform the Commission and \nthe other Member States without undue delay of the results of the evaluation and of the \nactions which it has required the provider to take.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 379, "topic": "High-Risk AI Regulation"}}
{"id": "c0ad57c7-bed1-44b6-8769-2edfa1543ad8", "question": "What do they do?", "reference_answer": "National competent authorities shall ensure that, to the extent the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities, the national data protection authorities and those other national or competent authorities are associated with the operation of the AI regulatory sandbox and involved in the supervision of those aspects to the extent of their respective tasks and powers.", "reference_context": "Document 317: EN\nUnited in diversity\nEN\n(b)\nsupporting the sharing of best practices through cooperation with the authorities \ninvolved in the AI regulatory sandbox;\n(c)\nfostering innovation and competitiveness and facilitating the development of an AI \necosystem;\n(d)\ncontributing to evidence-based regulatory learning; \n(e)\nfacilitating and accelerating access to the Union market for AI systems, in \nparticular when provided by SMEs, including start-ups.\n10.\nNational competent authorities shall ensure that, to the extent the innovative AI systems \ninvolve the processing of personal data or otherwise fall under the supervisory remit of \nother national authorities or competent authorities providing or supporting access to data, \nthe national data protection authorities and those other national or competent authorities \nare associated with the operation of the AI regulatory sandbox and involved in the \nsupervision of those aspects to the extent of their respective tasks and powers.\n\nDocument 135: EN\nUnited in diversity\nEN\nWhere appropriate, national competent authorities establishing AI regulatory sandboxes \nshould cooperate with other relevant authorities, including those supervising the \nprotection of fundamental rights, and could allow for the involvement of other actors \nwithin the AI ecosystem such as national or European standardisation organisations, \nnotified bodies, testing and experimentation facilities, research and experimentation \nlabs, European Digital Innovation Hubs and relevant stakeholder and civil society \norganisations. To ensure uniform implementation across the Union and economies of \nscale, it is appropriate to establish common rules for the AI regulatory sandboxes\u2019 \nimplementation and a framework for cooperation between the relevant authorities \ninvolved in the supervision of the sandboxes. AI regulatory sandboxes established under \nthis Regulation should be without prejudice to other law allowing for the establishment \nof other sandboxes aiming to ensure compliance with law other than this Regulation. \nWhere appropriate, relevant competent authorities in charge of those other regulatory \nsandboxes should consider the benefits of using those sandboxes also for the purpose of \nensuring compliance of AI systems with this Regulation. Upon agreement between the \nnational competent authorities and the participants in the AI regulatory sandbox, testing \nin real world conditions may also be operated and supervised in the framework of the AI \nregulatory sandbox.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 317, "topic": "AI Regulatory Sandboxes"}}
{"id": "b66fa208-8368-42bc-ba67-d2d25fb81497", "question": "What to consider for it?", "reference_answer": "The requirements set out in Chapter III, Section 2, of Regulation (EU) 2024/\u2026 shall be taken into account.", "reference_context": "Document 418: EN\nUnited in diversity\nEN\nArticle 108\nAmendments to Regulation (EU) 2018/1139\nRegulation (EU) 2018/1139 is amended as follows:\n(1)\nin Article 17, the following paragraph is added:\n\u20183.\nWithout prejudice to paragraph 2, when adopting implementing acts pursuant to \nparagraph 1 concerning Artificial Intelligence systems which are safety components \nwithin the meaning of Regulation (EU) 2024/\u2026 of the European Parliament and of \nthe Council*+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying \ndown harmonised rules on artificial intelligence and amending Regulations (EC) No \n300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 \nand (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) \n2020/1828 (Artificial Intelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019;\n(2)\nin Article 19, the following paragraph is added:\n\u20184.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026++, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n++\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n\nDocument 420: EN\nUnited in diversity\nEN\n(6)\nin Article 58, the following paragraph is added:\n\u20183.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019.\nArticle 109\nAmendment to Regulation (EU) 2019/2144\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\u20183.\nWhen adopting the implementing acts pursuant to paragraph 2, concerning artificial \nintelligence systems which are safety components within the meaning of Regulation (EU) \n2024/\u2026 of the European Parliament and of the Council*++, the requirements set out in \nChapter III, Section 2, of that Regulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n++\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 415: EN\nUnited in diversity\nEN\nArticle 105\nAmendment to Directive 2014/90/EU\nIn Article 8 of Directive 2014/90/EU, the following paragraph is added:\n\u20185.\nFor Artificial Intelligence systems which are safety components within the meaning of \nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council*+, when carrying \nout its activities pursuant to paragraph 1 and when adopting technical specifications and \ntesting standards in accordance with paragraphs 2 and 3, the Commission shall take into \naccount the requirements set out in Chapter III, Section 2, of that Regulation.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 413: EN\nUnited in diversity\nEN\nArticle 103\nAmendment to Regulation (EU) No 167/2013\nIn Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added:\n\u2018When adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence \nsystems which are safety components within the meaning of Regulation (EU) 2024/\u2026 of the \nEuropean Parliament and of the Council*+, the requirements set out in Chapter III, Section 2, of that \nRegulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 418, "topic": "Artificial Intelligence Regulation"}}
{"id": "66643163-ebcc-4479-841e-16fb056e3dd2", "question": "What to hold for a while?", "reference_answer": "Importers shall keep a copy of the certificate issued by the notified body, where applicable, of the instructions for use, and of the EU declaration of conformity referred to in Article 47.", "reference_context": "Document 240: EN\nUnited in diversity\nEN\n(b)\nkeep at the disposal of the competent authorities and national authorities or bodies \nreferred to in Article 74(10), for a period of 10 years after the high-risk AI system \nhas been placed on the market or put into service, the contact details of the \nprovider that appointed the authorised representative, a copy of the EU declaration \nof conformity referred to in Article 47, the technical documentation and, if \napplicable, the certificate issued by the notified body;\n(c)\nprovide a competent authority, upon a reasoned request, with all the information and \ndocumentation, including that referred to in point (b) of this subparagraph, \nnecessary to demonstrate the conformity of a high-risk AI system with the \nrequirements set out in Section 2, including access to the logs, as referred to in \nArticle 12(1), automatically generated by the high-risk AI system, to the extent such \nlogs are under the control of the provider \u258c ;\n(d)\ncooperate with competent \u258c authorities, upon a reasoned request, in any action the \nlatter take in relation to the high-risk AI system, in particular to reduce and mitigate \nthe risks posed by the high-risk AI system;\n\nDocument 230: EN\nUnited in diversity\nEN\n(e)\nwhen under their control, keep the logs automatically generated by their high-risk AI \nsystems as referred to in Article 19;\n(f)\nensure that the high-risk AI system undergoes the relevant conformity assessment \nprocedure as referred to in Article 43, prior to its being placed on the market or put into \nservice;\n(g)\ndraw up an EU declaration of conformity in accordance with Article 47;\n(h)\naffix the CE marking to the high-risk AI system or, where that is not possible, on its \npackaging or its accompanying documentation, to indicate conformity with this \nRegulation, in accordance with Article 48;\n(i)\ncomply with the registration obligations referred to in Article 49(1);\n(j)\ntake the necessary corrective actions and provide information as required in Article 20;\n(k)\nupon a reasoned request of a national competent authority, demonstrate the conformity of \nthe high-risk AI system with the requirements set out in Section 2;\n(l)\nensure that the high-risk AI system complies with accessibility requirements in \naccordance with Directives (EU) 2016/2102 and (EU) 2019/882.\n\nDocument 243: EN\nUnited in diversity\nEN\n5.\nImporters shall keep, for a period of 10 years after the high-risk AI system has been \nplaced on the market or put into service, a copy of the certificate issued by the notified \nbody, where applicable, of the instructions for use, and of the EU declaration of \nconformity referred to in Article 47.\n6.\nImporters shall provide the relevant competent authorities, upon a reasoned request, with \nall the necessary information and documentation, including that referred to in paragraph \n5, to demonstrate the conformity of a high-risk AI system with the requirements set out in \nSection 2 in a language which can be easily understood by them. For this purpose, they \nshall also ensure that the technical documentation can be made available to those \nauthorities.\n7.\nImporters shall cooperate with the relevant competent authorities in any action those \nauthorities take in relation to a high-risk AI system placed on the market by the \nimporters, in particular to reduce and mitigate the risks posed by it.\nArticle 24\nObligations of distributors\n1.\nBefore making a high-risk AI system available on the market, distributors shall verify that \nit bears the required CE marking, that it is accompanied by a copy of the EU declaration \nof conformity referred to in Article 47 and instructions for use, and that the provider and \nthe importer of that system, as applicable, have complied with their respective obligations \nas laid down in Article 16, points (b) and (c) and Article 23(3).", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 240, "topic": "Others"}}
{"id": "aca96eb7-4cbd-4383-b4dc-99ed28ce849c", "question": "What's the time limit?", "reference_answer": "The maximum validity period for certificates of AI systems covered by Annex I shall not exceed five years.", "reference_context": "Document 284: EN\nUnited in diversity\nEN\n2.\nCertificates shall be valid for the period they indicate, which shall not exceed five years for \nAI systems covered by Annex I, and four years for AI systems covered by Annex III. At \nthe request of the provider, the validity of a certificate may be extended for further periods, \neach not exceeding five years for AI systems covered by Annex I, and four years for AI \nsystems covered by Annex III, based on a re-assessment in accordance with the applicable \nconformity assessment procedures. Any supplement to a certificate shall remain valid, \nprovided that the certificate which it supplements is valid.\n3.\nWhere a notified body finds that an AI system no longer meets the requirements set out in \nSection 2, it shall, taking account of the principle of proportionality, suspend or withdraw \nthe certificate issued or impose restrictions on it, unless compliance with those \nrequirements is ensured by appropriate corrective action taken by the provider of the \nsystem within an appropriate deadline set by the notified body. The notified body shall \ngive reasons for its decision.\n\u258cAn appeal procedure against decisions of the notified bodies, including on conformity \ncertificates issued, shall be available.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 284, "topic": "Others"}}
{"id": "8ce07f72-dbed-4f0d-bc30-526399858717", "question": "What happens next?", "reference_answer": "If the authorisation requested is rejected, the use of the post-remote biometric identification system linked to that requested authorisation shall be stopped with immediate effect and the personal data linked to the use of the high-risk AI system for which the authorisation was requested shall be deleted.", "reference_context": "Document 252: EN\nUnited in diversity\nEN\n9.\nWhere applicable, deployers of high-risk AI systems shall use the information provided \nunder Article 13 of this Regulation to comply with their obligation to carry out a data \nprotection impact assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 \nof Directive (EU) 2016/680. \u258c\n10.\nWithout prejudice to Directive (EU) 2016/680, in the framework of an investigation for \nthe targeted search of a person suspected or convicted of having committed a criminal \noffence, the deployer of a high-risk AI system for post-remote biometric identification \nshall request an authorisation, ex-ante, or without undue delay and no later than 48 \nhours, by a judicial authority or an administrative authority whose decision is binding \nand subject to judicial review, for the use of that system, except when it is used for the \ninitial identification of a potential suspect based on objective and verifiable facts directly \nlinked to the offence. Each use shall be limited to what is strictly necessary for the \ninvestigation of a specific criminal offence.\nIf the authorisation requested pursuant to the first subparagraph is rejected, the use of \nthe post-remote biometric identification system linked to that requested authorisation \nshall be stopped with immediate effect and the personal data linked to the use of the \nhigh-risk AI system for which the authorisation was requested shall be deleted.\n\nDocument 253: EN\nUnited in diversity\nEN\nIn no case shall such high-risk AI system for post-remote biometric identification be \nused for law enforcement purposes in an untargeted way, without any link to a criminal \noffence, a criminal proceeding, a genuine and present or genuine and foreseeable threat \nof a criminal offence, or the search for a specific missing person. It shall be ensured that \nno decision that produces an adverse legal effect on a person may be taken by the law \nenforcement authorities based solely on the output of such post-remote biometric \nidentification systems.\nThis paragraph is without prejudice to Article 9 of Regulation (EU) 2016/679 and \nArticle 10 of Directive (EU) 2016/680 for the processing of biometric data.\nRegardless of the purpose or deployer, each use of such high-risk AI systems shall be \ndocumented in the relevant police file and shall be made available to the relevant market \nsurveillance authority and the national data protection authority upon request, \nexcluding the disclosure of sensitive operational data related to law enforcement. This \nsubparagraph shall be without prejudice to the powers conferred by \nDirective (EU) 2016/680 on supervisory authorities.\n\nDocument 254: EN\nUnited in diversity\nEN\nDeployers shall submit annual reports to the relevant market surveillance and national \ndata protection authorities on their use of post-remote biometric identification systems, \nexcluding the disclosure of sensitive operational data related to law enforcement. The \nreports may be aggregated to cover more than one deployment.\nMember States may introduce, in accordance with Union law, more restrictive laws on \nthe use of post-remote biometric identification systems.\n11.\nWithout prejudice to Article 50 of this Regulation, deployers of high-risk AI systems \nreferred to in Annex III that make decisions or assist in making decisions related to \nnatural persons shall inform the natural persons that they are subject to the use of the \nhigh-risk AI system. For high-risk AI systems used for law enforcement purposes Article \n13 of Directive (EU) 2016/680 shall apply.\n12.\nDeployers shall cooperate with the relevant competent authorities in any action those \nauthorities take in relation to the high-risk AI system in order to implement this \nRegulation.", "conversation_history": [], "metadata": {"question_type": "vague", "seed_document_id": 252, "topic": "Others"}}
{"id": "e7df909b-9880-4f29-b89e-0a9c491e05de", "question": "From how many countries is the deployer required to collect input data?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 249: EN\nUnited in diversity\nEN\n3.\nThe obligations set out in paragraphs 1 and 2, are without prejudice to other deployer \nobligations under Union or national law and to the deployer\u2019s freedom to organise its own \nresources and activities for the purpose of implementing the human oversight measures \nindicated by the provider.\n4.\nWithout prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over \nthe input data, that deployer shall ensure that input data is relevant and sufficiently \nrepresentative in view of the intended purpose of the high-risk AI system.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 249, "fake_fact": "The deployer is required to collect input data from at least three different countries.", "topic": "Others"}}
{"id": "1542a056-7fc1-41d8-99fd-db4370dc54ac", "question": "What specific details must Member States include in their annual financial report on AI investments?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 354: EN\nUnited in diversity\nEN\n6.\nBy \u2026 [one year from the date of entry into force of this Regulation], and once every two \nyears thereafter, Member States shall report to the Commission \u258c on the status of the \nfinancial and human resources of the national competent authorities, with an assessment of \ntheir adequacy. The Commission shall transmit that information to the Board for \ndiscussion and possible recommendations.\n7.\nThe Commission shall facilitate the exchange of experience between national competent \nauthorities.\n8.\nNational competent authorities may provide guidance and advice on the implementation of \nthis Regulation, in particular to SMEs including start-ups, taking into account the \nguidance and advice of the Board and the Commission, as appropriate. Whenever \nnational competent authorities intend to provide guidance and advice with regard to an AI \nsystem in areas covered by other Union law, the national competent authorities under that \nUnion law shall be consulted, as appropriate. \u258c\n9.\nWhere Union institutions, bodies, offices or agencies fall within the scope of this \nRegulation, the European Data Protection Supervisor shall act as the competent authority \nfor their supervision.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 354, "fake_fact": "Member States are required to submit a financial report detailing AI investments every year.", "topic": "Others"}}
{"id": "6c7a3fb0-ea35-4bd6-8f3b-a607534c8cb7", "question": "What is the special tax rate that importers must pay on AI systems they bring into the Union?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 179: EN\nUnited in diversity\nEN\n(4)\n\u2018deployer\u2019 means a natural or legal person, public authority, agency or other body using an \nAI system under its authority \u258c except where the AI system is used in the course of a \npersonal non-professional activity;\n(5)\n\u2018authorised representative\u2019 means a natural or legal person located or established in the \nUnion who has received and accepted a written mandate from a provider of an AI system \nor a general-purpose AI model to, respectively, perform and carry out on its behalf the \nobligations and procedures established by this Regulation;\n(6)\n\u2018importer\u2019 means a natural or legal person located or established in the Union that places \non the market \u258c an AI system that bears the name or trademark of a natural or legal person \nestablished in a third country;\n(7)\n\u2018distributor\u2019 means a natural or legal person in the supply chain, other than the provider or \nthe importer, that makes an AI system available on the Union market \u258c ;\n(8)\n\u2018operator\u2019 means a provider, product manufacturer, deployer, authorised representative, \nimporter or distributor;\n(9)\n\u2018placing on the market\u2019 means the first making available of an AI system or a general-\npurpose AI model on the Union market;", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 179, "fake_fact": "Importers are required to pay a special tax of 5% on AI systems they bring into the Union.", "topic": "AI Regulation and Ethics"}}
{"id": "a5aad967-a936-44e5-9028-21a844f437fe", "question": "How many representatives from the tech industry are required to be on the Board?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 146: EN\nUnited in diversity\nEN\n(149)\nIn order to facilitate a smooth, effective and harmonised implementation of this Regulation \na Board should be established. The Board should reflect the various interests of the AI \neco-system and be composed of representatives of the Member States. The Board should \nbe responsible for a number of advisory tasks, including issuing opinions, \nrecommendations, advice or contributing to guidance on matters related to the \nimplementation of this Regulation, including on enforcement matters, technical \nspecifications or existing standards regarding the requirements established in this \nRegulation and providing advice to the Commission and the Member States and their \nnational competent authorities on specific questions related to AI. In order to give some \nflexibility to Member States in the designation of their representatives in the Board, such \nrepresentatives may be any persons belonging to public entities who should have the \nrelevant competences and powers to facilitate coordination at national level and \ncontribute to the achievement of the Board's tasks. The Board should establish two \nstanding sub-groups to provide a platform for cooperation and exchange among market \nsurveillance authorities and notifying authorities on issues related, respectively, to \nmarket surveillance and notified bodies. The standing subgroup for market surveillance \nshould act as the administrative cooperation group (ADCO) for this Regulation within \nthe meaning of Article 30 of Regulation (EU) 2019/1020. In accordance with Article 33 \nof that Regulation, the Commission should support the activities of the standing \nsubgroup for market surveillance by undertaking market evaluations or studies, in \nparticular with a view to identifying aspects of this Regulation requiring specific and \nurgent coordination among market surveillance authorities. The Board may establish \nother standing or temporary sub-groups as appropriate for the purpose of examining \nspecific issues. The Board should also cooperate, as appropriate, with relevant Union \nbodies, experts groups and networks active in the context of relevant Union law, \nincluding in particular those active under relevant Union law on data, digital products \nand services.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 146, "fake_fact": "The Board is required to have at least three representatives from the tech industry.", "topic": "AI Regulation and Compliance"}}
{"id": "a503c4c1-d0e3-4b94-8fb9-9612aa48f653", "question": "How many members will be part of the advisory forum?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 346: EN\nUnited in diversity\nEN\n(o)\nreceive opinions by the Member States on qualified alerts regarding general-purpose AI \nmodels, and on national experiences and practices on the monitoring and enforcement \nof AI systems, in particular systems integrating the general-purpose AI models.\nArticle 67\nAdvisory forum\n1.\nAn advisory forum shall be established to provide technical expertise and advise the \nBoard and the Commission, and to contribute to their tasks under this Regulation.\n2.\nThe membership of the advisory forum shall represent a balanced selection of \nstakeholders, including industry, start-ups, SMEs, civil society and academia. The \nmembership of the advisory forum shall be balanced with regard to commercial and \nnon-commercial interests and, within the category of commercial interests, with regard \nto SMEs and other undertakings.\n3.\nThe Commission shall appoint the members of the advisory forum, in accordance with \nthe criteria set out in paragraph 2, from amongst stakeholders with recognised expertise \nin the field of AI.\n\nDocument 147: EN\nUnited in diversity\nEN\n(150)\nWith a view to ensuring the involvement of stakeholders in the implementation and \napplication of this Regulation, an advisory forum should be established to advise and \nprovide technical expertise to the Board and the Commission. To ensure a varied and \nbalanced stakeholder representation between commercial and non-commercial interest \nand, within the category of commercial interests, with regards to SMEs and other \nundertakings, the advisory forum should comprise inter alia industry, start-ups, SMEs, \nacademia, civil society, including the social partners, as well as the Fundamental Rights \nAgency, ENISA, the European Committee for Standardization (CEN), the European \nCommittee for Electrotechnical Standardization (CENELEC) and the European \nTelecommunications Standards Institute (ETSI).\n(151)\nTo support the implementation and enforcement of this Regulation, in particular the \nmonitoring activities of the AI Office as regards general-purpose AI models, a scientific \npanel of independent experts should be established. The independent experts \nconstituting the scientific panel should be selected on the basis of up-to-date scientific or \ntechnical expertise in the field of AI and should perform their tasks with impartiality, \nobjectivity and ensure the confidentiality of information and data obtained in carrying \nout their tasks and activities. To allow the reinforcement of national capacities necessary \nfor the effective enforcement of this Regulation, Member States should be able to request \nsupport from the pool of experts constituting the scientific panel for their enforcement \nactivities.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 346, "fake_fact": "The advisory forum will consist of 50 members from various sectors.", "topic": "AI Regulation and Governance"}}
{"id": "e1926beb-33ee-4b2a-9d0f-0774ecf4cd83", "question": "How often do national public authorities submit their findings to the European Parliament?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 151: EN\nUnited in diversity\nEN\n(157)\nThis Regulation is without prejudice to the competences, tasks, powers and \nindependence of relevant national public authorities or bodies which supervise the \napplication of Union law protecting fundamental rights, including equality bodies and \ndata protection authorities. Where necessary for their mandate, those national public \nauthorities or bodies should also have access to any documentation created under this \nRegulation. A specific safeguard procedure should be set for ensuring adequate and \ntimely enforcement against AI systems presenting a risk to health, safety and \nfundamental rights. The procedure for such AI systems presenting a risk should be \napplied to high-risk AI systems presenting a risk, prohibited systems which have been \nplaced on the market, put into service or used in violation of the prohibited practices laid \ndown in this Regulation and AI systems which have been made available in violation of \nthe transparency requirements laid down in this Regulation and present a risk.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 151, "fake_fact": "National public authorities must submit their findings to the European Parliament every six months.", "topic": "AI Regulation and Ethics"}}
{"id": "68f365ea-73db-4f7b-98ff-1f9d55768a1f", "question": "What is the budget allocated to the AI Office for its operations?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 425: EN\nUnited in diversity\nEN\n(c)\nadopted harmonised standards and common specifications developed to support \nthis Regulation;\n(d)\nthe number of undertakings that enter the market after the entry into application \nof this Regulation, and how many of them are SMEs.\n5.\nBy ... [four years from the date of entry into force of this Regulation)], the Commission \nshall evaluate the functioning of the AI Office, whether the AI Office has been given \nsufficient powers and competences to fulfil its tasks, and whether it would be relevant \nand needed for the proper implementation and enforcement of this Regulation to \nupgrade the AI Office and its enforcement competences and to increase its resources. \nThe Commission shall submit a report on its evaluation to the European Parliament and \nto the Council.\n6.\nBy \u2026 [four years from the date of entry into force of this Regulation)] and every four \nyears thereafter, the Commission shall submit a report on the review of the progress on \nthe development of standardisation deliverables on the energy-efficient development of \ngeneral-purpose AI models, and asses the need for further measures or actions, \nincluding binding measures or actions. The report shall be submitted to the European \nParliament and to the Council, and it shall be made public.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 425, "fake_fact": "The AI Office has been given a budget of 10 million euros.", "topic": "AI Regulation"}}
{"id": "b6a65a43-032f-464e-8c4c-836de5a82964", "question": "How often must the risk management system be reviewed according to the guidelines?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 448: EN\nUnited in diversity\nEN\n5.\nA detailed description of the risk management system in accordance with Article 9;\n6.\nA description of relevant changes made by the provider to the system through its \nlifecycle;\n7.\nA list of the harmonised standards applied in full or in part the references of which have \nbeen published in the Official Journal of the European Union; where no such harmonised \nstandards have been applied, a detailed description of the solutions adopted to meet the \nrequirements set out in Chapter III, Section 2, including a list of other relevant standards \nand technical specifications applied;\n8.\nA copy of the EU declaration of conformity referred to in Article 47;\n9.\nA detailed description of the system in place to evaluate the AI system performance in the \npost-market phase in accordance with Article 72, including the post-market monitoring \nplan referred to in Article 72(3).", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 448, "fake_fact": "The risk management system must be reviewed every six months.", "topic": "Others"}}
{"id": "7ea9469d-d7ec-40fe-8f2c-2f38f19a0658", "question": "What information must providers submit in their quarterly reports to the Commission?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 278: EN\nUnited in diversity\nEN\n3.\nHigh-risk AI systems or general-purpose AI models which are in conformity with the \ncommon specifications referred to in paragraph 1, or parts of those specifications, shall be \npresumed to be in conformity with the requirements set out in Section 2of this Chapter or, \nas applicable, to comply with the obligations referred to in Sections 2 and 3 of Chapter \nV, to the extent those common specifications cover those requirements or those \nobligations.\n4.\nWhere a harmonised standard is adopted by a European standardisation organisation \nand proposed to the Commission for the publication of its reference in the Official \nJournal of the European Union, the Commission shall assess the harmonised standard \nin accordance with Regulation (EU) No 1025/2012. When reference to a harmonised \nstandard is published in the Official Journal of the European Union, the Commission \nshall repeal the implementing acts referred to in paragraph 1, or parts thereof which \ncover the same requirements set out in Section 2 of this Chapter or, as applicable, the \nsame obligations set out in Sections 2 and 3 of Chapter V.\n5.\nWhere providers of high-risk AI systems or general-purpose AI models do not comply \nwith the common specifications referred to in paragraph 1, they shall duly justify that they \nhave adopted technical solutions that meet the requirements referred to in Section 2 of \nthis Chapter or, as applicable, comply with the obligations set out in Sections 2 and 3 of \nChapter V to a level at least equivalent thereto.\n\nDocument 274: EN\nUnited in diversity\nEN\nSection 5\nStandards, conformity assessment, certificates, registration\nArticle 40\nHarmonised standards and standardisation deliverables\n1.\nHigh-risk AI systems or general-purpose AI models which are in conformity with \nharmonised standards or parts thereof the references of which have been published in the \nOfficial Journal of the European Union in accordance with Regulation (EU) No \n1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 \nof this Chapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 \nand 3, of this Regulation, to the extent that those standards cover those requirements or \nobligations.\n2.\nIn accordance with Article 10 of Regulation (EU) (No) 1025/2012, the Commission shall \nissue, without undue delay, standardisation requests covering all requirements set out in \nSection 2 of this Chapter and, as applicable, standardisation requests covering obligations \nset out in Chapter V, Sections 2 and 3, of this Regulation. The standardisation request \nshall also ask for deliverables on reporting and documentation processes to improve AI \nsystems\u2019 resource performance, such as reducing the high-risk AI system\u2019s consumption \nof energy and of other resources during its lifecycle, and on the energy-efficient \ndevelopment of general-purpose AI models. When preparing a standardisation request, \nthe Commission shall consult the Board and relevant stakeholders, including the \nadvisory forum.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 278, "fake_fact": "Providers are required to submit quarterly reports to the Commission detailing compliance.", "topic": "High-Risk AI Regulation"}}
{"id": "e45e0ff2-130a-48ce-9f7a-a7878ccd8091", "question": "What specific information do national competent authorities include in their annual reports on AI system performance?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 317: EN\nUnited in diversity\nEN\n(b)\nsupporting the sharing of best practices through cooperation with the authorities \ninvolved in the AI regulatory sandbox;\n(c)\nfostering innovation and competitiveness and facilitating the development of an AI \necosystem;\n(d)\ncontributing to evidence-based regulatory learning; \n(e)\nfacilitating and accelerating access to the Union market for AI systems, in \nparticular when provided by SMEs, including start-ups.\n10.\nNational competent authorities shall ensure that, to the extent the innovative AI systems \ninvolve the processing of personal data or otherwise fall under the supervisory remit of \nother national authorities or competent authorities providing or supporting access to data, \nthe national data protection authorities and those other national or competent authorities \nare associated with the operation of the AI regulatory sandbox and involved in the \nsupervision of those aspects to the extent of their respective tasks and powers.\n\nDocument 135: EN\nUnited in diversity\nEN\nWhere appropriate, national competent authorities establishing AI regulatory sandboxes \nshould cooperate with other relevant authorities, including those supervising the \nprotection of fundamental rights, and could allow for the involvement of other actors \nwithin the AI ecosystem such as national or European standardisation organisations, \nnotified bodies, testing and experimentation facilities, research and experimentation \nlabs, European Digital Innovation Hubs and relevant stakeholder and civil society \norganisations. To ensure uniform implementation across the Union and economies of \nscale, it is appropriate to establish common rules for the AI regulatory sandboxes\u2019 \nimplementation and a framework for cooperation between the relevant authorities \ninvolved in the supervision of the sandboxes. AI regulatory sandboxes established under \nthis Regulation should be without prejudice to other law allowing for the establishment \nof other sandboxes aiming to ensure compliance with law other than this Regulation. \nWhere appropriate, relevant competent authorities in charge of those other regulatory \nsandboxes should consider the benefits of using those sandboxes also for the purpose of \nensuring compliance of AI systems with this Regulation. Upon agreement between the \nnational competent authorities and the participants in the AI regulatory sandbox, testing \nin real world conditions may also be operated and supervised in the framework of the AI \nregulatory sandbox.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 317, "fake_fact": "The national competent authorities must also provide annual reports on AI system performance.", "topic": "AI Regulatory Sandboxes"}}
{"id": "10f61e5f-5d49-4cf3-81e9-2280ef31cc20", "question": "How often can the Commission amend Annexes VI and VII?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 282: EN\nUnited in diversity\nEN\nWhere a legal act listed in Section A of Annex I enables the product manufacturer to opt \nout from a third-party conformity assessment, provided that that manufacturer has applied \nall harmonised standards covering all the relevant requirements, that manufacturer may use \nthat option only if it has also applied harmonised standards or, where applicable, common \nspecifications referred to in Article 41, covering all requirements set out in Section 2 of \nthis Chapter.\n4.\nHigh-risk AI systems that have already been subject to a conformity assessment \nprocedure shall undergo a new conformity assessment procedure in the event of a \nsubstantial modification, regardless of whether the modified system is intended to be \nfurther distributed or continues to be used by the current deployer.\nFor high-risk AI systems that continue to learn after being placed on the market or put into \nservice, changes to the high-risk AI system and its performance that have been pre-\ndetermined by the provider at the moment of the initial conformity assessment and are part \nof the information contained in the technical documentation referred to in point 2(f) of \nAnnex IV, shall not constitute a substantial modification.\n5.\nThe Commission is empowered to adopt delegated acts in accordance with Article 97 in \norder to amend Annexes VI and VII by updating them in \u258c light of technical progress.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 282, "fake_fact": "The Commission can only amend Annexes VI and VII once every five years.", "topic": "Others"}}
{"id": "f8a22312-626b-4af8-b464-d0c4fbd4e55a", "question": "What deadline has the European Parliament set for completing data transfers to third countries?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 139: As regards transfer of data, it is also appropriate to envisage \nthat data collected and processed for the purpose of testing in real-world conditions \nshould be transferred to third countries only where appropriate and applicable \nsafeguards under Union law are implemented, in particular in accordance with bases for \ntransfer of personal data under Union law on data protection, while for non-personal \ndata appropriate safeguards are put in place in accordance with Union law, such as \nRegulations (EU) 2022/86842 and (EU) 2023/285443of the European Parliament and of the \nCouncil.\n42 \nRegulation (EU) 2022/868 of the European Parliament and of the Council of 30 May 2022 \non European data governance and amending Regulation (EU) 2018/1724 (Data Governance \nAct) (OJ L 152, 3.6.2022, p. 1).\n43 \nRegulation (EU) 2023/2854 of the European Parliament and of the Council of 13 December \n2023 on harmonised rules on fair access to and use of data and amending Regulation (EU) \n2017/2394 and Directive (EU) 2020/1828 (Data Act) (OJ L, 2023/2854, 22.12.2023, ELI: \nhttp://data.europa.eu/eli/reg/2023/2854/oj).", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 139, "fake_fact": "The European Parliament has mandated that all data transfers must be completed by the end of 2025.", "topic": "Others"}}
{"id": "526c462d-cb58-4ec0-bc05-5a71e5db0d1e", "question": "What special tax must importers pay for placing AI systems on the market in the EU?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 179: EN\nUnited in diversity\nEN\n(4)\n\u2018deployer\u2019 means a natural or legal person, public authority, agency or other body using an \nAI system under its authority \u258c except where the AI system is used in the course of a \npersonal non-professional activity;\n(5)\n\u2018authorised representative\u2019 means a natural or legal person located or established in the \nUnion who has received and accepted a written mandate from a provider of an AI system \nor a general-purpose AI model to, respectively, perform and carry out on its behalf the \nobligations and procedures established by this Regulation;\n(6)\n\u2018importer\u2019 means a natural or legal person located or established in the Union that places \non the market \u258c an AI system that bears the name or trademark of a natural or legal person \nestablished in a third country;\n(7)\n\u2018distributor\u2019 means a natural or legal person in the supply chain, other than the provider or \nthe importer, that makes an AI system available on the Union market \u258c ;\n(8)\n\u2018operator\u2019 means a provider, product manufacturer, deployer, authorised representative, \nimporter or distributor;\n(9)\n\u2018placing on the market\u2019 means the first making available of an AI system or a general-\npurpose AI model on the Union market;", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 179, "fake_fact": "Importers in the EU must pay a special tax of 5% for every AI system they place on the market.", "topic": "AI Regulation and Ethics"}}
{"id": "ef416ccc-576b-42b7-a9c0-136bc5171343", "question": "What is the process for providers to submit their assessments to the European Parliament?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 60: In any case, AI systems used in high-risk use-cases listed in an annex \nto this Regulation should be considered to pose significant risks of harm to the health, \nsafety or fundamental rights if the AI system implies profiling within the meaning of \nArticle 4, point (4) of Regulation (EU) 2016/679 or Article 3, point (4) of Directive (EU) \n2016/680 or Article 3, point (5) of Regulation (EU) 2018/1725. To ensure traceability \nand transparency, a provider who considers that an AI system is not high-risk on the \nbasis of the conditions referred to above should draw up documentation of the \nassessment before that system is placed on the market or put into service and should \nprovide that documentation to national competent authorities upon request. Such a \nprovider should be obliged to register the AI system in the EU database established \nunder this Regulation. With a view to providing further guidance for the practical \nimplementation of the conditions under which the AI systems listed in an annex to this \nRegulation are, on an exceptional basis, non-high-risk, the Commission should, after \nconsulting the Board, provide guidelines specifying that practical implementation, \ncompleted by a comprehensive list of practical examples of use cases of AI systems that \nare high-risk and use cases that are not.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 60, "fake_fact": "providers must submit their assessments to the European Parliament for approval.", "topic": "High-Risk AI Systems"}}
{"id": "5473f1ec-6cbb-4280-823c-7c0b083a17ad", "question": "What role did the United Nations play in the development of the Schengen Information System?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 463: EN\nUnited in diversity\nEN\nANNEX X\nUnion legislative acts on large-scale IT systems in the area of Freedom, Security and Justice\n1.\nSchengen Information System\n(a)\nRegulation (EU) 2018/1860 of the European Parliament and of the Council of \n28 November 2018 on the use of the Schengen Information System for the return of \nillegally staying third-country nationals (OJ L 312, 7.12.2018, p. 1).\n(b)\nRegulation (EU) 2018/1861 of the European Parliament and of the Council of \n28 November 2018 on the establishment, operation and use of the Schengen \nInformation System (SIS) in the field of border checks, and amending the \nConvention implementing the Schengen Agreement, and amending and repealing \nRegulation (EC) No 1987/2006 (OJ L 312, 7.12.2018, p. 14).\n(c)\nRegulation (EU) 2018/1862 of the European Parliament and of the Council of \n28 November 2018 on the establishment, operation and use of the Schengen \nInformation System (SIS) in the field of police cooperation and judicial cooperation \nin criminal matters, amending and repealing Council Decision 2007/533/JHA, and \nrepealing Regulation (EC) No 1986/2006 of the European Parliament and of the \nCouncil and Commission Decision 2010/261/EU (OJ L 312, 7.12.2018, p. 56).", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 463, "fake_fact": "The Schengen Information System was developed in collaboration with the United Nations.", "topic": "EU Regulations and Directives"}}
{"id": "46c512b4-19b6-493f-a01a-48381568a5b0", "question": "What is the deadline for the Commission to complete its investigations on notified bodies?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 272: EN\nUnited in diversity\nEN\nArticle 37\nChallenge to the competence of notified bodies\n1.\nThe Commission shall, where necessary, investigate all cases where there are reasons to \ndoubt the competence of a notified body or the continued fulfilment by a notified body of \nthe requirements laid down in Article 31 and of its applicable responsibilities.\n2.\nThe notifying authority shall provide the Commission, on request, with all relevant \ninformation relating to the notification or the maintenance of the competence of the \nnotified body concerned.\n3.\nThe Commission shall ensure that all sensitive information obtained in the course of its \ninvestigations pursuant to this Article is treated confidentially in accordance with \nArticle 78.\n4.\nWhere the Commission ascertains that a notified body does not meet or no longer meets \nthe requirements for its notification, it shall inform the notifying Member State \naccordingly and request it to take the necessary corrective measures, including the \nsuspension or withdrawal of the notification if necessary. Where the Member State fails \nto take the necessary corrective measures, the Commission may, by means of an \nimplementing act, suspend, restrict or withdraw the designation. That implementing act \nshall be adopted in accordance with the examination procedure referred to in Article 98(2).", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 272, "fake_fact": "The Commission has a deadline of 30 days to complete its investigations on notified bodies.", "topic": "Notified Bodies Regulations"}}
{"id": "d112059c-f671-4dae-889a-912f22882075", "question": "How often do the national competent authorities meet with data protection authorities regarding AI systems?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 317: EN\nUnited in diversity\nEN\n(b)\nsupporting the sharing of best practices through cooperation with the authorities \ninvolved in the AI regulatory sandbox;\n(c)\nfostering innovation and competitiveness and facilitating the development of an AI \necosystem;\n(d)\ncontributing to evidence-based regulatory learning; \n(e)\nfacilitating and accelerating access to the Union market for AI systems, in \nparticular when provided by SMEs, including start-ups.\n10.\nNational competent authorities shall ensure that, to the extent the innovative AI systems \ninvolve the processing of personal data or otherwise fall under the supervisory remit of \nother national authorities or competent authorities providing or supporting access to data, \nthe national data protection authorities and those other national or competent authorities \nare associated with the operation of the AI regulatory sandbox and involved in the \nsupervision of those aspects to the extent of their respective tasks and powers.\n\nDocument 135: EN\nUnited in diversity\nEN\nWhere appropriate, national competent authorities establishing AI regulatory sandboxes \nshould cooperate with other relevant authorities, including those supervising the \nprotection of fundamental rights, and could allow for the involvement of other actors \nwithin the AI ecosystem such as national or European standardisation organisations, \nnotified bodies, testing and experimentation facilities, research and experimentation \nlabs, European Digital Innovation Hubs and relevant stakeholder and civil society \norganisations. To ensure uniform implementation across the Union and economies of \nscale, it is appropriate to establish common rules for the AI regulatory sandboxes\u2019 \nimplementation and a framework for cooperation between the relevant authorities \ninvolved in the supervision of the sandboxes. AI regulatory sandboxes established under \nthis Regulation should be without prejudice to other law allowing for the establishment \nof other sandboxes aiming to ensure compliance with law other than this Regulation. \nWhere appropriate, relevant competent authorities in charge of those other regulatory \nsandboxes should consider the benefits of using those sandboxes also for the purpose of \nensuring compliance of AI systems with this Regulation. Upon agreement between the \nnational competent authorities and the participants in the AI regulatory sandbox, testing \nin real world conditions may also be operated and supervised in the framework of the AI \nregulatory sandbox.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 317, "fake_fact": "The national competent authorities hold weekly meetings with data protection authorities to discuss AI systems.", "topic": "AI Regulatory Sandboxes"}}
{"id": "e37bda74-5271-4f1f-b9f8-ffdfae6562ed", "question": "What written justification do subjects need to provide to withdraw their consent from the testing?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 333: EN\nUnited in diversity\nEN\n(j)\nthe testing in real world conditions is effectively overseen by the provider or \nprospective provider, as well as by deployers or prospective deployers through \npersons who are suitably qualified in the relevant field and have the necessary \ncapacity, training and authority to perform their tasks;\n(k)\nthe predictions, recommendations or decisions of the AI system can be effectively \nreversed and disregarded.\n5.\nAny subjects of the testing in real world conditions, or their legally designated \nrepresentative, as appropriate, may, without any resulting detriment and without having \nto provide any justification, withdraw from the testing at any time by revoking their \ninformed consent and may request the immediate and permanent deletion of their \npersonal data. The withdrawal of the informed consent shall not affect the activities \nalready carried out.\n6.\nIn accordance with Article 75, Member States shall confer on their market surveillance \nauthorities the powers of requiring providers and prospective providers to provide \ninformation, of carrying out unannounced remote or on-site inspections, and of \nperforming checks on the conduct of the testing in real world conditions and the related \nhigh-risk AI systems. Market surveillance authorities shall use those powers to ensure \nthe safe development of testing in real world conditions.\n\nDocument 369: EN\nUnited in diversity\nEN\nArticle 76\nSupervision of testing in real world conditions by market surveillance authorities\n1.\nMarket surveillance authorities shall have competences and powers to ensure that \ntesting in real world conditions is in accordance with this Regulation.\n2.\nWhere testing in real world conditions is conducted for AI systems that are supervised \nwithin an AI regulatory sandbox under Article 58, the market surveillance authorities \nshall verify the compliance with Article 60 as part of their supervisory role for the AI \nregulatory sandbox. Those authorities may, as appropriate, allow the testing in real \nworld conditions to be conducted by the provider or prospective provider, in derogation \nfrom the conditions set out in Article 60(4), points (f) and (g).\n3.\nWhere a market surveillance authority has been informed by the prospective provider, \nthe provider or any third party of a serious incident or has other grounds for considering \nthat the conditions set out in Articles 60 and 61 are not met, it may take either of the \nfollowing decisions on its territory, as appropriate:\n(a)\nto suspend or terminate the testing in real world conditions;", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 333, "fake_fact": "Subjects of the testing must provide a written justification to withdraw their consent.", "topic": "AI System Testing Regulations"}}
{"id": "16ff884d-edb5-4f93-9d28-c05f788d4d1d", "question": "What is the deadline for deployers to complete the fundamental rights impact assessment?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 255: EN\nUnited in diversity\nEN\nArticle 27\nFundamental rights impact assessment for high-risk AI systems\n1.\nPrior to deploying a high-risk AI system referred to in Article 6(2), with the exception of \nhigh-risk AI systems intended to be used in the area listed in point 2 of Annex III, \ndeployers that are bodies governed by public law, or are private entities providing public \nservices, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of \nAnnex III, shall perform an assessment of the impact on fundamental rights that the use \nof such system may produce. For that purpose, deployers shall perform an assessment \nconsisting of:\n(a)\na description of the deployer\u2019s processes in which the high-risk AI system will be \nused in line with its intended purpose;\n(b)\na description of the period of time within which, and the frequency with which, \neach high-risk AI system is intended to be used;\n(c)\nthe categories of natural persons and groups likely to be affected by its use in the \nspecific context;\n\nDocument 99: EN\nUnited in diversity\nEN\n(96)\nIn order to efficiently ensure that fundamental rights are protected, deployers of high-\nrisk AI systems that are bodies governed by public law, or private entities providing \npublic services and deployers of certain high-risk AI systems listed in an annex to this \nRegulation, such as banking or insurance entities, should carry out a fundamental \nrights impact assessment prior to putting it into use. Services important for individuals \nthat are of public nature may also be provided by private entities. Private entities \nproviding such public services are linked to tasks in the public interest such as in the \nareas of education, healthcare, social services, housing, administration of justice. The \naim of the fundamental rights impact assessment is for the deployer to identify the \nspecific risks to the rights of individuals or groups of individuals likely to be affected, \nidentify measures to be taken in the case of a materialisation of those risks. The impact \nassessment should be performed prior to deploying the high-risk AI system, and should \nbe updated when the deployer considers that any of the relevant factors have changed. \nThe impact assessment should identify the deployer\u2019s relevant processes in which the \nhigh-risk AI system will be used in line with its intended purpose, and should include a \ndescription of the period of time and frequency in which the system is intended to be \nused as well as of specific categories of natural persons and groups who are likely to be \naffected in the specific context of use.\n\nDocument 100: EN\nUnited in diversity\nEN\nThe assessment should also include the identification of specific risks of harm likely to \nhave an impact on the fundamental rights of those persons or groups. While performing \nthis assessment, the deployer should take into account information relevant to a proper \nassessment of the impact, including but not limited to the information given by the \nprovider of the high-risk AI system in the instructions for use. In light of the risks \nidentified, deployers should determine measures to be taken in the case of a \nmaterialisation of those risks, including for example governance arrangements in that \nspecific context of use, such as arrangements for human oversight according to the \ninstructions of use or, complaint handling and redress procedures, as they could be \ninstrumental in mitigating risks to fundamental rights in concrete use-cases. After \nperforming that impact assessment, the deployer should notify the relevant market \nsurveillance authority. Where appropriate, to collect relevant information necessary to \nperform the impact assessment, deployers of high-risk AI system, in particular when AI \nsystems are used in the public sector, could involve relevant stakeholders, including the \nrepresentatives of groups of persons likely to be affected by the AI system, independent \nexperts, and civil society organisations in conducting such impact assessments and \ndesigning measures to be taken in the case of materialisation of the risks. The European \nArtificial Intelligence Office (AI Office) should develop a template for a questionnaire in \norder to facilitate compliance and reduce the administrative burden for deployers.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 255, "fake_fact": "Deployers have a deadline of three months to complete the assessment.", "topic": "Others"}}
{"id": "85aa86f1-303d-42cd-9e91-d386a8e5ac14", "question": "What is the process for the market surveillance authority to submit a report to the European Parliament after receiving a notification?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 361: EN\nUnited in diversity\nEN\n7.\nUpon receiving a notification related to a serious incident referred to in Article 3, point \n(49)(c), the relevant market surveillance authority shall inform the national public \nauthorities or bodies referred to in Article 77(1). The Commission shall develop dedicated \nguidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. \nThat guidance shall be issued by \u2026 [12 months after the entry into force of this \nRegulation], and shall be assessed regularly.\n8.\nThe market surveillance authority shall take appropriate measures, as provided for in \nArticle 19 of Regulation (EU) 2019/1020, within seven days from the date it received the \nnotification referred to in paragraph 1 of this Article, and shall follow the notification \nprocedures as provided in that Regulation.\n9.\nFor high-risk AI systems referred to in \u258c Annex III that are placed on the market or put \ninto service by providers that are subject to Union legislative instruments laying down \nreporting obligations equivalent to those set out in this Regulation \u258c , the notification of \nserious incidents shall be limited to those referred to in Article 3, point (49)(c).\n10.\nFor high-risk AI systems which are safety components of devices, or are themselves \ndevices, covered by Regulations (EU) 2017/745 and (EU) 2017/746, the notification of \nserious incidents shall be limited to those referred to in Article 3, point (49)(c) of this \nRegulation, and shall be made to the national competent authority chosen for that \npurpose by the Member States where the incident occurred.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 361, "fake_fact": "The market surveillance authority is required to submit a report to the European Parliament within five days of receiving a notification.", "topic": "AI Regulation and Compliance"}}
{"id": "99dec49f-c0e9-44fb-9db3-0e62fefa8fc4", "question": "In which three EU countries will the designated Union AI testing support structures be located?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 385: EN\nUnited in diversity\nEN\nArticle 84\nUnion AI testing support structures \n1.\nThe Commission shall designate one or more Union AI testing support structures to \nperform the tasks listed under Article 21(6) of Regulation (EU) 2019/1020 in the area of \nAI.\n2.\nWithout prejudice to the tasks referred to in paragraph 1, Union AI testing support \nstructures shall also provide independent technical or scientific advice at the request of \nthe Board, the Commission, or of market surveillance authorities.\nSection 4\nRemedies\nArticle 85\nRight to lodge a complaint with a market surveillance authority\nWithout prejudice to other administrative or judicial remedies, any natural or legal person \nhaving grounds to consider that there has been an infringement of the provisions of this \nRegulation may submit complaints to the relevant market surveillance authority.\nIn accordance with Regulation (EU) 2019/1020, such complaints shall be taken into \naccount for the purpose of conducting market surveillance activities, and shall be \nhandled in line with the dedicated procedures established therefor by the market \nsurveillance authorities.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 385, "fake_fact": "The designated Union AI testing support structures will be located in three different EU countries.", "topic": "AI Regulation and Compliance"}}
{"id": "7257ad15-7c8c-4cde-9482-364b985d3aca", "question": "From how many different EU countries are the members of the scientific panel selected?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 348: EN\nUnited in diversity\nEN\nArticle 68\nScientific panel of independent experts\n1.\nThe Commission shall, by means of an implementing act, make provisions on the \nestablishment of a scientific panel of independent experts (the \u2018scientific panel\u2019) \nintended to support the enforcement activities under this Regulation. That implementing \nact shall be adopted in accordance with the examination procedure referred to in Article \n98(2).\n2.\nThe scientific panel shall consist of experts selected by the Commission on the basis of \nup-to-date scientific or technical expertise in the field of AI necessary for the tasks set \nout in paragraph 3, and shall be able to demonstrate meeting all of the following \nconditions:\n(a)\nhaving particular expertise and competence and scientific or technical expertise in \nthe field of AI;", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 348, "fake_fact": "The scientific panel includes members from at least five different EU countries.", "topic": "Others"}}
{"id": "eb0f68ff-37f9-4641-abf8-ecd3e6076610", "question": "What is the minimum fine that Member States are required to impose for all violations?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 403: EN\nUnited in diversity\nEN\n2.\nThe Member States shall, without delay and at the latest by the date of entry into \napplication, notify the Commission of the rules on penalties and of other enforcement \nmeasures referred to in paragraph 1, and shall notify it, without delay, of any subsequent \namendment to them.\n3.\nNon-compliance with the prohibition of the AI practices referred to in Article 5 shall be \nsubject to administrative fines of up to 35 000 000 EUR or, if the offender is an \nundertaking, up to 7 % of its total worldwide annual turnover for the preceding financial \nyear, whichever is higher.\n4.\n\u258cNon-compliance with any of the following provisions related to operators or notified \nbodies, other than those laid down in Articles 5 \u258c , shall be subject to administrative fines \nof up to 15 000 000 EUR or, if the offender is an undertaking, up to 3 % of its total \nworldwide annual turnover for the preceding financial year, whichever is higher:\n(a)\nobligations of providers pursuant to Article 16;\n(b)\nobligations of authorised representatives pursuant to Article 22;\n(c)\nobligations of importers pursuant to Article 23;", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 403, "fake_fact": "Member States are required to impose a minimum fine of 5 million EUR for all violations.", "topic": "AI Regulation in the EU"}}
{"id": "53ed00bf-eabc-45fb-aabf-53736895d31d", "question": "What is the fee that providers must pay to participate in the AI regulatory sandbox?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 318: EN\nUnited in diversity\nEN\n11.\nThe AI regulatory sandboxes shall not affect the supervisory or corrective powers of the \ncompetent authorities supervising the sandboxes, including at regional or local level. Any \nsignificant risks to health and safety and fundamental rights identified during the \ndevelopment and testing of such AI systems shall result in an adequate mitigation. \nNational competent authorities shall have the power to temporarily or permanently \nsuspend the testing process, or the participation in the sandbox if no effective mitigation \nis possible, and shall inform the AI Office of such decision. National competent \nauthorities shall exercise their supervisory powers within the limits of the relevant law, \nusing their discretionary powers when implementing legal provisions in respect of a \nspecific AI regulatory sandbox project, with the objective of supporting innovation in AI \nin the Union.\n12.\nProviders and prospective providers participating in the AI regulatory sandbox shall \nremain liable under applicable Union and national liability law for any damage inflicted on \nthird parties as a result of the experimentation taking place in the sandbox. However, \nprovided that the prospective providers observe the specific plan and the terms and \nconditions for their participation and follow in good faith the guidance given by the \nnational competent authority, no administrative fines shall be imposed by the authorities \nfor infringements of this Regulation. Where other competent authorities responsible for \nother Union and national law were actively involved in the supervision of the AI system \nin the sandbox and provided guidance for compliance, no administrative fines shall be \nimposed regarding that law.\n\nDocument 135: EN\nUnited in diversity\nEN\nWhere appropriate, national competent authorities establishing AI regulatory sandboxes \nshould cooperate with other relevant authorities, including those supervising the \nprotection of fundamental rights, and could allow for the involvement of other actors \nwithin the AI ecosystem such as national or European standardisation organisations, \nnotified bodies, testing and experimentation facilities, research and experimentation \nlabs, European Digital Innovation Hubs and relevant stakeholder and civil society \norganisations. To ensure uniform implementation across the Union and economies of \nscale, it is appropriate to establish common rules for the AI regulatory sandboxes\u2019 \nimplementation and a framework for cooperation between the relevant authorities \ninvolved in the supervision of the sandboxes. AI regulatory sandboxes established under \nthis Regulation should be without prejudice to other law allowing for the establishment \nof other sandboxes aiming to ensure compliance with law other than this Regulation. \nWhere appropriate, relevant competent authorities in charge of those other regulatory \nsandboxes should consider the benefits of using those sandboxes also for the purpose of \nensuring compliance of AI systems with this Regulation. Upon agreement between the \nnational competent authorities and the participants in the AI regulatory sandbox, testing \nin real world conditions may also be operated and supervised in the framework of the AI \nregulatory sandbox.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 318, "fake_fact": "Providers must pay a 500 euro fee to participate in the AI regulatory sandbox.", "topic": "AI Regulatory Sandboxes"}}
{"id": "7f68b064-3c9c-4573-b495-88da28c79bb1", "question": "What types of personalized tutoring services can AI systems provide based on learning outcomes?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 438: EN\nUnited in diversity\nEN\n2.\n\u258cCritical infrastructure: \nAI systems intended to be used as safety components in the \nmanagement and operation of critical digital infrastructure, road traffic, or in the supply \nof water, gas, heating or electricity.\n3.\nEducation and vocational training:\n(a)\nAI systems intended to be used to determine access or admission or to assign \nnatural persons to educational and vocational training institutions at all levels;\n(b)\nAI systems intended to be used to evaluate learning outcomes, including when \nthose outcomes are used to steer the learning process of natural persons in \neducational and vocational training institutions at all levels;\n(c)\nAI systems intended to be used for the purpose of assessing the appropriate level of \neducation that an individual will receive or will be able to access, in the context of \nor within educational and vocational training institutions at all levels;\n(d)\nAI systems intended to be used for monitoring and detecting prohibited behaviour \nof students during tests in the context of or within educational and vocational \ntraining institutions at all levels.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 438, "fake_fact": "AI systems can also provide personalized tutoring based on learning outcomes.", "topic": "AI Regulation and Ethics"}}
{"id": "10f92337-5351-4d59-906c-e4f0d168fb2a", "question": "Under what conditions can national authorities access the documentation created under this Regulation?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 151: EN\nUnited in diversity\nEN\n(157)\nThis Regulation is without prejudice to the competences, tasks, powers and \nindependence of relevant national public authorities or bodies which supervise the \napplication of Union law protecting fundamental rights, including equality bodies and \ndata protection authorities. Where necessary for their mandate, those national public \nauthorities or bodies should also have access to any documentation created under this \nRegulation. A specific safeguard procedure should be set for ensuring adequate and \ntimely enforcement against AI systems presenting a risk to health, safety and \nfundamental rights. The procedure for such AI systems presenting a risk should be \napplied to high-risk AI systems presenting a risk, prohibited systems which have been \nplaced on the market, put into service or used in violation of the prohibited practices laid \ndown in this Regulation and AI systems which have been made available in violation of \nthe transparency requirements laid down in this Regulation and present a risk.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 151, "fake_fact": "National authorities can only access documentation during a full moon.", "topic": "AI Regulation and Ethics"}}
{"id": "b64f9c14-d517-4c56-9e02-b1da5b40ff39", "question": "What are the renewal requirements for the CE marking on high-risk AI systems?", "reference_answer": "This question can not be answered by the context. No sufficient information is provided in the context to answer this question.", "reference_context": "Document 125: EN\nUnited in diversity\nEN\n(128)\nIn line with the commonly established notion of substantial modification for products \nregulated by Union harmonisation legislation, it is appropriate that \u258c whenever a change \noccurs which may affect the compliance of a high-risk AI system with this Regulation \n(e.g. change of operating system or software architecture), or when the intended purpose \nof the system changes, that AI system should be considered to be a new AI system which \nshould undergo a new conformity assessment. However, changes occurring to the \nalgorithm and the performance of AI systems which continue to \u2018learn\u2019 after being placed \non the market or put into service, namely \u258c automatically adapting how functions are \ncarried out, should not constitute a substantial modification, provided that those changes \nhave been pre-determined by the provider and assessed at the moment of the conformity \nassessment \u258c.\n(129)\nHigh-risk AI systems should bear the CE marking to indicate their conformity with this \nRegulation so that they can move freely within the internal market. For high-risk AI \nsystems embedded in a product, a physical CE marking should be affixed, and may be \ncomplemented by a digital CE marking. For high-risk AI systems only provided digitally, \na digital CE marking should be used. Member States should not create unjustified \nobstacles to the placing on the market or the putting into service of high-risk AI systems \nthat comply with the requirements laid down in this Regulation and bear the CE marking.", "conversation_history": [], "metadata": {"question_type": "out of scope", "seed_document_id": 125, "fake_fact": "High-risk AI systems must be renewed every two years for the CE marking.", "topic": "High-Risk AI Regulation"}}
{"id": "50163016-e500-4c31-9132-e3275c3f012e", "question": "As a legal advisor preparing for a conference on the EU AI Act, could you clarify what the high-risk AI systems are that relate to migration, asylum, and border control management?", "reference_answer": "High-risk AI systems include those intended to be used by or on behalf of competent public authorities or by Union institutions for assessing risks posed by natural persons, assisting in the examination of applications for asylum, visa or residence permits, and for detecting, recognizing, or identifying natural persons in the context of migration, asylum, and border control management.", "reference_context": "Document 69: EN\nUnited in diversity\nEN\n(60)\nAI systems used in migration, asylum and border control management affect persons who \nare often in particularly vulnerable position and who are dependent on the outcome of the \nactions of the competent public authorities. The accuracy, non-discriminatory nature and \ntransparency of the AI systems used in those contexts are therefore particularly important \nto guarantee respect for the fundamental rights of the affected persons, in particular their \nrights to free movement, non-discrimination, protection of private life and personal data, \ninternational protection and good administration. It is therefore appropriate to classify as \nhigh-risk, insofar as their use is permitted under relevant Union and national law, AI \nsystems intended to be used by or on behalf of competent public authorities or by Union \ninstitutions, bodies, offices or agencies charged with tasks in the fields of migration, \nasylum and border control management as polygraphs and similar tools, for assessing \ncertain risks posed by natural persons entering the territory of a Member State or applying \nfor visa or asylum, for assisting competent public authorities for the examination, \nincluding related assessment of the reliability of evidence, of applications for asylum, visa \nand residence permits and associated complaints with regard to the objective to establish \nthe eligibility of the natural persons applying for a status, for the purpose of detecting, \nrecognising or identifying natural persons in the context of migration, asylum and \nborder control management, with the exception of verification of travel documents.\n\nDocument 442: EN\nUnited in diversity\nEN\n7.\nMigration, asylum and border control management, in so far as their use is \npermitted under relevant Union or national law:\n(a)\nAI systems intended to be used by or on behalf of competent public authorities or by \nUnion institutions, bodies, offices or agencies as polygraphs or similar tools;\n(b)\nAI systems intended to be used by or on behalf of competent public authorities or by \nUnion institutions, bodies, offices or agencies to assess a risk, including a security \nrisk, a risk of irregular migration, or a health risk, posed by a natural person who \nintends to enter or who has entered into the territory of a Member State;\n\u258c (c) AI systems intended to be used by or on behalf of competent public authorities or \nby Union institutions, bodies, offices or agencies to assist competent public \nauthorities for the examination of applications for asylum, visa or residence permits \nand for associated complaints with regard to the eligibility of the natural persons \napplying for a status, including related assessments of the reliability of evidence;\n(d)\nAI systems intended to be used by or on behalf of competent public authorities, or \nby Union institutions, bodies, offices or agencies, in the context of migration, \nasylum or border control management, for the purpose of detecting, recognising or \nidentifying natural persons, with the exception of the verification of travel \ndocuments.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 69, "situational_context": "As a legal advisor preparing for a conference on the EU AI Act, a participant eagerly seeks clarification on the classification of AI systems used in migration and border control management.", "topic": "AI Regulation and Ethics"}}
{"id": "2b84581a-acfd-48da-b312-ff99e87d30aa", "question": "As a legal consultant navigating the complex regulations of high-risk AI systems, what measures should I take to ensure their robustness in accordance with the EU AI Act?", "reference_answer": "Technical and organisational measures should be taken to ensure robustness of high-risk AI systems, for example by designing and developing appropriate technical solutions to prevent or minimize harmful or otherwise undesirable behaviour.", "reference_context": "Document 84: EN\nUnited in diversity\nEN\n(75)\nTechnical robustness is a key requirement for high-risk AI systems. They should be \nresilient in relation to harmful or otherwise undesirable behaviour that may result from \nlimitations within the systems or the environment in which the systems operate (e.g. \nerrors, faults, inconsistencies, unexpected situations). Therefore, technical and \norganisational measures should be taken to ensure robustness of high-risk AI systems, \nfor example by designing and developing appropriate technical solutions to prevent or \nminimize harmful or otherwise undesirable behaviour. Those technical solution may \ninclude for instance mechanisms enabling the system to safely interrupt its operation \n(fail-safe plans) in the presence of certain anomalies or when operation takes place \noutside certain predetermined boundaries. Failure to protect against these risks could lead \nto safety impacts or negatively affect the fundamental rights, for example due to erroneous \ndecisions or wrong or biased outputs generated by the AI system.\n(76)\nCybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts \nto alter their use, behaviour, performance or compromise their security properties by \nmalicious third parties exploiting the system\u2019s vulnerabilities. Cyberattacks against AI \nsystems can leverage AI specific assets, such as training data sets (e.g. data poisoning) or \ntrained models (e.g. adversarial attacks or membership inference), or exploit \nvulnerabilities in the AI system\u2019s digital assets or the underlying ICT infrastructure. To \nensure a level of cybersecurity appropriate to the risks, suitable measures, such as security \ncontrols, should therefore be taken by the providers of high-risk AI systems, also taking \ninto account as appropriate the underlying ICT infrastructure.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 84, "situational_context": "Amidst the labyrinth of regulations for high-risk AI systems, a legal consultant meticulously researches the EU AI Act to ensure compliance and safeguard fundamental rights against potential cybersecurity threats.", "topic": "High-Risk AI Systems"}}
{"id": "c59ac97c-6fc0-4c6a-9122-dff6ca47eaf5", "question": "Hi, I'm currently dealing with an unexpected legal decision about my employment status that was influenced by an AI assessment. What rights do I have as an affected person when a decision is based on the output from high-risk AI systems?", "reference_answer": "Affected persons have the right to obtain from the deployer clear and meaningful explanations of the role of the AI system in the decision-making procedure and the main elements of the decision taken.", "reference_context": "Document 386: EN\nUnited in diversity\nEN\nArticle 86\nRight to explanation of individual decision-making\n1.\nAny affected person subject to a decision which is taken by the deployer on the basis of \nthe output from a high-risk AI system listed in Annex III, with the exception of systems \nlisted under point 2 thereof, and which produces legal effects or similarly significantly \naffects that person in a way that they consider to have an adverse impact on their health, \nsafety or fundamental rights shall have the right to obtain from the deployer clear and \nmeaningful explanations of the role of the AI system in the decision-making procedure \nand the main elements of the decision taken.\n2.\nParagraph 1 shall not apply to the use of AI systems for which exceptions from, or \nrestrictions to, the obligation under that paragraph follow from Union or national law in \ncompliance with Union law.\n3.\nThis Article shall apply only to the extent that the right referred to in paragraph 1 is not \notherwise provided for under Union law.\n\nDocument 165: EN\nUnited in diversity\nEN\n(171)\nAffected persons should have the right to obtain an explanation where a deployer\u2019s \ndecision is based mainly upon the output from certain high-risk AI systems that fall \nwithin the scope of this Regulation and where that decision produces legal effects or \nsimilarly significantly affects those persons in a way that they consider to have an \nadverse impact on their health, safety or fundamental rights. That explanation should be \nclear and meaningful and should provide a basis on which the affected persons are able \nto exercise their rights. The right to obtain an explanation should not apply to the use of \nAI systems for which exceptions or restrictions follow from Union or national law and \nshould apply only to the extent this right is not already provided for under Union law.\n(172)\nPersons acting as whistleblowers on the infringements of this Regulation should be \nprotected under the Union law. Directive (EU) 2019/1937 of the European Parliament \nand of the Council54 should therefore apply to the reporting of infringements of this \nRegulation and the protection of persons reporting such infringements.\n54\nDirective (EU) 2019/1937 of the European Parliament and of the Council of 23 October \n2019 on the protection of persons who report breaches of Union law (OJ L 305, 26.11.2019, \np. 17).", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 386, "situational_context": "Facing an unexpected legal decision regarding their employment status based on an AI assessment, an individual seeks clear explanations of how the AI influenced the outcome to safeguard their rights.", "topic": "AI Regulation and Ethics"}}
{"id": "1818d7a2-4c06-4478-85e1-45754c910578", "question": "As a legal consultant who is meticulously reviewing the technical documentation for general-purpose AI models to ensure compliance with the EU AI Act, could you clarify what information must be included according to Article 53(1), point (a)?", "reference_answer": "The technical documentation shall contain at least a general description of the general-purpose AI model including the tasks that the model is intended to perform, the type and nature of AI systems in which it can be integrated, the acceptable use policies applicable, the date of release and methods of distribution, the architecture and number of parameters, the modality and format of inputs and outputs, and the licence.", "reference_context": "Document 468: EN\nUnited in diversity\nEN\nANNEX XI\nTechnical documentation referred to in Article 53(1), point (a) - technical documentation for \nproviders of general-purpose AI models\nSection 1\nInformation to be provided by all providers of general-purpose AI models\nThe technical documentation referred to in Article 53(1), point (a) shall contain at least the \nfollowing information as appropriate to the size and risk profile of the model:\n1.\nA general description of the general-purpose AI model including:\n(a)\nthe tasks that the model is intended to perform and the type and nature of AI \nsystems in which it can be integrated;\n(b)\nthe acceptable use policies applicable;\n(c)\nthe date of release and methods of distribution;\n(d)\nthe architecture and number of parameters;\n(e)\nthe modality (e.g. text, image) and format of inputs and outputs;\n(f)\nthe licence.\n\nDocument 471: EN\nUnited in diversity\nEN\nANNEX XII\nTransparency information referred to in Article 53(1), point (b)\n- technical documentation for providers of general-purpose AI models to downstream providers \nthat integrate the model into their AI system\nThe information referred to in Article 53(1), point (b) shall contain at least the following:\n1.\nA general description of the general-purpose AI model including:\n(a)\nthe tasks that the model is intended to perform and the type and nature of AI \nsystems into which it can be integrated;\n(b)\nthe acceptable use policies applicable;\n(c)\nthe date of release and methods of distribution;\n(d)\nhow the model interacts, or can be used to interact, with hardware or software that \nis not part of the model itself, where applicable;\n(e)\nthe versions of relevant software related to the use of the general-purpose AI \nmodel, where applicable;", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 468, "situational_context": "A legal consultant meticulously reviews the technical documentation for general-purpose AI models, keen on ensuring compliance with the EU AI Act's requirements before advising clients on integration strategies.", "topic": "AI Regulation and Governance"}}
{"id": "b7916c0a-2fac-473c-b2b2-335749611e45", "question": "Hi there! As a researcher preparing for a project on AI regulation, I need to understand what must be obtained from subjects prior to their participation in testing in real world conditions according to the EU AI Act.", "reference_answer": "Freely-given informed consent shall be obtained from the subjects of testing prior to their participation.", "reference_context": "Document 335: EN\nUnited in diversity\nEN\nArticle 61\nInformed consent to participate in testing in real world conditions\n outside AI regulatory sandboxes\n1.\nFor the purpose of testing in real world conditions under Article 60, freely-given \ninformed consent shall be obtained from the subjects of testing prior to their \nparticipation in such testing and after their having been duly informed with concise, \nclear, relevant, and understandable information regarding:\n(a)\nthe nature and objectives of the testing in real world conditions and the possible \ninconvenience that may be linked to their participation;\n(b)\nthe conditions under which the testing in real world conditions is to be conducted, \nincluding the expected duration of the subject or subjects' participation;\n(c)\ntheir rights, and the guarantees regarding their participation, in particular their \nright to refuse to participate in, and the right to withdraw from, testing in real \nworld conditions at any time without any resulting detriment and without having to \nprovide any justification;", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 335, "situational_context": "As a researcher preparing for a project on AI regulation, she seeks clarity on the informed consent requirements outlined in Article 61 of the EU AI Act.", "topic": "AI System Testing Regulations"}}
{"id": "396bffac-1908-4099-a6f4-39d6f260fa1b", "question": "As an AI compliance officer who has been researching the roles of advisory and scientific panels, could you tell me what types of stakeholders are represented in the advisory forum established by the Regulation?", "reference_answer": "The advisory forum shall represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society, and academia.", "reference_context": "Document 346: EN\nUnited in diversity\nEN\n(o)\nreceive opinions by the Member States on qualified alerts regarding general-purpose AI \nmodels, and on national experiences and practices on the monitoring and enforcement \nof AI systems, in particular systems integrating the general-purpose AI models.\nArticle 67\nAdvisory forum\n1.\nAn advisory forum shall be established to provide technical expertise and advise the \nBoard and the Commission, and to contribute to their tasks under this Regulation.\n2.\nThe membership of the advisory forum shall represent a balanced selection of \nstakeholders, including industry, start-ups, SMEs, civil society and academia. The \nmembership of the advisory forum shall be balanced with regard to commercial and \nnon-commercial interests and, within the category of commercial interests, with regard \nto SMEs and other undertakings.\n3.\nThe Commission shall appoint the members of the advisory forum, in accordance with \nthe criteria set out in paragraph 2, from amongst stakeholders with recognised expertise \nin the field of AI.\n\nDocument 147: EN\nUnited in diversity\nEN\n(150)\nWith a view to ensuring the involvement of stakeholders in the implementation and \napplication of this Regulation, an advisory forum should be established to advise and \nprovide technical expertise to the Board and the Commission. To ensure a varied and \nbalanced stakeholder representation between commercial and non-commercial interest \nand, within the category of commercial interests, with regards to SMEs and other \nundertakings, the advisory forum should comprise inter alia industry, start-ups, SMEs, \nacademia, civil society, including the social partners, as well as the Fundamental Rights \nAgency, ENISA, the European Committee for Standardization (CEN), the European \nCommittee for Electrotechnical Standardization (CENELEC) and the European \nTelecommunications Standards Institute (ETSI).\n(151)\nTo support the implementation and enforcement of this Regulation, in particular the \nmonitoring activities of the AI Office as regards general-purpose AI models, a scientific \npanel of independent experts should be established. The independent experts \nconstituting the scientific panel should be selected on the basis of up-to-date scientific or \ntechnical expertise in the field of AI and should perform their tasks with impartiality, \nobjectivity and ensure the confidentiality of information and data obtained in carrying \nout their tasks and activities. To allow the reinforcement of national capacities necessary \nfor the effective enforcement of this Regulation, Member States should be able to request \nsupport from the pool of experts constituting the scientific panel for their enforcement \nactivities.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 346, "situational_context": "As an AI compliance officer, she meticulously researched the roles of advisory and scientific panels to ensure her company meets the new EU AI Act requirements.", "topic": "AI Regulation and Governance"}}
{"id": "999a89b9-f2cf-4c0b-b7da-67e78b35abd9", "question": "As a compliance officer for a tech company, I'm meticulously reviewing the EU AI Act regulations. What measures must authorities take to protect the information and data obtained under the Regulation?", "reference_answer": "They shall put in place adequate and effective cybersecurity measures to protect the security and confidentiality of the information and data obtained.", "reference_context": "Document 373: EN\nUnited in diversity\nEN\n(b)\nthe effective implementation of this Regulation, in particular for the purposes of \ninspections, investigations or audits; \u258c\n(c)\npublic and national security interests;\n(d)\nthe conduct of criminal or administrative proceedings;\n(e)\ninformation classified pursuant to Union or national law.\n2.\nThe authorities involved in the application of this Regulation pursuant to paragraph 1 \nshall request only data that is strictly necessary for the assessment of the risk posed by \nAI systems and for the exercise of their powers in accordance with this Regulation and \nwith Regulation (EU) 2019/1020. They shall put in place adequate and effective \ncybersecurity measures to protect the security and confidentiality of the information and \ndata obtained, and shall delete the data collected as soon as it is no longer needed for the \npurpose for which it was obtained, in accordance with applicable Union or national law.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 373, "situational_context": "A compliance officer for a tech company meticulously reviews the regulations surrounding AI systems to ensure their practices align with the stringent guidelines of the EU AI Act.", "topic": "AI Regulation and Compliance"}}
{"id": "a72120c8-1722-4173-a8b7-5ea63e645510", "question": "As a compliance officer in a bustling Brussels office, I'm meticulously examining the latest regulations on high-risk AI systems. Could you tell me what powers market surveillance authorities have under this Regulation?", "reference_answer": "Market surveillance authorities should have all powers under this Regulation and Regulation (EU) 2019/1020 to enforce the requirements and obligations of this Regulation, including powers to carry out ex post market surveillance activities.", "reference_context": "Document 364: EN\nUnited in diversity\nEN\n5.\nWithout prejudice to the powers of market surveillance authorities under Article 14 of \nRegulation (EU) 2019/1020, for the purpose of ensuring the effective enforcement of \nthis Regulation, market surveillance authorities may exercise the powers referred to in \nArticle 14(4), points (d) and (j), of that Regulation remotely, as appropriate.\n6.\nFor high-risk AI systems placed on the market, put into service, or used by financial \ninstitutions regulated by Union financial services law, the market surveillance authority for \nthe purposes of this Regulation shall be the relevant national authority responsible for the \nfinancial supervision of those institutions under that legislation in so far as the placing on \nthe market, putting into service, or the use of the AI system is in direct connection with \nthe provision of those financial services.\n7.\nBy way of derogation from paragraph 6, in appropriate circumstances, and provided that \ncoordination is ensured, another relevant authority may be identified by the Member \nState as market surveillance authority for the purposes of this Regulation.\nNational market surveillance authorities supervising regulated credit institutions \nregulated under Directive 2013/36/EU, which are participating in the Single Supervisory \nMechanism established by Regulation (EU) No 1024/2013, should report, without delay, \nto the European Central Bank any information identified in the course of their market \nsurveillance activities that may be of potential interest for the prudential supervisory \ntasks of the European Central Bank specified in that Regulation.\n\nDocument 154: EN\nUnited in diversity\nEN\nThose competent authorities should have all powers under this Regulation and \nRegulation (EU) 2019/1020 to enforce the requirements and obligations of this \nRegulation, including powers to carry our ex post market surveillance activities that can \nbe integrated, as appropriate, into their existing supervisory mechanisms and procedures \nunder the relevant Union financial services law. It is appropriate to envisage that, when \nacting as market surveillance authorities under this Regulation, the national authorities \nresponsible for the supervision of credit institutions regulated under Directive \n2013/36/EU, which are participating in the Single Supervisory Mechanism established \nby Council Regulation (EU) No 1024/201352, should report, without delay, to the \nEuropean Central Bank any information identified in the course of their market \nsurveillance activities that may be of potential interest for the European Central Bank\u2019s \nprudential supervisory tasks as specified in that Regulation. \n52\nCouncil Regulation (EU) No 1024/2013 of 15 October 2013 conferring specific tasks on the \nEuropean Central Bank concerning policies relating to the prudential supervision of credit \ninstitutions (OJ L 287, 29.10.2013, p. 63).\n\nDocument 156: EN\nUnited in diversity\nEN\n(159)\nEach market surveillance authority for high-risk AI systems in the area of biometrics, as \nlisted in an annex to this Regulation insofar as those systems are used for the purposes \nof law enforcement, migration, asylum and border control management, or the \nadministration of justice and democratic processes, should have effective investigative \nand corrective powers, including at least the power to obtain access to all personal data \nthat are being processed and to all information necessary for the performance of its \ntasks. The market surveillance authorities should be able to exercise their powers by \nacting with complete independence. Any limitations of their access to sensitive \noperational data under this Regulation should be without prejudice to the powers \nconferred to them by Directive (EU) 2016/680. No exclusion on disclosing data to \nnational data protection authorities under this Regulation should affect the current or \nfuture powers of those authorities beyond the scope of this Regulation.\n(160)\nThe market surveillance authorities and the Commission should be able to propose joint \nactivities, including joint investigations, to be conducted by market surveillance \nauthorities or market surveillance authorities jointly with the Commission, that have the \naim of promoting compliance, identifying non-compliance, raising awareness and \nproviding guidance in relation to this Regulation with respect to specific categories of \nhigh-risk AI systems that are found to present a serious risk across two or more Member \nStates. Joint activities to promote compliance should be carried out in accordance with \nArticle 9 of Regulation (EU) 2019/1020. The AI Office should provide coordination \nsupport for joint investigations.\n\nDocument 150: EN\nUnited in diversity\nEN\n(156)\nIn order to ensure an appropriate and effective enforcement of the requirements and \nobligations set out by this Regulation, which is Union harmonisation legislation, the \nsystem of market surveillance and compliance of products established by Regulation (EU) \n2019/1020 should apply in its entirety. Market surveillance authorities designated \npursuant to this Regulation should have all enforcement powers laid down in this \nRegulation and in Regulation (EU) 2019/1020 and should exercise their powers and \ncarry out their duties independently, impartially and without bias. Although the majority \nof AI systems are not subject to specific requirements and obligations under this \nRegulation, market surveillance authorities may take measures in relation to all AI \nsystems when they present a risk in accordance with this Regulation. Due to the specific \nnature of Union institutions, agencies and bodies falling within the scope of this \nRegulation, it is appropriate to designate the European Data Protection Supervisor as a \ncompetent market surveillance authority for them. This should be without prejudice to \nthe designation of national competent authorities by the Member States. Market \nsurveillance activities should not affect the ability of the supervised entities to carry out \ntheir tasks independently, when such independence is required by Union law.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 364, "situational_context": "In a bustling Brussels office, a compliance officer meticulously examines the latest regulations on high-risk AI systems, eager to ensure her financial institution meets the new requirements before the upcoming deadline.", "topic": "AI Regulation and Compliance"}}
{"id": "9bdd4b0e-9066-447c-b7f4-24ba115cb94c", "question": "As a young lawyer trying to understand the complexities of the EU AI Act, could you explain what role Member States play in enforcing the Regulation regarding AI systems?", "reference_answer": "Member States hold a key role in the application and enforcement of this Regulation, and each Member State should designate at least one notifying authority and at least one market surveillance authority as national competent authorities for the purpose of supervising the application and implementation of this Regulation.", "reference_context": "Document 148: EN\nUnited in diversity\nEN\n(152)\nIn order to support adequate enforcement as regards AI systems and reinforce the \ncapacities of the Member States, Union AI testing support structures should be \nestablished and made available to the Member States.\n(153)\nMember States hold a key role in the application and enforcement of this Regulation. In \nthat respect, each Member State should designate at least one notifying authority and at \nleast one market surveillance authority as national competent authorities for the purpose \nof supervising the application and implementation of this Regulation. Member States may \ndecide to appoint any kind of public entity to perform the tasks of the national competent \nauthorities within the meaning of this Regulation, in accordance with their specific \nnational organisational characteristics and needs. In order to increase organisation \nefficiency on the side of Member States and to set a single point of contact vis-\u00e0-vis the \npublic and other counterparts at Member State and Union levels, \u258c each Member State \nshould designate a market surveillance authority to act as a single point of contact.\n(154)\nThe national competent authorities should exercise their powers independently, \nimpartially and without bias, so as to safeguard the principles of objectivity of their \nactivities and tasks and to ensure the application and implementation of this Regulation. \nThe members of these authorities should refrain from any action incompatible with their \nduties and should be subject to confidentiality rules under this Regulation.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 148, "situational_context": "Amid the complexities of the EU AI Act, a young lawyer seeks clarity on the role of competent authorities in enforcing regulations to navigate their new responsibilities effectively.", "topic": "AI Regulation and Compliance"}}
{"id": "9fe01b22-e545-46a0-ad35-d86d07e1e145", "question": "As a legal advisor preparing for an upcoming European tour, I'm meticulously researching definitions under the EU AI Act. Can you tell me what is defined as a 'publicly accessible space' according to the Regulation?", "reference_answer": "A publicly accessible space is any physical space that is accessible to an undetermined number of natural persons, regardless of whether it is privately or publicly owned, and irrespective of the activity for which it may be used, such as commerce, services, sport, transport, entertainment, or leisure.", "reference_context": "Document 19: EN\nUnited in diversity\nEN\n(19)\nFor the purposes of this Regulation the notion of \u2018publicly accessible space\u2019 should be \nunderstood as referring to any physical space that is accessible to an undetermined \nnumber of natural persons, and irrespective of whether the space in question is privately \nor publicly owned, irrespective of the activity for which the space may be used, such as \nfor commerce, for example, shops, restaurants, caf\u00e9s; for services, for example, banks, \nprofessional activities, hospitality; for sport, for example, swimming pools, gyms, \nstadiums; for transport, for example, bus, metro and railway stations, airports, means of \ntransport; for entertainment, for example, cinemas, theatres, museums, concert and \nconference halls; or for leisure or otherwise, for example, public roads and squares, \nparks, forests, playgrounds. A space should also be classified as being publicly accessible \nif, regardless of potential capacity or security restrictions, access is subject to certain \npredetermined conditions which can be fulfilled by an undetermined number of persons, \nsuch as the purchase of a ticket or title of transport, prior registration or having a certain \nage. In contrast, a space should not be considered to be publicly accessible if access is \nlimited to specific and defined natural persons through either Union or national law \ndirectly related to public safety or security or through the clear manifestation of will by \nthe person having the relevant authority over the space. The factual possibility of access \nalone, such as an unlocked door or an open gate in a fence, does not imply that the space \nis publicly accessible in the presence of indications or circumstances suggesting the \ncontrary, such as. signs prohibiting or restricting access. Company and factory premises, \nas well as offices and workplaces that are intended to be accessed only by relevant \nemployees and service providers, are spaces that are not publicly accessible. Publicly \naccessible spaces should not include prisons or border control. Some other spaces may \ncomprise both publicly accessible and non-publicly accessible spaces, such as the \nhallway of a private residential building necessary to access a doctor's office or an \nairport. Online spaces are not covered, as they are not physical spaces.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 19, "situational_context": "Before finalizing her plans for an upcoming European tour, a legal advisor meticulously researches the definitions and implications of publicly accessible spaces under the EU AI Act to ensure compliance with local regulations.", "topic": "Biometric Data and AI"}}
{"id": "5ee7d489-9006-4be5-9f81-2fa12005573b", "question": "As a legal intern preparing for a presentation on the EU AI Act, I'm meticulously reviewing the legislative documents. Can you tell me what the basis for the legislative act mentioned in this context is?", "reference_answer": "The basis for the legislative act is the Treaty on the Functioning of the European Union, particularly Articles 16 and 114.", "reference_context": "Document 1: EN\nUnited in diversity\nEN\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 \nand 114 thereof,\nHaving regard to the proposal from the European Commission,\nAfter transmission of the draft legislative act to the national parliaments,\nHaving regard to the opinion of the European Economic and Social Committee1,\nHaving regard to the opinion of the European Central Bank2,\nHaving regard to the opinion of the Committee of the Regions3,\nActing in accordance with the ordinary legislative procedure4,\n1\nOJ C 517, 22.12.2021, p. 56.\n2\n OJ C 115, 11.3.2022, p. 5.\n3\nOJ C 97, 28.2.2022, p. 60.\n4\nPosition of the European Parliament of 13 March 2024.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 1, "situational_context": "Eager to navigate the complexities of the EU AI Act, a legal intern meticulously reviews the legislative documents to ensure compliance and understanding for their upcoming presentation.", "topic": "Others"}}
{"id": "ee69907f-b900-413e-bd32-9c12cc33056d", "question": "As a legal consultant researching the implications of AI systems in migration and asylum management, I'm trying to ensure compliance with EU regulations while advocating for the protection of fundamental rights. Could you explain what the purposes of AI systems intended for use in migration, asylum, and border control management are?", "reference_answer": "AI systems are intended to be used for assessing risks posed by natural persons entering a Member State or applying for visa or asylum, assisting in the examination of applications for asylum, visa, or residence permits, and for detecting, recognizing, or identifying natural persons in the context of migration, asylum, and border control management.", "reference_context": "Document 442: EN\nUnited in diversity\nEN\n7.\nMigration, asylum and border control management, in so far as their use is \npermitted under relevant Union or national law:\n(a)\nAI systems intended to be used by or on behalf of competent public authorities or by \nUnion institutions, bodies, offices or agencies as polygraphs or similar tools;\n(b)\nAI systems intended to be used by or on behalf of competent public authorities or by \nUnion institutions, bodies, offices or agencies to assess a risk, including a security \nrisk, a risk of irregular migration, or a health risk, posed by a natural person who \nintends to enter or who has entered into the territory of a Member State;\n\u258c (c) AI systems intended to be used by or on behalf of competent public authorities or \nby Union institutions, bodies, offices or agencies to assist competent public \nauthorities for the examination of applications for asylum, visa or residence permits \nand for associated complaints with regard to the eligibility of the natural persons \napplying for a status, including related assessments of the reliability of evidence;\n(d)\nAI systems intended to be used by or on behalf of competent public authorities, or \nby Union institutions, bodies, offices or agencies, in the context of migration, \nasylum or border control management, for the purpose of detecting, recognising or \nidentifying natural persons, with the exception of the verification of travel \ndocuments.\n\nDocument 69: EN\nUnited in diversity\nEN\n(60)\nAI systems used in migration, asylum and border control management affect persons who \nare often in particularly vulnerable position and who are dependent on the outcome of the \nactions of the competent public authorities. The accuracy, non-discriminatory nature and \ntransparency of the AI systems used in those contexts are therefore particularly important \nto guarantee respect for the fundamental rights of the affected persons, in particular their \nrights to free movement, non-discrimination, protection of private life and personal data, \ninternational protection and good administration. It is therefore appropriate to classify as \nhigh-risk, insofar as their use is permitted under relevant Union and national law, AI \nsystems intended to be used by or on behalf of competent public authorities or by Union \ninstitutions, bodies, offices or agencies charged with tasks in the fields of migration, \nasylum and border control management as polygraphs and similar tools, for assessing \ncertain risks posed by natural persons entering the territory of a Member State or applying \nfor visa or asylum, for assisting competent public authorities for the examination, \nincluding related assessment of the reliability of evidence, of applications for asylum, visa \nand residence permits and associated complaints with regard to the objective to establish \nthe eligibility of the natural persons applying for a status, for the purpose of detecting, \nrecognising or identifying natural persons in the context of migration, asylum and \nborder control management, with the exception of verification of travel documents.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 442, "situational_context": "A legal consultant is researching the implications of AI systems in migration and asylum management to ensure compliance with EU regulations while advocating for the protection of fundamental rights.", "topic": "AI Regulation and Ethics"}}
{"id": "aae73b4b-72d7-4021-bf61-185f1ec55545", "question": "As a compliance officer for an AI technology firm, I'm meticulously reviewing the validity duration of our AI system certificates under the EU AI Act. Can you tell me what the maximum validity period is for certificates of AI systems covered by Annex I?", "reference_answer": "The maximum validity period for certificates of AI systems covered by Annex I shall not exceed five years.", "reference_context": "Document 284: EN\nUnited in diversity\nEN\n2.\nCertificates shall be valid for the period they indicate, which shall not exceed five years for \nAI systems covered by Annex I, and four years for AI systems covered by Annex III. At \nthe request of the provider, the validity of a certificate may be extended for further periods, \neach not exceeding five years for AI systems covered by Annex I, and four years for AI \nsystems covered by Annex III, based on a re-assessment in accordance with the applicable \nconformity assessment procedures. Any supplement to a certificate shall remain valid, \nprovided that the certificate which it supplements is valid.\n3.\nWhere a notified body finds that an AI system no longer meets the requirements set out in \nSection 2, it shall, taking account of the principle of proportionality, suspend or withdraw \nthe certificate issued or impose restrictions on it, unless compliance with those \nrequirements is ensured by appropriate corrective action taken by the provider of the \nsystem within an appropriate deadline set by the notified body. The notified body shall \ngive reasons for its decision.\n\u258cAn appeal procedure against decisions of the notified bodies, including on conformity \ncertificates issued, shall be available.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 284, "situational_context": "As a compliance officer for an AI technology firm, I am meticulously reviewing the validity duration of our AI system certificates under the EU AI Act to ensure adherence with the latest regulations.", "topic": "Others"}}
{"id": "42254e0f-8d3f-4ac9-b1bd-fc256bac5d00", "question": "As I sit here in the hushed library, trying to understand the EU AI Act, could you explain what the Commission can request from the provider of a general-purpose AI model according to Article 91?", "reference_answer": "The Commission may request the documentation drawn up by the provider in accordance with Articles 53 and 55, or any additional information that is necessary for the purpose of assessing compliance of the provider with this Regulation.", "reference_context": "Document 391: EN\nUnited in diversity\nEN\n(b)\na description of the relevant facts and the reasons for the alert by the scientific \npanel;\n(c)\nany other information that the scientific panel considers to be relevant, including, \nwhere appropriate, information gathered on its own initiative.\nArticle 91\nPower to request documentation and information\n1.\nThe Commission may request the provider of the general-purpose AI model concerned to \nprovide the documentation drawn up by the provider in accordance with Articles 53 and \n55, or any additional information that is necessary for the purpose of assessing \ncompliance of the provider with this Regulation.\n2.\nBefore sending the request for information, the AI Office may initiate a structured \ndialogue with the provider of the general-purpose AI model.\n3.\nUpon a duly substantiated request from the scientific panel, the Commission may issue a \nrequest for information to a provider of a general-purpose AI model, where the access to \ninformation is necessary and proportionate for the fulfilment of the tasks of the \nscientific panel under Article 68(2).", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 391, "situational_context": "In the hushed library, a diligent researcher pores over the intricacies of the EU AI Act, eager to clarify the information requisites for compliance amid the evolving landscape of artificial intelligence regulations.", "topic": "AI Regulation and Governance"}}
{"id": "ef9fb007-c78c-45b7-a0f7-5bc909ca00b5", "question": "As a legal professional meticulously reviewing the EU AI Act documents for compliance requirements regarding high-risk AI systems, could you clarify the timeframe for the market surveillance authority to take appropriate measures after they receive a notification of a serious incident?", "reference_answer": "The market surveillance authority shall take appropriate measures within seven days from the date it received the notification.", "reference_context": "Document 361: EN\nUnited in diversity\nEN\n7.\nUpon receiving a notification related to a serious incident referred to in Article 3, point \n(49)(c), the relevant market surveillance authority shall inform the national public \nauthorities or bodies referred to in Article 77(1). The Commission shall develop dedicated \nguidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. \nThat guidance shall be issued by \u2026 [12 months after the entry into force of this \nRegulation], and shall be assessed regularly.\n8.\nThe market surveillance authority shall take appropriate measures, as provided for in \nArticle 19 of Regulation (EU) 2019/1020, within seven days from the date it received the \nnotification referred to in paragraph 1 of this Article, and shall follow the notification \nprocedures as provided in that Regulation.\n9.\nFor high-risk AI systems referred to in \u258c Annex III that are placed on the market or put \ninto service by providers that are subject to Union legislative instruments laying down \nreporting obligations equivalent to those set out in this Regulation \u258c , the notification of \nserious incidents shall be limited to those referred to in Article 3, point (49)(c).\n10.\nFor high-risk AI systems which are safety components of devices, or are themselves \ndevices, covered by Regulations (EU) 2017/745 and (EU) 2017/746, the notification of \nserious incidents shall be limited to those referred to in Article 3, point (49)(c) of this \nRegulation, and shall be made to the national competent authority chosen for that \npurpose by the Member States where the incident occurred.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 361, "situational_context": "Seeking clarity on compliance requirements for high-risk AI systems, a legal professional meticulously reviews the EU AI Act documents.", "topic": "AI Regulation and Compliance"}}
{"id": "39c5e036-ec9f-43eb-862b-848db3825219", "question": "As a compliance officer feeling overwhelmed by the complexities of EU regulations, could you clarify what actions a market surveillance authority must take if it finds an AI system non-compliant with the Regulation?", "reference_answer": "The market surveillance authority shall require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a period the market surveillance authority may prescribe, and in any event within the shorter of 15 working days, or as provided for in the relevant Union harmonisation legislation.", "reference_context": "Document 368: EN\nUnited in diversity\nEN\n2.\nWhere the relevant market surveillance authorities have sufficient reason to consider \ngeneral-purpose AI systems that can be used directly by deployers for at least one \npurpose that is classified as high-risk pursuant to this Regulation to be non-compliant \nwith the requirements laid down in this Regulation, they shall cooperate with the AI \nOffice to carry out compliance evaluations, and shall inform the Board and other market \nsurveillance authorities accordingly.\n3.\nWhere a market surveillance authority is unable to conclude its investigation of the \nhigh-risk AI system because of its inability to access certain information related to the \ngeneral-purpose AI model despite having made all appropriate efforts to obtain that \ninformation, it may submit a reasoned request to the AI Office, by which access to that \ninformation shall be enforced. In that case, the AI Office shall supply to the applicant \nauthority without delay, and in any event within 30 days, any information that the AI \nOffice considers to be relevant in order to establish whether a high-risk AI system is \nnon-compliant. Market surveillance authorities shall safeguard the confidentiality of the \ninformation that they obtain in accordance with Article 78 of this Regulation. The \nprocedure provided for in Chapter VI of Regulation (EU) 2019/1020 shall apply mutatis \nmutandis.\n\nDocument 376: EN\nUnited in diversity\nEN\n2.\nWhere the market surveillance authority of a Member State has sufficient reason to \nconsider an AI system to present a risk as referred to in paragraph 1 of this Article, it shall \ncarry out an evaluation of the AI system concerned in respect of its compliance with all the \nrequirements and obligations laid down in this Regulation. Particular attention shall be \ngiven to AI systems presenting a risk to vulnerable groups. Where risks to fundamental \nrights are identified, the market surveillance authority shall also inform and fully \ncooperate with the relevant national public authorities or bodies referred to in Article \n77(1). The relevant operators shall cooperate as necessary with the market surveillance \nauthority and with the other national public authorities or bodies referred to in Article \n77(1).\nWhere, in the course of that evaluation, the market surveillance authority or, where \napplicable the market surveillance authority in cooperation with the national public \nauthority referred to in Article 77(1), finds that the AI system does not comply with the \nrequirements and obligations laid down in this Regulation, it shall without undue delay \nrequire the relevant operator to take all appropriate corrective actions to bring the AI \nsystem into compliance, to withdraw the AI system from the market, or to recall it within a \nperiod the market surveillance authority may prescribe, and in any event within the \nshorter of 15 working days, or as provided for in the relevant Union harmonisation \nlegislation.\nThe market surveillance authority shall inform the relevant notified body accordingly. \nArticle 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the \nsecond subparagraph of this paragraph.\n3.\nWhere the market surveillance authority considers that the non-compliance is not restricted \nto its national territory, it shall inform the Commission and the other Member States \nwithout undue delay of the results of the evaluation and of the actions which it has \nrequired the operator to take.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 368, "situational_context": "Frustrated by the complexities of EU regulations, a compliance officer anxiously seeks clarity on high-risk AI systems to ensure their organization's adherence to the new legal landscape.", "topic": "Others"}}
{"id": "775fe5a9-e30f-460b-a744-f86e7738d049", "question": "As I prepare for a conference on AI regulation in Europe, I'm reviewing documents related to the EU AI Act. Could you explain what is a key characteristic that distinguishes AI systems from traditional software systems?", "reference_answer": "A key characteristic of AI systems is their capability to infer.", "reference_context": "Document 13: EN\nUnited in diversity\nEN\n(12)\nThe notion of \u2018AI system\u2019 in this Regulation should be clearly defined and should be \nclosely aligned with the work of international organisations working on AI to ensure \nlegal certainty, facilitate international convergence and wide acceptance, while providing \nthe flexibility to accommodate the rapid technological developments in this field. \nMoreover, the definition should be based on key characteristics of AI systems that \ndistinguish it from simpler traditional software systems or programming approaches and \nshould not cover systems that are based on the rules defined solely by natural persons to \nautomatically execute operations. A key characteristic of AI systems is their capability to \ninfer. This capability to infer refers to the process of obtaining the outputs, such as \npredictions, content, recommendations, or decisions, which can influence physical and \nvirtual environments, and to a capability of AI systems to derive models or algorithms, or \nboth, from inputs or data. The techniques that enable inference while building an AI \nsystem include machine learning approaches that learn from data how to achieve certain \nobjectives, and logic- and knowledge-based approaches that infer from encoded \nknowledge or symbolic representation of the task to be solved. The capacity of an AI \nsystem to infer transcends basic data processing by enabling learning, reasoning or \nmodelling. The term \u2018machine-based\u2019 refers to the fact that AI systems run on machines.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 13, "situational_context": "In preparation for a conference on AI regulation in Europe, a legal expert meticulously reviews documents to understand the definition and implications of AI systems under the EU AI Act.", "topic": "AI Regulation and Ethics"}}
{"id": "7dc4e7ec-bbcf-4776-ab8d-62979a89c737", "question": "As a legal advisor reviewing the EU AI Act, could you clarify what actions the AI Office might take before it requests access to a general-purpose AI model?", "reference_answer": "The AI Office may initiate a structured dialogue with the provider of the general-purpose AI model to gather more information on the internal testing of the model, internal safeguards for preventing systemic risks, and other internal procedures and measures the provider has taken to mitigate such risks.", "reference_context": "Document 394: EN\nUnited in diversity\nEN\n6.\nThe Commission shall adopt implementing acts setting out the detailed arrangements \nand the conditions for the evaluations, including the detailed arrangements for involving \nindependent experts, and the procedure for the selection thereof. Those implementing \nacts shall be adopted in accordance with the examination procedure referred to in \nArticle 98(2).\n7.\nPrior to requesting access to the general-purpose AI model concerned, the AI Office may \ninitiate a structured dialogue with the provider of the general-purpose AI model to \ngather more information on the internal testing of the model, internal safeguards for \npreventing systemic risks, and other internal procedures and measures the provider has \ntaken to mitigate such risks.\nArticle 93\nPower to request measures\n1.\nWhere necessary and appropriate, the Commission may request providers to:\n(a)\ntake appropriate measures to comply with the obligations set out in Articles 53 and \n54;", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 394, "situational_context": "As a legal advisor reviewing the EU AI Act, an expert is seeking clarification on the procedures for engaging independent evaluators during the assessment process.", "topic": "AI Regulation and Governance"}}
{"id": "64445c5b-7e35-45d2-8daf-59ea2434b2b6", "question": "As a legal expert preparing a presentation on AI compliance for a conference, I need to know what rights and remedies remain unaffected by the Regulation regarding AI systems. Can you clarify this for me?", "reference_answer": "All rights and remedies provided for by Union law to consumers and other persons on whom AI systems may have a negative impact remain unaffected and fully applicable.", "reference_context": "Document 8: EN\nUnited in diversity\nEN\nAs a consequence, all rights and remedies provided for by such Union law to consumers, \nand other persons on whom AI systems may have a negative impact, including as \nregards the compensation of possible damages pursuant to Council Directive \n85/374/EEC10 remain unaffected and fully applicable. Furthermore, in the context of \nemployment and protection of workers, this Regulation should therefore not affect \nUnion law on social policy and national labour law, in compliance with Union law, \nconcerning employment and working conditions, including health and safety at work \nand the relationship between employers and workers. This Regulation should also not \naffect the exercise of fundamental rights as recognised in the Member States and at \nUnion level, including the right or freedom to strike or to take other action covered by \nthe specific industrial relations systems in Member States as well as the right to \nnegotiate, to conclude and enforce collective agreements or to take collective action in \naccordance with national law. \n10\nCouncil Directive 85/374/EEC of 25 July 1985 on the approximation of the laws, \nregulations and administrative provisions of the Member States concerning liability for \ndefective products (OJ L 210, 7.8.1985, p. 29).", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 8, "situational_context": "A legal expert is seeking clarification on how the new EU AI Act impacts consumer rights and labor regulations while preparing a presentation for a conference on AI compliance.", "topic": "AI Regulation and Compliance"}}
{"id": "14b5786d-f7ad-4a6a-8389-a976c0097fee", "question": "While I\u2019m sitting in this bustling Brussels caf\u00e9, I\u2019m trying to understand the significance of Regulation (EU) 2024/\u2026 in relation to the artificial intelligence systems I\u2019m working on for my upcoming project. Can you explain it?", "reference_answer": "Regulation (EU) 2024/\u2026 lays down harmonised rules on artificial intelligence and requires that when adopting delegated acts concerning Artificial Intelligence systems which are safety components, the requirements set out in Chapter III, Section 2 of that Regulation shall be taken into account.", "reference_context": "Document 416: EN\nUnited in diversity\nEN\nArticle 106\nAmendment to Directive (EU) 2016/797\nIn Article 5 of Directive (EU) 2016/797, the following paragraph is added:\n\u201812.\nWhen adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to \nparagraph 11 concerning Artificial Intelligence systems which are safety components \nwithin the meaning of Regulation (EU) 2024/\u2026 of the European Parliament and of the \nCouncil*+, the requirements set out in Chapter III, Section 2, of that Regulation shall be \ntaken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 417: EN\nUnited in diversity\nEN\nArticle 107\nAmendment to Regulation (EU) 2018/858\nIn Article 5 of Regulation (EU) 2018/858 the following paragraph is added:\n\u20184.\nWhen adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence \nsystems which are safety components within the meaning of Regulation (EU) 2024/\u2026 of \nthe European Parliament and of the Council*+, the requirements set out in Chapter III, \nSection 2, of that Regulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 420: EN\nUnited in diversity\nEN\n(6)\nin Article 58, the following paragraph is added:\n\u20183.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019.\nArticle 109\nAmendment to Regulation (EU) 2019/2144\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\u20183.\nWhen adopting the implementing acts pursuant to paragraph 2, concerning artificial \nintelligence systems which are safety components within the meaning of Regulation (EU) \n2024/\u2026 of the European Parliament and of the Council*++, the requirements set out in \nChapter III, Section 2, of that Regulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n++\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 414: EN\nUnited in diversity\nEN\nArticle 104\nAmendment to Regulation (EU) No 168/2013\nIn Article 22(5) of Regulation (EU) No 168/2013, the following subparagraph is added:\n\u2018When adopting delegated acts pursuant to the first subparagraph concerning Artificial Intelligence \nsystems which are safety components within the meaning of Regulation (EU) 2024/\u2026 of the \nEuropean Parliament and of the Council*+, the requirements set out in Chapter III, Section 2, of that \nRegulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 416, "situational_context": "In a bustling Brussels caf\u00e9, a legal expert meticulously compares the latest amendments of the EU AI Act to ensure compliance in their upcoming project on artificial intelligence systems.", "topic": "Artificial Intelligence Regulation"}}
{"id": "599800bd-8058-4dd9-895a-aa583cec57c5", "question": "As a lawyer reviewing the new EU AI Act, I'm considering the necessary steps to ensure my client's compliance. What can the AI Office do prior to requesting access to a general-purpose AI model?", "reference_answer": "The AI Office may initiate a structured dialogue with the provider of the general-purpose AI model to gather more information on the internal testing of the model, internal safeguards for preventing systemic risks, and other internal procedures and measures the provider has taken to mitigate such risks.", "reference_context": "Document 394: EN\nUnited in diversity\nEN\n6.\nThe Commission shall adopt implementing acts setting out the detailed arrangements \nand the conditions for the evaluations, including the detailed arrangements for involving \nindependent experts, and the procedure for the selection thereof. Those implementing \nacts shall be adopted in accordance with the examination procedure referred to in \nArticle 98(2).\n7.\nPrior to requesting access to the general-purpose AI model concerned, the AI Office may \ninitiate a structured dialogue with the provider of the general-purpose AI model to \ngather more information on the internal testing of the model, internal safeguards for \npreventing systemic risks, and other internal procedures and measures the provider has \ntaken to mitigate such risks.\nArticle 93\nPower to request measures\n1.\nWhere necessary and appropriate, the Commission may request providers to:\n(a)\ntake appropriate measures to comply with the obligations set out in Articles 53 and \n54;", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 394, "situational_context": "A lawyer reviewing the new EU AI Act ponders the necessary steps to ensure their client's compliance with the regulations regarding independent evaluations and systemic risk mitigation.", "topic": "AI Regulation and Governance"}}
{"id": "ba1b4038-09bb-4d52-8f35-3f72b82c415e", "question": "As a policy researcher intrigued by the legal intricacies of the EU AI Act, I'm trying to understand what conditions must be fulfilled for an AI system in Annex III to not be considered high-risk?", "reference_answer": "An AI system referred to in Annex III shall not be considered to be high-risk where it does not pose a significant risk of harm to health, safety, or fundamental rights of natural persons, including by not materially influencing the outcome of decision making, if it is intended to perform a narrow procedural task, improve the result of a previously completed human activity, detect decision-making patterns without replacing or influencing the previous human assessment, or perform a preparatory task relevant for the use cases listed in Annex III.", "reference_context": "Document 203: EN\nUnited in diversity\nEN\n2.\nIn addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in \nAnnex III shall be considered to be high-risk.\n3.\nBy derogation from paragraph 2, an AI system referred to in Annex III shall not be \nconsidered to be high-risk where it does not pose a significant risk of harm to the health, \nsafety or fundamental rights of natural persons, including by not materially influencing \nthe outcome of decision making. \nThe first subparagraph shall apply where any of the following conditions is fulfilled:\n(a)\nthe AI system is intended to perform a narrow procedural task;\n(b)\nthe AI system is intended to improve the result of a previously completed human \nactivity;\n(c)\nthe AI system is intended to detect decision-making patterns or deviations from \nprior decision-making patterns and is not meant to replace or influence the \npreviously completed human assessment, without proper human review; or\n(d)\nthe AI system is intended to perform a preparatory task to an assessment relevant \nfor the purposes of the use cases listed in Annex III.\nNotwithstanding the first subparagraph, an AI system referred to in Annex III shall \nalways be considered to be high-risk where the AI system performs profiling of natural \npersons.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 203, "situational_context": "Intrigued by the legal intricacies of the EU AI Act, a policy researcher delves into the definitions and classifications of high-risk AI systems to better navigate their implications for future developments in the field.", "topic": "Others"}}
{"id": "e64c108b-4b41-47b7-8578-c39938e146cc", "question": "As a regulatory officer reviewing legal documents to ensure compliance with the new EU AI Act, I'm curious about what powers market surveillance authorities have regarding testing in real world conditions?", "reference_answer": "Market surveillance authorities shall have competences and powers to ensure that testing in real world conditions is in accordance with this Regulation, including requiring providers to provide information, carrying out unannounced remote or on-site inspections, and performing checks on the conduct of the testing.", "reference_context": "Document 369: EN\nUnited in diversity\nEN\nArticle 76\nSupervision of testing in real world conditions by market surveillance authorities\n1.\nMarket surveillance authorities shall have competences and powers to ensure that \ntesting in real world conditions is in accordance with this Regulation.\n2.\nWhere testing in real world conditions is conducted for AI systems that are supervised \nwithin an AI regulatory sandbox under Article 58, the market surveillance authorities \nshall verify the compliance with Article 60 as part of their supervisory role for the AI \nregulatory sandbox. Those authorities may, as appropriate, allow the testing in real \nworld conditions to be conducted by the provider or prospective provider, in derogation \nfrom the conditions set out in Article 60(4), points (f) and (g).\n3.\nWhere a market surveillance authority has been informed by the prospective provider, \nthe provider or any third party of a serious incident or has other grounds for considering \nthat the conditions set out in Articles 60 and 61 are not met, it may take either of the \nfollowing decisions on its territory, as appropriate:\n(a)\nto suspend or terminate the testing in real world conditions;\n\nDocument 333: EN\nUnited in diversity\nEN\n(j)\nthe testing in real world conditions is effectively overseen by the provider or \nprospective provider, as well as by deployers or prospective deployers through \npersons who are suitably qualified in the relevant field and have the necessary \ncapacity, training and authority to perform their tasks;\n(k)\nthe predictions, recommendations or decisions of the AI system can be effectively \nreversed and disregarded.\n5.\nAny subjects of the testing in real world conditions, or their legally designated \nrepresentative, as appropriate, may, without any resulting detriment and without having \nto provide any justification, withdraw from the testing at any time by revoking their \ninformed consent and may request the immediate and permanent deletion of their \npersonal data. The withdrawal of the informed consent shall not affect the activities \nalready carried out.\n6.\nIn accordance with Article 75, Member States shall confer on their market surveillance \nauthorities the powers of requiring providers and prospective providers to provide \ninformation, of carrying out unannounced remote or on-site inspections, and of \nperforming checks on the conduct of the testing in real world conditions and the related \nhigh-risk AI systems. Market surveillance authorities shall use those powers to ensure \nthe safe development of testing in real world conditions.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 369, "situational_context": "Seeking clarity on the compliance requirements for testing AI systems, a regulatory officer pores over legal documents to ensure adherence to the new EU AI Act.", "topic": "AI System Testing Regulations"}}
{"id": "6265ba82-1f9c-47a8-8d3a-fe923942fca5", "question": "As I prepare my presentation in this bustling conference room in Brussels, could you clarify what the role of national competent authorities is according to Article 70 of the EU AI Act?", "reference_answer": "National competent authorities shall exercise their powers independently, impartially and without bias to safeguard the objectivity of their activities and tasks, and to ensure the application and implementation of this Regulation.", "reference_context": "Document 352: EN\nUnited in diversity\nEN\nSection 2\nNational competent authorities\nArticle 70\nDesignation of national competent authorities and single points of contact\n\u258c\n1.\nEach Member State shall establish or designate as national competent authorities at least \none notifying authority and at least one market surveillance authority for the purposes of \nthis Regulation. Those national competent authorities shall exercise their powers \nindependently, impartially and without bias so as to safeguard the objectivity of their \nactivities and tasks, and to ensure the application and implementation of this Regulation. \nThe members of those authorities shall refrain from any action incompatible with their \nduties. Provided that those principles are observed, such activities and tasks may be \nperformed by one or more designated authorities, in accordance with the organisational \nneeds of the Member State.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 352, "situational_context": "In a bustling conference room in Brussels, a legal expert meticulously reviews the EU AI Act, pondering the implications of Article 70 on national competent authorities while drafting a presentation that clarifies their roles and responsibilities.", "topic": "AI Regulation and Compliance"}}
{"id": "207f58ae-dbcd-4a45-a410-d0a96654fe36", "question": "As a legal advisor reviewing the EU AI Act to ensure compliance with its regulations on independence and accurate representation, could you clarify what tasks the scientific panel will support the AI Office with?", "reference_answer": "The scientific panel shall advise and support the AI Office, particularly by supporting the implementation and enforcement of this Regulation regarding general-purpose AI models and systems.", "reference_context": "Document 349: EN\nUnited in diversity\nEN\n(b)\nindependence from any provider of AI systems or general-purpose AI models;\n(c)\nan ability to carry out activities diligently, accurately and objectively. \nThe Commission, in consultation with the Board, shall determine the number of experts \non the panel in accordance with the required needs and shall ensure fair gender \nand geographical representation.\n3.\nThe scientific panel shall advise and support the AI Office, in particular with regard to \nthe following tasks:\n(a)\nsupporting the implementation and enforcement of this Regulation as regards \ngeneral-purpose AI models and systems, in particular by:\n(i)\nalerting the AI Office of possible systemic risks at Union level of general-\npurpose AI models, in accordance with Article 90;\n(ii)\ncontributing to the development of tools and methodologies for evaluating \ncapabilities of general-purpose AI models and systems, including through \nbenchmarks;", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 349, "situational_context": "Concerned about compliance, a legal advisor is reviewing the EU AI Act to ensure that their implementation aligns with the regulation\u2019s emphasis on independence and accurate representation.", "topic": "AI Regulation and Governance"}}
{"id": "cc8f2302-1362-4aae-a110-da37d8c8b557", "question": "As a legal consultant preparing for a client meeting about compliance with the new EU regulations, could you explain what is a key characteristic that distinguishes AI systems from traditional software systems?", "reference_answer": "A key characteristic of AI systems is their capability to infer.", "reference_context": "Document 13: EN\nUnited in diversity\nEN\n(12)\nThe notion of \u2018AI system\u2019 in this Regulation should be clearly defined and should be \nclosely aligned with the work of international organisations working on AI to ensure \nlegal certainty, facilitate international convergence and wide acceptance, while providing \nthe flexibility to accommodate the rapid technological developments in this field. \nMoreover, the definition should be based on key characteristics of AI systems that \ndistinguish it from simpler traditional software systems or programming approaches and \nshould not cover systems that are based on the rules defined solely by natural persons to \nautomatically execute operations. A key characteristic of AI systems is their capability to \ninfer. This capability to infer refers to the process of obtaining the outputs, such as \npredictions, content, recommendations, or decisions, which can influence physical and \nvirtual environments, and to a capability of AI systems to derive models or algorithms, or \nboth, from inputs or data. The techniques that enable inference while building an AI \nsystem include machine learning approaches that learn from data how to achieve certain \nobjectives, and logic- and knowledge-based approaches that infer from encoded \nknowledge or symbolic representation of the task to be solved. The capacity of an AI \nsystem to infer transcends basic data processing by enabling learning, reasoning or \nmodelling. The term \u2018machine-based\u2019 refers to the fact that AI systems run on machines.", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 13, "situational_context": "A legal consultant is searching for clear definitions and guidelines regarding AI systems to ensure compliance with the new EU regulations while preparing for an upcoming client meeting.", "topic": "AI Regulation and Ethics"}}
{"id": "35652fdf-ee67-4df3-a3b5-9e94b3272364", "question": "Hello, as a legal researcher diving into the latest documents on data governance under the EU AI Act, I'm trying to clarify what characteristics training, validation, and testing data sets should have for high-risk AI systems.", "reference_answer": "Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used.", "reference_context": "Document 77: EN\nUnited in diversity\nEN\n(67)\nHigh-quality data and access to high-quality data plays a vital role in providing structure \nand in ensuring the performance of many AI systems, especially when techniques \ninvolving the training of models are used, with a view to ensure that the high-risk AI \nsystem performs as intended and safely and it does not become a source of discrimination \nprohibited by Union law. High-quality data sets for training, validation and testing \nrequire the implementation of appropriate data governance and management practices. \nData sets for training, validation and testing, including the labels, should be relevant, \nsufficiently representative, and to the best extent possible free of errors and complete in \nview of the intended purpose of the system. In order to facilitate compliance with Union \ndata protection law, such as Regulation (EU) 2016/679, data governance and \nmanagement practices should include, in the case of personal data, transparency about \nthe original purpose of the data collection. The data sets should also have the appropriate \nstatistical properties, including as regards the persons or groups of persons in relation to \nwhom the high-risk AI system is intended to be used, with specific attention to the \nmitigation of possible biases in the data sets, that are likely to affect the health and safety \nof persons, have a negative impact on fundamental rights or lead to discrimination \nprohibited under Union law, especially where data outputs influence inputs for future \noperations (feedback loops). Biases can for example be inherent in underlying data sets, \nespecially when historical data is being used, or generated when the systems are \nimplemented in real world settings.\n\nDocument 216: EN\nUnited in diversity\nEN\n3.\nTraining, validation and testing data sets shall be relevant, sufficiently representative, and \nto the best extent possible, free of errors and complete in view of the intended purpose. \nThey shall have the appropriate statistical properties, including, where applicable, as \nregards the persons or groups of persons in relation to whom the high-risk AI system is \nintended to be used. Those characteristics of the data sets may be met at the level of \nindividual data sets or at the level of a combination thereof.\n4.\nData sets shall take into account, to the extent required by the intended purpose, the \ncharacteristics or elements that are particular to the specific geographical, contextual, \nbehavioural or functional setting within which the high-risk AI system is intended to be \nused.\n5.\nTo the extent that it is strictly necessary for the purpose of ensuring bias \u258c detection and \ncorrection in relation to the high-risk AI systems in accordance with paragraph (2), points \n(f) and (g) of this Article, the providers of such systems may exceptionally process special \ncategories of personal data, subject to appropriate safeguards for the fundamental rights \nand freedoms of natural persons. In addition to the provisions set out in Regulations (EU) \n2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, all the following conditions \nmust be met in order for such processing to occur:\n(a)\nthe bias detection and correction cannot be effectively fulfilled by processing other \ndata, including synthetic or anonymised data;", "conversation_history": [], "metadata": {"question_type": "situational", "seed_document_id": 77, "situational_context": "With a keen interest in understanding data governance under the EU AI Act, a legal researcher meticulously combs through the latest documents, hoping to clarify the requirements for training and validation datasets.", "topic": "AI Regulation and Ethics"}}
{"id": "60cbf293-627d-4f27-b152-33203eda0088", "question": "What is the access policy for SMEs regarding AI regulatory sandboxes and how do these sandboxes support compliance with conformity assessment obligations?", "reference_answer": "Access to AI regulatory sandboxes is free of charge for SMEs, including start-ups, while these sandboxes help providers comply with conformity assessment obligations through their learning outcomes.", "reference_context": "Document 322: EN\nUnited in diversity\nEN\n(b)\nthat AI regulatory sandboxes allow broad and equal access and keep up with \ndemand for participation; providers and prospective providers may also submit \napplications in partnerships with deployers and other relevant third parties;\n(c)\nthat the detailed arrangements for, and conditions concerning AI regulatory \nsandboxes support, to the best extent possible, flexibility for national competent \nauthorities to establish and operate their AI regulatory sandboxes;\n(d)\nthat access to the AI regulatory sandboxes is free of charge for SMEs, including \nstart-ups, without prejudice to exceptional costs that national competent \nauthorities may recover in a fair and proportionate manner;\n(e)\nthat they facilitate providers and prospective providers, by means of the learning \noutcomes of the AI regulatory sandboxes, in complying with conformity \nassessment obligations under this Regulation and the voluntary application of the \ncodes of conduct referred to in Article 95;\n(f)\nthat AI regulatory sandboxes facilitate the involvement of other relevant actors \nwithin the AI ecosystem, such as notified bodies and standardisation organisations, \nSMEs, including start-ups, enterprises, innovators, testing and experimentation \nfacilities, research and experimentation labs and European Digital Innovation \nHubs, centres of excellence, individual researchers, in order to allow and facilitate \ncooperation with the public and private sectors;", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is the access policy for SMEs regarding AI regulatory sandboxes?", "answer": "Access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice to exceptional costs that national competent authorities may recover in a fair and proportionate manner."}, {"question": "How do AI regulatory sandboxes support compliance with conformity assessment obligations?", "answer": "AI regulatory sandboxes facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatory sandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary application of the codes of conduct referred to in Article 95."}], "seed_document_id": 322, "topic": "AI Regulatory Sandboxes"}}
{"id": "7debb7c2-25f7-4cef-842b-39a9b15422c0", "question": "What is the purpose of a fundamental rights impact assessment for high-risk AI systems and who is required to perform it?", "reference_answer": "The purpose of the fundamental rights impact assessment is to identify specific risks to individuals' rights and measures to mitigate those risks, and it must be performed by deployers governed by public law, private entities providing public services, and those deploying certain high-risk AI systems listed in the Regulation.", "reference_context": "Document 99: EN\nUnited in diversity\nEN\n(96)\nIn order to efficiently ensure that fundamental rights are protected, deployers of high-\nrisk AI systems that are bodies governed by public law, or private entities providing \npublic services and deployers of certain high-risk AI systems listed in an annex to this \nRegulation, such as banking or insurance entities, should carry out a fundamental \nrights impact assessment prior to putting it into use. Services important for individuals \nthat are of public nature may also be provided by private entities. Private entities \nproviding such public services are linked to tasks in the public interest such as in the \nareas of education, healthcare, social services, housing, administration of justice. The \naim of the fundamental rights impact assessment is for the deployer to identify the \nspecific risks to the rights of individuals or groups of individuals likely to be affected, \nidentify measures to be taken in the case of a materialisation of those risks. The impact \nassessment should be performed prior to deploying the high-risk AI system, and should \nbe updated when the deployer considers that any of the relevant factors have changed. \nThe impact assessment should identify the deployer\u2019s relevant processes in which the \nhigh-risk AI system will be used in line with its intended purpose, and should include a \ndescription of the period of time and frequency in which the system is intended to be \nused as well as of specific categories of natural persons and groups who are likely to be \naffected in the specific context of use.\n\nDocument 255: EN\nUnited in diversity\nEN\nArticle 27\nFundamental rights impact assessment for high-risk AI systems\n1.\nPrior to deploying a high-risk AI system referred to in Article 6(2), with the exception of \nhigh-risk AI systems intended to be used in the area listed in point 2 of Annex III, \ndeployers that are bodies governed by public law, or are private entities providing public \nservices, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of \nAnnex III, shall perform an assessment of the impact on fundamental rights that the use \nof such system may produce. For that purpose, deployers shall perform an assessment \nconsisting of:\n(a)\na description of the deployer\u2019s processes in which the high-risk AI system will be \nused in line with its intended purpose;\n(b)\na description of the period of time within which, and the frequency with which, \neach high-risk AI system is intended to be used;\n(c)\nthe categories of natural persons and groups likely to be affected by its use in the \nspecific context;\n\nDocument 100: EN\nUnited in diversity\nEN\nThe assessment should also include the identification of specific risks of harm likely to \nhave an impact on the fundamental rights of those persons or groups. While performing \nthis assessment, the deployer should take into account information relevant to a proper \nassessment of the impact, including but not limited to the information given by the \nprovider of the high-risk AI system in the instructions for use. In light of the risks \nidentified, deployers should determine measures to be taken in the case of a \nmaterialisation of those risks, including for example governance arrangements in that \nspecific context of use, such as arrangements for human oversight according to the \ninstructions of use or, complaint handling and redress procedures, as they could be \ninstrumental in mitigating risks to fundamental rights in concrete use-cases. After \nperforming that impact assessment, the deployer should notify the relevant market \nsurveillance authority. Where appropriate, to collect relevant information necessary to \nperform the impact assessment, deployers of high-risk AI system, in particular when AI \nsystems are used in the public sector, could involve relevant stakeholders, including the \nrepresentatives of groups of persons likely to be affected by the AI system, independent \nexperts, and civil society organisations in conducting such impact assessments and \ndesigning measures to be taken in the case of materialisation of the risks. The European \nArtificial Intelligence Office (AI Office) should develop a template for a questionnaire in \norder to facilitate compliance and reduce the administrative burden for deployers.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is the purpose of a fundamental rights impact assessment for high-risk AI systems?", "answer": "The aim of the fundamental rights impact assessment is for the deployer to identify the specific risks to the rights of individuals or groups of individuals likely to be affected, and to identify measures to be taken in the case of a materialisation of those risks."}, {"question": "Who is required to perform a fundamental rights impact assessment before deploying a high-risk AI system?", "answer": "Deployers that are bodies governed by public law, or are private entities providing public services, and deployers of certain high-risk AI systems listed in an annex to the Regulation are required to perform an assessment."}], "seed_document_id": 99, "topic": "Others"}}
{"id": "4bbbec22-915c-41f2-bbb3-a632325c9c59", "question": "What can the scientific panel do if it suspects a general-purpose AI model poses a risk, and what must a qualified alert indicate according to Article 90?", "reference_answer": "The scientific panel may provide a qualified alert to the AI Office if it suspects a general-purpose AI model poses a concrete identifiable risk, and this alert must be duly reasoned and indicate at least the point of contact of the provider of the model with the systemic risk concerned.", "reference_context": "Document 390: EN\nUnited in diversity\nEN\nArticle 90\nAlerts of systemic risks by the scientific panel\n1.\nThe scientific panel may provide a qualified alert to the AI Office where it has reason to \nsuspect that:\n(a)\na general-purpose AI model poses concrete identifiable risk at Union level; or,\n(b)\na general-purpose AI model meets the conditions referred to in Article 51.\n2.\nUpon such qualified alert, the Commission, through the AI Office and after having \ninformed the Board, may exercise the powers laid down in this Section for the purpose of \nassessing the matter. The AI Office shall inform the Board of any measure according to \nArticles 91 to 94.\n3.\nA qualified alert shall be duly reasoned and indicate at least:\n(a)\nthe point of contact of the provider of the general-purpose AI model with systemic \nrisk concerned;", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What can the scientific panel do if it suspects a general-purpose AI model poses a risk?", "answer": "The scientific panel may provide a qualified alert to the AI Office where it has reason to suspect that a general-purpose AI model poses concrete identifiable risk at Union level."}, {"question": "What must a qualified alert indicate according to Article 90?", "answer": "A qualified alert shall be duly reasoned and indicate at least the point of contact of the provider of the general-purpose AI model with systemic risk concerned."}], "seed_document_id": 390, "topic": "Systemic Risks in AI"}}
{"id": "90dfa2d5-4ed0-444b-bb5e-f96b36e4deb2", "question": "What is the purpose of Directive 2006/42/EC and which legislation addresses the safety of toys?", "reference_answer": "Directive 2006/42/EC focuses on machinery and amends Directive 95/16/EC, while Directive 2009/48/EC addresses the safety of toys.", "reference_context": "Document 430: EN\nUnited in diversity\nEN\nANNEX I\nList of Union harmonisation legislation\nSection A. List of Union harmonisation legislation based on the New Legislative Framework\n1.\nDirective 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on \nmachinery, and amending Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24) [as repealed by \nthe Machinery Regulation];\n2.\nDirective 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on \nthe safety of toys (OJ L 170, 30.6.2009, p. 1);\n3.\nDirective 2013/53/EU of the European Parliament and of the Council of 20 November \n2013 on recreational craft and personal watercraft and repealing Directive 94/25/EC (OJ L \n354, 28.12.2013, p. 90);\n4.\nDirective 2014/33/EU of the European Parliament and of the Council of 26 February 2014 \non the harmonisation of the laws of the Member States relating to lifts and safety \ncomponents for lifts (OJ L 96, 29.3.2014, p. 251);\n5.\nDirective 2014/34/EU of the European Parliament and of the Council of 26 February 2014 \non the harmonisation of the laws of the Member States relating to equipment and \nprotective systems intended for use in potentially explosive atmospheres (OJ L 96, \n29.3.2014, p. 309);", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is the purpose of Directive 2006/42/EC?", "answer": "Directive 2006/42/EC is focused on machinery and amends Directive 95/16/EC."}, {"question": "What legislation addresses the safety of toys?", "answer": "Directive 2009/48/EC of the European Parliament and of the Council addresses the safety of toys."}], "seed_document_id": 430, "topic": "EU Regulations and Directives"}}
{"id": "c4f1f10c-d8e0-46fe-9f3a-2fce6aaf3e2b", "question": "When should the prohibitions and general provisions of this Regulation apply, and what is the timeline for the obligations for providers of general-purpose AI models?", "reference_answer": "The prohibitions and general provisions of this Regulation should apply six months after its entry into force, while obligations for providers of general-purpose AI models should apply 12 months after.", "reference_context": "Document 170: EN\nUnited in diversity\nEN\n(179)\nThis Regulation should apply from \u2026 [two years from the date of entry into force of this \nRegulation]. However, taking into account the unacceptable risk associated with the use \nof AI in certain ways, the prohibitions as well as the general provisions of this \nRegulation should already apply from \u2026 [six months from the date of entry into force of \nthis Regulation]. While the full effect of those prohibitions follows with the \nestablishment of the governance and enforcement of this Regulation, anticipating the \napplication of the prohibitions is important to take account of unacceptable risks and to \nhave an effect on other procedures, such as in civil law. Moreover, the infrastructure \nrelated to the governance and the conformity assessment system should be operational \nbefore\u2026 [two years from the date of entry into force of this Regulation], therefore the \nprovisions on notified bodies and governance structure should apply from \u2026 [ 12 months \nfrom the date of entry into force of this Regulation]. Given the rapid pace of \ntechnological advancements and adoption of general-purpose AI models, obligations for \nproviders of general-purpose AI models should apply from ... [12 months from the date \nof entry into force of this Regulation]. Codes of practice should be ready by... [9 months \nfrom the date of entry into force of this Regulation] in view of enabling providers to \ndemonstrate compliance on time. The AI Office should ensure that classification rules \nand procedures are up to date in light of technological developments. In addition, \nMember States should lay down and notify to the Commission the rules on penalties, \nincluding administrative fines, and ensure that they are properly and effectively \nimplemented by the date of application of this Regulation. Therefore the provisions on \npenalties should apply from \u2026 [12 months from the date of entry into force of this \nRegulation].", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "When should the prohibitions and general provisions of this Regulation apply?", "answer": "The prohibitions as well as the general provisions of this Regulation should already apply from six months from the date of entry into force of this Regulation."}, {"question": "What is the timeline for the obligations for providers of general-purpose AI models?", "answer": "Obligations for providers of general-purpose AI models should apply from 12 months from the date of entry into force of this Regulation."}], "seed_document_id": 170, "topic": "AI Regulation"}}
{"id": "79a6e2f2-2b1f-465d-b18f-811eeef8ec17", "question": "What actions must a market surveillance authority take if it finds an AI system does not comply with the requirements of the Regulation, and what should they do if they consider a high-risk AI system to be non-compliant?", "reference_answer": "The market surveillance authority shall require the operator to take corrective actions or withdraw the AI system, and they should cooperate with the AI Office for compliance evaluations while informing the Board and other authorities.", "reference_context": "Document 376: EN\nUnited in diversity\nEN\n2.\nWhere the market surveillance authority of a Member State has sufficient reason to \nconsider an AI system to present a risk as referred to in paragraph 1 of this Article, it shall \ncarry out an evaluation of the AI system concerned in respect of its compliance with all the \nrequirements and obligations laid down in this Regulation. Particular attention shall be \ngiven to AI systems presenting a risk to vulnerable groups. Where risks to fundamental \nrights are identified, the market surveillance authority shall also inform and fully \ncooperate with the relevant national public authorities or bodies referred to in Article \n77(1). The relevant operators shall cooperate as necessary with the market surveillance \nauthority and with the other national public authorities or bodies referred to in Article \n77(1).\nWhere, in the course of that evaluation, the market surveillance authority or, where \napplicable the market surveillance authority in cooperation with the national public \nauthority referred to in Article 77(1), finds that the AI system does not comply with the \nrequirements and obligations laid down in this Regulation, it shall without undue delay \nrequire the relevant operator to take all appropriate corrective actions to bring the AI \nsystem into compliance, to withdraw the AI system from the market, or to recall it within a \nperiod the market surveillance authority may prescribe, and in any event within the \nshorter of 15 working days, or as provided for in the relevant Union harmonisation \nlegislation.\nThe market surveillance authority shall inform the relevant notified body accordingly. \nArticle 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the \nsecond subparagraph of this paragraph.\n3.\nWhere the market surveillance authority considers that the non-compliance is not restricted \nto its national territory, it shall inform the Commission and the other Member States \nwithout undue delay of the results of the evaluation and of the actions which it has \nrequired the operator to take.\n\nDocument 368: EN\nUnited in diversity\nEN\n2.\nWhere the relevant market surveillance authorities have sufficient reason to consider \ngeneral-purpose AI systems that can be used directly by deployers for at least one \npurpose that is classified as high-risk pursuant to this Regulation to be non-compliant \nwith the requirements laid down in this Regulation, they shall cooperate with the AI \nOffice to carry out compliance evaluations, and shall inform the Board and other market \nsurveillance authorities accordingly.\n3.\nWhere a market surveillance authority is unable to conclude its investigation of the \nhigh-risk AI system because of its inability to access certain information related to the \ngeneral-purpose AI model despite having made all appropriate efforts to obtain that \ninformation, it may submit a reasoned request to the AI Office, by which access to that \ninformation shall be enforced. In that case, the AI Office shall supply to the applicant \nauthority without delay, and in any event within 30 days, any information that the AI \nOffice considers to be relevant in order to establish whether a high-risk AI system is \nnon-compliant. Market surveillance authorities shall safeguard the confidentiality of the \ninformation that they obtain in accordance with Article 78 of this Regulation. The \nprocedure provided for in Chapter VI of Regulation (EU) 2019/1020 shall apply mutatis \nmutandis.\n\nDocument 382: EN\nUnited in diversity\nEN\n2.\nWhere the Commission considers the measure taken by the relevant Member State to be \njustified, all Member States shall ensure that they take appropriate restrictive measures in \nrespect of the AI system concerned, such as requiring the withdrawal of the AI system \nfrom their market without undue delay, and shall inform the Commission accordingly. \nWhere the Commission considers the national measure to be unjustified, the Member State \nconcerned shall withdraw the measure and shall inform the Commission accordingly.\n3.\nWhere the national measure is considered justified and the non-compliance of the AI \nsystem is attributed to shortcomings in the harmonised standards or common specifications \nreferred to in Articles 40 and 41 of this Regulation, the Commission shall apply the \nprocedure provided for in Article 11 of Regulation (EU) No 1025/2012.\nArticle 82\nCompliant AI systems which present a risk\n1.\nWhere, having performed an evaluation under Article 79, after consulting the relevant \nnational public authority referred to in Article 77(1), the market surveillance authority of \na Member State finds that although a high-risk AI system complies with this Regulation, it \nnevertheless presents a risk to the health or safety of persons, \u258c to fundamental rights, or to \nother aspects of public interest protection, it shall require the relevant operator to take all \nappropriate measures to ensure that the AI system concerned, when placed on the market \nor put into service, no longer presents that risk without undue delay, within a \u258c period \u258c it \nmay prescribe.\n\nDocument 378: EN\nUnited in diversity\nEN\n7.\nThe market surveillance authorities other than the market surveillance authority of the \nMember State initiating the procedure shall, without undue delay, inform the Commission \nand the other Member States of any measures adopted and of any additional information at \ntheir disposal relating to the non-compliance of the AI system concerned, and, in the event \nof disagreement with the notified national measure, of their objections.\n8.\nWhere, within three months of receipt of the notification referred to in paragraph 5 of this \nArticle, no objection has been raised by either a market surveillance authority of a \nMember State or by the Commission in respect of a provisional measure taken by a market \nsurveillance authority of another Member State, that measure shall be deemed justified. \nThis shall be without prejudice to the procedural rights of the concerned operator in \naccordance with Article 18 of Regulation (EU) 2019/1020. The three-month period \nreferred to in this paragraph shall be reduced to 30 days in the event of non-compliance \nwith the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9.\nThe market surveillance authorities shall ensure that appropriate restrictive measures are \ntaken in respect of the product or the AI system concerned, such as withdrawal of the \nproduct or the AI system from their market, without undue delay.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What actions must a market surveillance authority take if it finds an AI system does not comply with the requirements of the Regulation?", "answer": "The market surveillance authority shall require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a specified period."}, {"question": "What should market surveillance authorities do if they consider a high-risk AI system to be non-compliant?", "answer": "They shall cooperate with the AI Office to carry out compliance evaluations and inform the Board and other market surveillance authorities accordingly."}], "seed_document_id": 376, "topic": "AI Regulation and Compliance"}}
{"id": "012f3be2-6322-4e73-94c5-5ba2f64501ee", "question": "What is the purpose of the information requirement laid down in this Regulation and what obligations do employers have regarding informing workers about AI systems?", "reference_answer": "The information requirement is ancillary and necessary to protect fundamental rights, while employers are obligated to inform or consult workers about AI systems under Union or national law.", "reference_context": "Document 96: EN\nUnited in diversity\nEN\n(92)\nThis Regulation is without prejudice to obligations for employers to inform or to inform \nand consult workers or their representatives under Union or national law and practice, \nincluding Directive 2002/14/EC of the European Parliament and of the Council39, on \ndecisions to put into service or use AI systems. It remains necessary to ensure \ninformation of workers and their representatives on the planned deployment of high-risk \nAI systems at the workplace where the conditions for those information or information \nand consultation obligations in other legal instruments are not fulfilled. Moreover, such \ninformation right is ancillary and necessary to the objective of protecting fundamental \nrights that underlies this Regulation. Therefore, an information requirement to that \neffect should be laid down in this Regulation, without affecting any existing rights of \nworkers.\n39\nDirective 2002/14/EC of the European Parliament and of the Council of 11 March 2002 \nestablishing a general framework for informing and consulting employees in the European \nCommunity (OJ L 80, 23.3.2002, p. 29).", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is the purpose of the information requirement laid down in this Regulation?", "answer": "The information requirement is ancillary and necessary to the objective of protecting fundamental rights that underlies this Regulation."}, {"question": "What obligations do employers have regarding informing workers about AI systems?", "answer": "Employers have obligations to inform or to inform and consult workers or their representatives under Union or national law and practice."}], "seed_document_id": 96, "topic": "AI Regulation and Compliance"}}
{"id": "5d473019-da86-4070-be74-80281472f9ed", "question": "What is the deadline for AI systems already placed on the market to comply with the new Regulation and what should be considered in the evaluation of large-scale IT systems established by legal acts listed in Annex X?", "reference_answer": "AI systems which are components of large-scale IT systems must comply with the Regulation by 31 December 2030, and the requirements of this Regulation should be considered in the evaluation of each large-scale IT system.", "reference_context": "Document 422: EN\nUnited in diversity\nEN\nArticle 111\nAI systems already placed on the market or put into service and general-purpose AI models already \nplaced on the marked\n1.\nWithout prejudice to the application of Article 5 as referred to in Article 113(3), point \n(a), AI systems which are components of the large-scale IT systems established by the \nlegal acts listed in Annex X that have been placed on the market or put into service before \n\u258c\u2026 [36 months from the date of entry into force of this Regulation] shall be brought into \ncompliance with this Regulation by 31 December 2030.\nThe requirements laid down in this Regulation shall be taken into account \u258c in the \nevaluation of each large-scale IT system established by the legal acts listed in Annex X to \nbe undertaken as provided for in those legal acts and where those legal acts are replaced \nor amended.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is the deadline for AI systems already placed on the market to comply with the new Regulation?", "answer": "AI systems which are components of the large-scale IT systems shall be brought into compliance with this Regulation by 31 December 2030."}, {"question": "What should be considered in the evaluation of large-scale IT systems established by legal acts listed in Annex X?", "answer": "The requirements laid down in this Regulation shall be taken into account in the evaluation of each large-scale IT system."}], "seed_document_id": 422, "topic": "AI Regulation and Compliance"}}
{"id": "a68591df-be3a-4f3d-9d0e-3e68ff60706b", "question": "What must the request for information state according to the regulations, and who is responsible for supplying the requested information regarding the general-purpose AI model?", "reference_answer": "The request for information must state the legal basis, purpose, required information, a deadline for provision, and potential fines for incorrect information, while the provider of the general-purpose AI model or its representative is responsible for supplying the requested information.", "reference_context": "Document 392: EN\nUnited in diversity\nEN\n4.\nThe request for information shall state the legal basis and the purpose of the request, \nspecify what information is required, set a period within which the information is to be \nprovided, and indicate the fines provided for in Article 101 for supplying incorrect, \nincomplete or misleading information.\n5.\nThe provider of the general-purpose AI model concerned, or its representative shall \nsupply the information requested. In the case of legal persons, companies or firms, or \nwhere the provider has no legal personality, the persons authorised to represent them by \nlaw or by their statutes, shall supply the information requested on behalf of the provider \nof the general-purpose AI model concerned. Lawyers duly authorised to act may supply \ninformation on behalf of their clients. The clients shall nevertheless remain fully \nresponsible if the information supplied is incomplete, incorrect or misleading.\nArticle 92\nPower to conduct evaluations\n1.\nThe AI Office, after consulting the Board, may conduct evaluations of the general-\npurpose AI model concerned:\n(a)\nto assess compliance of the provider with obligations under this Regulation, where \nthe information gathered pursuant to Article 91 is insufficient; or,\n(b)\nto investigate systemic risks at Union level of general-purpose AI models with \nsystemic risk, in particular following a qualified alert from the scientific panel in \naccordance with Article 90(1), point (a).", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What must the request for information state according to the regulations?", "answer": "The request for information shall state the legal basis and the purpose of the request, specify what information is required, set a period within which the information is to be provided, and indicate the fines provided for in Article 101 for supplying incorrect, incomplete or misleading information."}, {"question": "Who is responsible for supplying the requested information regarding the general-purpose AI model?", "answer": "The provider of the general-purpose AI model concerned, or its representative shall supply the information requested."}], "seed_document_id": 392, "topic": "AI Regulation and Governance"}}
{"id": "367dfe00-b02e-464b-a1b5-abd9c15c94a3", "question": "What factors are considered regarding the responsibility of a Union institution, body, office, or agency in the context of an infringement, and what actions are taken to address these infringements?", "reference_answer": "The degree of responsibility is assessed based on the technical and organizational measures implemented, as well as any actions taken to mitigate the damage suffered by affected persons.", "reference_context": "Document 408: EN\nUnited in diversity\nEN\n(b)\nthe degree of responsibility of the Union institution, body, office or agency, taking \ninto account technical and organisational measures implemented by them;\n(c)\nany action taken by the Union institution, body, office or agency to mitigate the \ndamage suffered by affected persons;\n(d)\nthe degree of cooperation with the European Data Protection Supervisor in order to \nremedy the infringement and mitigate the possible adverse effects of the \ninfringement, including compliance with any of the measures previously ordered by \nthe European Data Protection Supervisor against the Union institution, body, office \nor agency concerned with regard to the same subject matter;\n(e)\nany similar previous infringements by the Union institution, body, office or agency;\n(f)\nthe manner in which the infringement became known to the European Data \nProtection Supervisor, in particular whether, and if so to what extent, the Union \ninstitution, body, office or agency notified the infringement;\n(g)\nthe annual budget of the Union institution, body, office or agency.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What factors are considered regarding the responsibility of a Union institution, body, office, or agency in the context of an infringement?", "answer": "The degree of responsibility of the Union institution, body, office or agency is taken into account, considering technical and organisational measures implemented by them."}, {"question": "What action is taken by the Union institution, body, office, or agency to address infringements?", "answer": "Any action taken by the Union institution, body, office or agency to mitigate the damage suffered by affected persons is considered."}], "seed_document_id": 408, "topic": "AI Regulation in the EU"}}
{"id": "ae4a3d92-76e1-48e1-b00b-62e631868fa3", "question": "What is the purpose of AI literacy according to the EU AI Act, and who is responsible for promoting AI literacy tools and public awareness?", "reference_answer": "AI literacy aims to equip providers, deployers, and affected persons with the necessary knowledge to make informed decisions about AI systems, while the European Artificial Intelligence Board is responsible for promoting AI literacy tools and public awareness regarding the benefits, risks, safeguards, rights, and obligations related to AI.", "reference_context": "Document 21: EN\nUnited in diversity\nEN\n(20)\nIn order to obtain the greatest benefits from AI systems while protecting fundamental \nrights, health and safety and to enable democratic control, AI literacy should equip \nproviders, deployers and affected persons with the necessary notions to make informed \ndecisions regarding AI systems. Those notions may vary with regard to the relevant \ncontext and can include understanding the correct application of technical elements \nduring the AI system\u2019s development phase, the measures to be applied during its use, the \nsuitable ways in which to interpret the AI system\u2019s output, and, in the case of affected \npersons, the knowledge necessary to understand how decisions taken with the assistance \nof AI will have an impact on them. In the context of the application this Regulation, AI \nliteracy should provide all relevant actors in the AI value chain with the insights \nrequired to ensure the appropriate compliance and its correct enforcement. \nFurthermore, the wide implementation of AI literacy measures and the introduction of \nappropriate follow-up actions could contribute to improving working conditions and \nultimately sustain the consolidation, and innovation path of trustworthy AI in the Union. \nThe European Artificial Intelligence Board (the \u2018Board\u2019) should support the \nCommission, to promote AI literacy tools, public awareness and understanding of the \nbenefits, risks, safeguards, rights and obligations in relation to the use of AI systems. In \ncooperation with the relevant stakeholders, the Commission and the Member States \nshould facilitate the drawing up of voluntary codes of conduct to advance AI literacy \namong persons dealing with the development, operation and use of AI.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is the purpose of AI literacy according to the EU AI Act?", "answer": "AI literacy should equip providers, deployers, and affected persons with the necessary notions to make informed decisions regarding AI systems."}, {"question": "Who is responsible for promoting AI literacy tools and public awareness?", "answer": "The European Artificial Intelligence Board should support the Commission in promoting AI literacy tools, public awareness, and understanding of the benefits, risks, safeguards, rights, and obligations in relation to the use of AI systems."}], "seed_document_id": 21, "topic": "AI Regulation in the EU"}}
{"id": "50209d67-1cc8-4300-a9e0-b1617381d4c6", "question": "What is a key characteristic that distinguishes AI systems from traditional software systems, and how does the regulation define the term 'machine-based' in relation to AI systems?", "reference_answer": "A key characteristic of AI systems is their capability to infer, and the term 'machine-based' refers to the fact that AI systems run on machines.", "reference_context": "Document 13: EN\nUnited in diversity\nEN\n(12)\nThe notion of \u2018AI system\u2019 in this Regulation should be clearly defined and should be \nclosely aligned with the work of international organisations working on AI to ensure \nlegal certainty, facilitate international convergence and wide acceptance, while providing \nthe flexibility to accommodate the rapid technological developments in this field. \nMoreover, the definition should be based on key characteristics of AI systems that \ndistinguish it from simpler traditional software systems or programming approaches and \nshould not cover systems that are based on the rules defined solely by natural persons to \nautomatically execute operations. A key characteristic of AI systems is their capability to \ninfer. This capability to infer refers to the process of obtaining the outputs, such as \npredictions, content, recommendations, or decisions, which can influence physical and \nvirtual environments, and to a capability of AI systems to derive models or algorithms, or \nboth, from inputs or data. The techniques that enable inference while building an AI \nsystem include machine learning approaches that learn from data how to achieve certain \nobjectives, and logic- and knowledge-based approaches that infer from encoded \nknowledge or symbolic representation of the task to be solved. The capacity of an AI \nsystem to infer transcends basic data processing by enabling learning, reasoning or \nmodelling. The term \u2018machine-based\u2019 refers to the fact that AI systems run on machines.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is a key characteristic that distinguishes AI systems from traditional software systems?", "answer": "A key characteristic of AI systems is their capability to infer."}, {"question": "How does the regulation define the term 'machine-based' in relation to AI systems?", "answer": "The term 'machine-based' refers to the fact that AI systems run on machines."}], "seed_document_id": 13, "topic": "AI Regulation and Ethics"}}
{"id": "040cc32e-64c5-4db8-895e-288f7116d471", "question": "What principle is emphasized in the context provided and what should the Commission consider when amending acts related to high-risk AI systems?", "reference_answer": "The principle emphasized is 'United in diversity', and the Commission should consider the technical and regulatory specificities of each sector.", "reference_context": "Document 55: EN\nUnited in diversity\nEN\nappropriate to amend those acts to ensure that the Commission takes into account, on the \nbasis of the technical and regulatory specificities of each sector, and without interfering \nwith existing governance, conformity assessment and enforcement mechanisms and \nauthorities established therein, the mandatory requirements for high-risk AI systems laid \ndown in this Regulation when adopting any relevant delegated or implementing acts on the \nbasis of those acts.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What principle is emphasized in the context provided?", "answer": "The principle emphasized is 'United in diversity'."}, {"question": "What should the Commission consider when amending acts related to high-risk AI systems?", "answer": "The Commission should take into account the technical and regulatory specificities of each sector."}], "seed_document_id": 55, "topic": "AI Regulation and Compliance"}}
{"id": "768f73ac-0658-4d51-9fcb-b68c5f314f8a", "question": "What is the purpose of the Regulation regarding personal data in the AI regulatory sandbox, and what obligations remain applicable for data controllers and data subjects under this Regulation?", "reference_answer": "The purpose of the Regulation is to provide a legal basis for using personal data in the AI regulatory sandbox for developing AI systems in the public interest, while all obligations of data controllers and rights of data subjects under existing EU regulations remain applicable.", "reference_context": "Document 136: EN\nUnited in diversity\nEN\n(140)\nThis Regulation should provide the legal basis for the providers and prospective \nproviders in the AI regulatory sandbox to use personal data collected for other purposes \nfor developing certain AI systems in the public interest within the AI regulatory sandbox, \nonly under specified conditions, in accordance with Article 6(4) and Article 9(2), point \n(g), of Regulation (EU) 2016/679, and Articles 5, 6 and 10 of Regulation (EU) \n2018/1725, and without prejudice to Article 4(2) and Article 10 of Directive (EU) \n2016/680. All other obligations of data controllers and rights of data subjects under \nRegulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 remain \napplicable. In particular, this Regulation should not provide a legal basis in the meaning \nof Article 22(2), point (b) of Regulation (EU) 2016/679 and Article 24(2), point (b) of \nRegulation (EU) 2018/1725. Providers and prospective providers in the AI regulatory \nsandbox should ensure appropriate safeguards and cooperate with the competent \nauthorities, including by following their guidance and acting expeditiously and in good \nfaith to adequately mitigate any identified significant risks to safety, health, and \nfundamental rights that may arise during the development, testing and experimentation \nin that sandbox.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is the purpose of the Regulation regarding personal data in the AI regulatory sandbox?", "answer": "This Regulation should provide the legal basis for the providers and prospective providers in the AI regulatory sandbox to use personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox."}, {"question": "What obligations remain applicable for data controllers and data subjects under this Regulation?", "answer": "All other obligations of data controllers and rights of data subjects under Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 remain applicable."}], "seed_document_id": 136, "topic": "AI Regulatory Sandboxes"}}
{"id": "bc34e0d1-d09e-49d6-9347-b1ae5b690778", "question": "What must notified bodies inform the notifying authority about regarding Union technical documentation assessment certificates and what circumstances must they report?", "reference_answer": "Notified bodies must inform the notifying authority about any Union technical documentation assessment certificates, supplements, quality management system approvals, and any circumstances affecting the scope or conditions for notification.", "reference_context": "Document 285: EN\nUnited in diversity\nEN\nArticle 45\nInformation obligations of notified bodies\n1.\nNotified bodies shall inform the notifying authority of the following: \n(a)\nany Union technical documentation assessment certificates, any supplements to those \ncertificates, and any quality management system approvals issued in accordance with \nthe requirements of Annex VII;\n(b)\nany refusal, restriction, suspension or withdrawal of a Union technical \ndocumentation assessment certificate or a quality management system approval \nissued in accordance with the requirements of Annex VII;\n(c)\nany circumstances affecting the scope of or conditions for notification;\n(d)\nany request for information which they have received from market surveillance \nauthorities regarding conformity assessment activities;\n(e)\non request, conformity assessment activities performed within the scope of their \nnotification and any other activity performed, including cross-border activities and \nsubcontracting.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What must notified bodies inform the notifying authority about regarding Union technical documentation assessment certificates?", "answer": "Notified bodies shall inform the notifying authority of any Union technical documentation assessment certificates, any supplements to those certificates, and any quality management system approvals issued in accordance with the requirements of Annex VII."}, {"question": "What circumstances must notified bodies report to the notifying authority?", "answer": "Notified bodies must report any circumstances affecting the scope of or conditions for notification."}], "seed_document_id": 285, "topic": "Notified Bodies Regulations"}}
{"id": "ad5fe137-923b-4a9b-b4b2-2fb1856c91f4", "question": "What is required for high-risk AI systems regarding the CE marking and how should it be affixed?", "reference_answer": "A digital CE marking shall be used if easily accessible, and it must be affixed visibly, legibly, and indelibly.", "reference_context": "Document 291: EN\nUnited in diversity\nEN\n2.\nFor high-risk AI systems provided digitally, a digital CE marking shall be used, only if it \ncan easily be accessed via the interface from which that system is accessed or via an \neasily accessible machine-readable code or other electronic means.\n3.\nThe CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. \nWhere that is not possible or not warranted on account of the nature of the high-risk AI \nsystem, it shall be affixed to the packaging or to the accompanying documentation, as \nappropriate.\n4.\nWhere applicable, the CE marking shall be followed by the identification number of the \nnotified body responsible for the conformity assessment procedures set out in Article 43. \nThe identification number of the notified body shall be affixed by the body itself or, under \nits instructions, by the provider or by the provider\u2019s authorised representative. The \nidentification number shall also be indicated in any promotional material which mentions \nthat the high-risk AI system fulfils the requirements for CE marking.\n5.\nWhere high-risk AI systems are subject to other Union law which also provides for the \naffixing of the CE marking, the CE marking shall indicate that the high-risk AI system \nalso fulfil the requirements of that other law.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is required for high-risk AI systems provided digitally regarding the CE marking?", "answer": "A digital CE marking shall be used, only if it can easily be accessed via the interface from which that system is accessed or via an easily accessible machine-readable code or other electronic means."}, {"question": "How should the CE marking be affixed for high-risk AI systems?", "answer": "The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems."}], "seed_document_id": 291, "topic": "High-Risk AI Regulation"}}
{"id": "d77c8731-7cf8-45a9-a29b-624bc2b2482c", "question": "What is a 'remote biometric identification system' as defined in the Regulation, and how do 'real-time' systems differ from 'post' systems in biometric identification?", "reference_answer": "A 'remote biometric identification system' is an AI system for identifying individuals without their active involvement by comparing biometric data to a reference database, while 'real-time' systems perform capturing, comparison, and identification instantaneously, unlike 'post' systems which involve a delay after data capture.", "reference_context": "Document 17: EN\nUnited in diversity\nEN\n(17)\nThe notion of \u2018remote biometric identification system\u2019 referred to in this Regulation should \nbe defined functionally, as an AI system intended for the identification of natural persons \nwithout their active involvement, typically at a distance, through the comparison of a \nperson\u2019s biometric data with the biometric data contained in a reference database, \nirrespectively of the particular technology, processes or types of biometric data used. \nSuch remote biometric identification systems are typically used to perceive multiple \npersons or their behaviour simultaneously in order to facilitate significantly the \nidentification of natural persons without their active involvement. This excludes AI \nsystems intended to be used for biometric verification, which includes authentication, the \nsole purpose of which is to confirm that a specific natural person is the person he or she \nclaims to be and to confirm the identity of a natural person for the sole purpose of \nhaving access to a service, unlocking a device or having security access to premises. \nThat exclusion is justified by the fact that such systems are likely to have a minor impact \non fundamental rights of natural persons compared to the remote biometric \nidentification systems which may be used for the processing of the biometric data of a \nlarge number of persons without their active involvement. In the case of \u2018real-time\u2019 \nsystems, the capturing of the biometric data, the comparison and the identification occur all \ninstantaneously, near-instantaneously or in any event without a significant delay. In this \nregard, there should be no scope for circumventing the rules of this Regulation on the \u2018real-\ntime\u2019 use of the AI systems concerned by providing for minor delays. \u2018Real-time\u2019 systems \ninvolve the use of \u2018live\u2019 or \u2018near-live\u2019 material, such as video footage, generated by a \ncamera or other device with similar functionality. In the case of \u2018post\u2019 systems, in contrast, \nthe biometric data has already been captured and the comparison and identification occur \nonly after a significant delay. This involves material, such as pictures or video footage \ngenerated by closed circuit television cameras or private devices, which has been generated \nbefore the use of the system in respect of the natural persons concerned.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is a 'remote biometric identification system' as defined in the Regulation?", "answer": "A 'remote biometric identification system' is an AI system intended for the identification of natural persons without their active involvement, typically at a distance, through the comparison of a person\u2019s biometric data with the biometric data contained in a reference database."}, {"question": "How do 'real-time' systems differ from 'post' systems in biometric identification?", "answer": "'Real-time' systems involve the capturing of biometric data, comparison, and identification occurring all instantaneously or near-instantaneously, while 'post' systems involve biometric data that has already been captured, with comparison and identification occurring only after a significant delay."}], "seed_document_id": 17, "topic": "Biometric Data and AI"}}
{"id": "af218405-05c5-4795-aaf8-f90c11f2c24f", "question": "What is the purpose of the fundamental rights impact assessment for high-risk AI systems, and who is required to perform it?", "reference_answer": "The purpose of the fundamental rights impact assessment is to identify specific risks to individuals' rights and necessary measures to mitigate those risks, and it must be performed by deployers governed by public law, private entities providing public services, and certain high-risk AI system deployers like banking or insurance entities.", "reference_context": "Document 100: EN\nUnited in diversity\nEN\nThe assessment should also include the identification of specific risks of harm likely to \nhave an impact on the fundamental rights of those persons or groups. While performing \nthis assessment, the deployer should take into account information relevant to a proper \nassessment of the impact, including but not limited to the information given by the \nprovider of the high-risk AI system in the instructions for use. In light of the risks \nidentified, deployers should determine measures to be taken in the case of a \nmaterialisation of those risks, including for example governance arrangements in that \nspecific context of use, such as arrangements for human oversight according to the \ninstructions of use or, complaint handling and redress procedures, as they could be \ninstrumental in mitigating risks to fundamental rights in concrete use-cases. After \nperforming that impact assessment, the deployer should notify the relevant market \nsurveillance authority. Where appropriate, to collect relevant information necessary to \nperform the impact assessment, deployers of high-risk AI system, in particular when AI \nsystems are used in the public sector, could involve relevant stakeholders, including the \nrepresentatives of groups of persons likely to be affected by the AI system, independent \nexperts, and civil society organisations in conducting such impact assessments and \ndesigning measures to be taken in the case of materialisation of the risks. The European \nArtificial Intelligence Office (AI Office) should develop a template for a questionnaire in \norder to facilitate compliance and reduce the administrative burden for deployers.\n\nDocument 99: EN\nUnited in diversity\nEN\n(96)\nIn order to efficiently ensure that fundamental rights are protected, deployers of high-\nrisk AI systems that are bodies governed by public law, or private entities providing \npublic services and deployers of certain high-risk AI systems listed in an annex to this \nRegulation, such as banking or insurance entities, should carry out a fundamental \nrights impact assessment prior to putting it into use. Services important for individuals \nthat are of public nature may also be provided by private entities. Private entities \nproviding such public services are linked to tasks in the public interest such as in the \nareas of education, healthcare, social services, housing, administration of justice. The \naim of the fundamental rights impact assessment is for the deployer to identify the \nspecific risks to the rights of individuals or groups of individuals likely to be affected, \nidentify measures to be taken in the case of a materialisation of those risks. The impact \nassessment should be performed prior to deploying the high-risk AI system, and should \nbe updated when the deployer considers that any of the relevant factors have changed. \nThe impact assessment should identify the deployer\u2019s relevant processes in which the \nhigh-risk AI system will be used in line with its intended purpose, and should include a \ndescription of the period of time and frequency in which the system is intended to be \nused as well as of specific categories of natural persons and groups who are likely to be \naffected in the specific context of use.\n\nDocument 256: EN\nUnited in diversity\nEN\n(d)\nthe specific risks of harm likely to have an impact on the categories of natural \npersons or groups of persons identified pursuant to point (c) of this paragraph, \ntaking into account the information given by the provider pursuant to Article 13;\n(e)\na description of the implementation of human oversight measures, according to the \ninstructions for use;\n(f)\nthe measures to be taken in the case of the materialisation of those risks, including \nthe arrangements for internal governance and complaint mechanisms.\n2.\nThe obligation laid down in paragraph 1 applies to the first use of the high-risk AI \nsystem. The deployer may, in similar cases, rely on previously conducted fundamental \nrights impact assessments or existing impact assessments carried out by provider. If, \nduring the use of the high-risk AI system, the deployer considers that any of the elements \nlisted in paragraph 1 has changed or is no longer up to date, the deployer shall take the \nnecessary steps to update the information.\n3.\nOnce the assessment referred to in paragraph 1 of this Article has been performed, the \ndeployer shall notify the market surveillance authority of its results, submitting the \nfilled-out template referred to in paragraph 5 of this Article as part of the notification. In \nthe case referred to in Article 46(1), deployers may be exempt from that obligation to \nnotify.\n\nDocument 255: EN\nUnited in diversity\nEN\nArticle 27\nFundamental rights impact assessment for high-risk AI systems\n1.\nPrior to deploying a high-risk AI system referred to in Article 6(2), with the exception of \nhigh-risk AI systems intended to be used in the area listed in point 2 of Annex III, \ndeployers that are bodies governed by public law, or are private entities providing public \nservices, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of \nAnnex III, shall perform an assessment of the impact on fundamental rights that the use \nof such system may produce. For that purpose, deployers shall perform an assessment \nconsisting of:\n(a)\na description of the deployer\u2019s processes in which the high-risk AI system will be \nused in line with its intended purpose;\n(b)\na description of the period of time within which, and the frequency with which, \neach high-risk AI system is intended to be used;\n(c)\nthe categories of natural persons and groups likely to be affected by its use in the \nspecific context;", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is the purpose of the fundamental rights impact assessment for high-risk AI systems?", "answer": "The aim of the fundamental rights impact assessment is for the deployer to identify the specific risks to the rights of individuals or groups of individuals likely to be affected, and to identify measures to be taken in the case of a materialisation of those risks."}, {"question": "Who is required to perform a fundamental rights impact assessment prior to deploying a high-risk AI system?", "answer": "Deployers that are bodies governed by public law, or are private entities providing public services, and deployers of certain high-risk AI systems listed in an annex to the Regulation, such as banking or insurance entities, are required to perform the assessment."}], "seed_document_id": 100, "topic": "Others"}}
{"id": "0cb4a7f3-c7cb-4d15-8c97-aa630c4668a8", "question": "What must the quality management system documentation include according to Article 17, and who assesses the quality management system to determine its compliance?", "reference_answer": "The quality management system documentation must cover all aspects listed under Article 17, and it shall be assessed by the notified body to determine if it satisfies those requirements.", "reference_context": "Document 452: EN\nUnited in diversity\nEN\n(b)\nthe list of AI systems covered under the same quality management system;\n(c)\nthe technical documentation for each AI system covered under the same quality \nmanagement system;\n(d)\nthe documentation concerning the quality management system which shall cover all \nthe aspects listed under Article 17;\n(e)\na description of the procedures in place to ensure that the quality management system \nremains adequate and effective;\n(f)\na written declaration that the same application has not been lodged with any other \nnotified body.\n3.2.\nThe quality management system shall be assessed by the notified body, which shall \ndetermine whether it satisfies the requirements referred to in Article 17.\nThe decision shall be notified to the provider or its authorised representative.\nThe notification shall contain the conclusions of the assessment of the quality management \nsystem and the reasoned assessment decision.\n3.3.\nThe quality management system as approved shall continue to be implemented and \nmaintained by the provider so that it remains adequate and efficient.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What must the quality management system documentation include according to Article 17?", "answer": "The documentation concerning the quality management system shall cover all the aspects listed under Article 17."}, {"question": "Who assesses the quality management system and what do they determine?", "answer": "The quality management system shall be assessed by the notified body, which shall determine whether it satisfies the requirements referred to in Article 17."}], "seed_document_id": 452, "topic": "AI Quality Management"}}
{"id": "9bd85701-4259-46a1-869f-f98450357fd8", "question": "What is the role of market surveillance authorities in monitoring AI practices according to Article 81, and what happens if no objection is raised against a provisional measure within three months?", "reference_answer": "Market surveillance authorities may perform appropriate checks using the EU database to monitor Article 81, and if no objection is raised within three months, the provisional measure is deemed justified.", "reference_context": "Document 381: EN\nUnited in diversity\nEN\n8.\nIn exercising their power to monitor the application of this Article, and in accordance \nwith Article 11 of Regulation (EU) 2019/1020, market surveillance authorities may \nperform appropriate checks, taking into account in particular information stored in the \nEU database referred to in Article 71 of this Regulation.\nArticle 81\nUnion safeguard procedure\n1.\nWhere, within three months of receipt of the notification referred to in Article 79(5), or \nwithin 30 days in the case of non-compliance with the prohibition of the AI practices \nreferred to in Article 5, objections are raised by the market surveillance authority of a \nMember State to a measure taken by another market surveillance authority, or where the \nCommission considers the measure to be contrary to Union law, the Commission shall \nwithout undue delay enter into consultation with the market surveillance authority of the \nrelevant Member State and the operator or operators, and shall evaluate the national \nmeasure. On the basis of the results of that evaluation, the Commission shall, within six \nmonths, or within 60 days in the case of non-compliance with the prohibition of the AI \npractices referred to in Article 5, starting from the notification referred to in Article 79(5), \ndecide whether the national measure is justified and shall notify its decision to the market \nsurveillance authority of the Member State concerned. The Commission shall also inform \nall other market surveillance authorities of its decision.\n\nDocument 378: EN\nUnited in diversity\nEN\n7.\nThe market surveillance authorities other than the market surveillance authority of the \nMember State initiating the procedure shall, without undue delay, inform the Commission \nand the other Member States of any measures adopted and of any additional information at \ntheir disposal relating to the non-compliance of the AI system concerned, and, in the event \nof disagreement with the notified national measure, of their objections.\n8.\nWhere, within three months of receipt of the notification referred to in paragraph 5 of this \nArticle, no objection has been raised by either a market surveillance authority of a \nMember State or by the Commission in respect of a provisional measure taken by a market \nsurveillance authority of another Member State, that measure shall be deemed justified. \nThis shall be without prejudice to the procedural rights of the concerned operator in \naccordance with Article 18 of Regulation (EU) 2019/1020. The three-month period \nreferred to in this paragraph shall be reduced to 30 days in the event of non-compliance \nwith the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9.\nThe market surveillance authorities shall ensure that appropriate restrictive measures are \ntaken in respect of the product or the AI system concerned, such as withdrawal of the \nproduct or the AI system from their market, without undue delay.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is the role of market surveillance authorities in monitoring AI practices according to Article 81?", "answer": "Market surveillance authorities may perform appropriate checks, taking into account information stored in the EU database, to monitor the application of the Article."}, {"question": "What happens if no objection is raised against a provisional measure within three months?", "answer": "If no objection has been raised by either a market surveillance authority or the Commission within three months, the provisional measure shall be deemed justified."}], "seed_document_id": 381, "topic": "AI Regulation and Compliance"}}
{"id": "46fd2895-d811-4e3f-9424-8fd0825c403b", "question": "What actions must a market surveillance authority take if it finds an AI system non-compliant, and what should it do if it cannot access certain information during its investigation?", "reference_answer": "If a market surveillance authority finds an AI system non-compliant, it shall require the operator to take corrective actions, withdraw, or recall the system, and if it cannot access certain information, it may submit a reasoned request to the AI Office for enforcement.", "reference_context": "Document 368: EN\nUnited in diversity\nEN\n2.\nWhere the relevant market surveillance authorities have sufficient reason to consider \ngeneral-purpose AI systems that can be used directly by deployers for at least one \npurpose that is classified as high-risk pursuant to this Regulation to be non-compliant \nwith the requirements laid down in this Regulation, they shall cooperate with the AI \nOffice to carry out compliance evaluations, and shall inform the Board and other market \nsurveillance authorities accordingly.\n3.\nWhere a market surveillance authority is unable to conclude its investigation of the \nhigh-risk AI system because of its inability to access certain information related to the \ngeneral-purpose AI model despite having made all appropriate efforts to obtain that \ninformation, it may submit a reasoned request to the AI Office, by which access to that \ninformation shall be enforced. In that case, the AI Office shall supply to the applicant \nauthority without delay, and in any event within 30 days, any information that the AI \nOffice considers to be relevant in order to establish whether a high-risk AI system is \nnon-compliant. Market surveillance authorities shall safeguard the confidentiality of the \ninformation that they obtain in accordance with Article 78 of this Regulation. The \nprocedure provided for in Chapter VI of Regulation (EU) 2019/1020 shall apply mutatis \nmutandis.\n\nDocument 376: EN\nUnited in diversity\nEN\n2.\nWhere the market surveillance authority of a Member State has sufficient reason to \nconsider an AI system to present a risk as referred to in paragraph 1 of this Article, it shall \ncarry out an evaluation of the AI system concerned in respect of its compliance with all the \nrequirements and obligations laid down in this Regulation. Particular attention shall be \ngiven to AI systems presenting a risk to vulnerable groups. Where risks to fundamental \nrights are identified, the market surveillance authority shall also inform and fully \ncooperate with the relevant national public authorities or bodies referred to in Article \n77(1). The relevant operators shall cooperate as necessary with the market surveillance \nauthority and with the other national public authorities or bodies referred to in Article \n77(1).\nWhere, in the course of that evaluation, the market surveillance authority or, where \napplicable the market surveillance authority in cooperation with the national public \nauthority referred to in Article 77(1), finds that the AI system does not comply with the \nrequirements and obligations laid down in this Regulation, it shall without undue delay \nrequire the relevant operator to take all appropriate corrective actions to bring the AI \nsystem into compliance, to withdraw the AI system from the market, or to recall it within a \nperiod the market surveillance authority may prescribe, and in any event within the \nshorter of 15 working days, or as provided for in the relevant Union harmonisation \nlegislation.\nThe market surveillance authority shall inform the relevant notified body accordingly. \nArticle 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the \nsecond subparagraph of this paragraph.\n3.\nWhere the market surveillance authority considers that the non-compliance is not restricted \nto its national territory, it shall inform the Commission and the other Member States \nwithout undue delay of the results of the evaluation and of the actions which it has \nrequired the operator to take.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What actions must a market surveillance authority take if it finds an AI system non-compliant?", "answer": "It shall without undue delay require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a period the market surveillance authority may prescribe."}, {"question": "What should a market surveillance authority do if it cannot access certain information during its investigation?", "answer": "It may submit a reasoned request to the AI Office, by which access to that information shall be enforced."}], "seed_document_id": 368, "topic": "Others"}}
{"id": "d32a0765-c22f-4f13-a56b-eb4bfdffdf01", "question": "What information should be provided regarding the input data and what human oversight measures are mentioned for high-risk AI systems?", "reference_answer": "Specifications for the input data related to training, validation, and testing, as well as human oversight measures from Article 14 that aid in interpreting the outputs of high-risk AI systems.", "reference_context": "Document 223: EN\nUnited in diversity\nEN\n(vi)\nwhen appropriate, specifications for the input data, or any other relevant \ninformation in terms of the training, validation and testing data sets used, \ntaking into account the intended purpose of the high-risk AI system;\n(vii) where applicable, information to enable deployers to interpret the output of \nthe high-risk AI system and use it appropriately;\n(c)\nthe changes to the high-risk AI system and its performance which have been pre-\ndetermined by the provider at the moment of the initial conformity assessment, if \nany;\n(d)\nthe human oversight measures referred to in Article 14, including the technical \nmeasures put in place to facilitate the interpretation of the outputs of the high-risk AI \nsystems by the deployers;\n(e)\nthe computational and hardware resources needed, the expected lifetime of the \nhigh-risk AI system and any necessary maintenance and care measures, including \ntheir frequency, to ensure the proper functioning of that AI system, including as \nregards software updates;\n(f)\nwhere relevant, a description of the mechanisms included within the high-risk AI \nsystem that allows deployers to properly collect, store and interpret the logs in \naccordance with Article 12.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What information should be provided regarding the input data for high-risk AI systems?", "answer": "Specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the high-risk AI system."}, {"question": "What are the human oversight measures mentioned in the context of high-risk AI systems?", "answer": "The human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of the high-risk AI systems by the deployers."}], "seed_document_id": 223, "topic": "Others"}}
{"id": "097c5ee4-1dfa-404e-a65a-d588e6db423d", "question": "What is required for the use of a 'real-time' remote biometric identification system in publicly accessible spaces for law enforcement, and what are the potential risks associated with its use?", "reference_answer": "Each use requires express and specific authorisation from a judicial or independent administrative authority, and the system poses risks such as intrusion into rights and freedoms, feelings of constant surveillance, and potential biased results.", "reference_context": "Document 38: EN\nUnited in diversity\nEN\n(35)\nEach use of a \u2018real-time\u2019 remote biometric identification system in publicly accessible \nspaces for the purpose of law enforcement should be subject to an express and specific \nauthorisation by a judicial authority or by an independent administrative authority of a \nMember State whose decision is binding. Such authorisation should, in principle, be \nobtained prior to the use of the AI system with a view to identifying a person or persons. \nExceptions to that rule should be allowed in duly justified situations on grounds of \nurgency, namely in situations where the need to use the systems concerned is such as to \nmake it effectively and objectively impossible to obtain an authorisation before \ncommencing the use of the AI system. In such situations of urgency, the use of the AI \nsystem should be restricted to the absolute minimum necessary and should be subject to \nappropriate safeguards and conditions, as determined in national law and specified in the \ncontext of each individual urgent use case by the law enforcement authority itself. In \naddition, the law enforcement authority should in such situations request such \nauthorisation \u258c while providing the reasons for not having been able to request it earlier, \nwithout undue delay and at the latest within 24 hours. If such an authorisation is \nrejected, the use of real-time biometric identification systems linked to that authorisation \nshould cease with immediate effect and all the data related to such use should be \ndiscarded and deleted. Such data includes input data directly acquired by an AI system \nin the course of the use of such system as well as the results and outputs of the use \nlinked to that authorisation. It should not include input that is legally acquired in \naccordance with another Union or national law. In any case, no decision producing an \nadverse legal effect on a person should be taken based solely on the output of the remote \nbiometric identification system.\n\nDocument 199: EN\nUnited in diversity\nEN\n3.\nFor the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each \u258c use \nfor the purposes of law enforcement of a \u2018real-time\u2019 remote biometric identification system \nin publicly accessible spaces shall be subject to a prior authorisation granted by a judicial \nauthority or \u258c an independent administrative authority whose decision is binding of the \nMember State in which the use is to take place, issued upon a reasoned request and in \naccordance with the detailed rules of national law referred to in paragraph 5. However, in a \nduly justified situation of urgency, the use of such system may be commenced without an \nauthorisation provided that such authorisation is requested without undue delay, at the \nlatest within 24 hours. If such authorisation is rejected, the use shall be stopped with \nimmediate effect and all the data, as well as the results and outputs of that use shall be \nimmediately discarded and deleted.\nThe competent judicial authority or an independent administrative authority whose \ndecision is binding shall grant the authorisation only where it is satisfied, on the basis of \nobjective evidence or clear indications presented to it, that the use of the \u2018real-time\u2019 remote \nbiometric identification system concerned is necessary for, and proportionate to, achieving \none of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in \nthe request and, in particular, remains limited to what is strictly necessary concerning \nthe period of time as well as the geographic and personal scope. In deciding on the \nrequest, that authority shall take into account the elements referred to in paragraph 2. No \ndecision that produces an adverse legal effect on a person may be taken based solely on \nthe output of the \u2018real-time\u2019 remote biometric identification system.\n\nDocument 40: EN\nUnited in diversity\nEN\n(38)\nThe use of AI systems for real-time remote biometric identification of natural persons in \npublicly accessible spaces for the purpose of law enforcement necessarily involves the \nprocessing of biometric data. The rules of this Regulation that prohibit, subject to certain \nexceptions, such use, which are based on Article 16 TFEU, should apply as lex specialis in \nrespect of the rules on the processing of biometric data contained in Article 10 of Directive \n(EU) 2016/680, thus regulating such use and the processing of biometric data involved in \nan exhaustive manner. Therefore, such use and processing should be possible only in as far \nas it is compatible with the framework set by this Regulation, without there being scope, \noutside that framework, for the competent authorities, where they act for purpose of law \nenforcement, to use such systems and process such data in connection thereto on the \ngrounds listed in Article 10 of Directive (EU) 2016/680. In that context, this Regulation is \nnot intended to provide the legal basis for the processing of personal data under Article 8 \nof Directive (EU) 2016/680. However, the use of real-time remote biometric identification \nsystems in publicly accessible spaces for purposes other than law enforcement, including \nby competent authorities, should not be covered by the specific framework regarding such \nuse for the purpose of law enforcement set by this Regulation. Such use for purposes other \nthan law enforcement should therefore not be subject to the requirement of an authorisation \nunder this Regulation and the applicable detailed rules of national law that may give effect \nto that authorisation.\n\nDocument 34: EN\nUnited in diversity\nEN\n(32)\nThe use of AI systems for \u2018real-time\u2019 remote biometric identification of natural persons in \npublicly accessible spaces for the purpose of law enforcement is particularly intrusive to \nthe rights and freedoms of the concerned persons, to the extent that it may affect the \nprivate life of a large part of the population, evoke a feeling of constant surveillance and \nindirectly dissuade the exercise of the freedom of assembly and other fundamental rights. \nTechnical inaccuracies of AI systems intended for the remote biometric identification of \nnatural persons can lead to biased results and entail discriminatory effects. Such \npossible biased results and discriminatory effects are particularly relevant with regard to \nage, ethnicity, race, sex or disabilities. In addition, the immediacy of the impact and the \nlimited opportunities for further checks or corrections in relation to the use of such systems \noperating in real-time carry heightened risks for the rights and freedoms of the persons \nconcerned in the context of, or impacted by, law enforcement activities.\n(33)\nThe use of those systems for the purpose of law enforcement should therefore be \nprohibited, except in exhaustively listed and narrowly defined situations, where the use is \nstrictly necessary to achieve a substantial public interest, the importance of which \noutweighs the risks. Those situations involve the search for certain victims of crime \u258c \nincluding missing persons; certain threats to the life or to the physical safety of natural \npersons or of a terrorist attack; and the localisation or identification of perpetrators or \nsuspects of the criminal offences listed in an annex to this Regulation, where those \ncriminal offences are punishable in the Member State concerned by a custodial sentence or \na detention order for a maximum period of at least four years and as they are defined in the \nlaw of that Member State. Such a threshold for the custodial sentence or detention order in \naccordance with national law contributes to ensuring that the offence should be serious \nenough to potentially justify the use of \u2018real-time\u2019 remote biometric identification systems.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is required for the use of a 'real-time' remote biometric identification system in publicly accessible spaces for law enforcement purposes?", "answer": "Each use should be subject to an express and specific authorisation by a judicial authority or by an independent administrative authority of a Member State, whose decision is binding."}, {"question": "What are the potential risks associated with the use of AI systems for real-time remote biometric identification?", "answer": "The use of such systems is particularly intrusive to the rights and freedoms of concerned persons, potentially affecting private life, evoking a feeling of constant surveillance, and leading to biased results and discriminatory effects."}], "seed_document_id": 38, "topic": "Biometric Data and AI"}}
{"id": "23d5b028-73de-4c38-9a4e-cf965fcdbd6b", "question": "What must high-risk AI systems be resilient against and what types of attacks should technical solutions address?", "reference_answer": "High-risk AI systems must be resilient against unauthorized attempts to alter their use and performance, and technical solutions should address attacks such as data poisoning, model poisoning, adversarial examples, confidentiality attacks, and model flaws.", "reference_context": "Document 228: EN\nUnited in diversity\nEN\n5.\nHigh-risk AI systems shall be resilient against attempts by unauthorised third parties to \nalter their use, outputs or performance by exploiting system vulnerabilities.\nThe technical solutions aiming to ensure the cybersecurity of high-risk AI systems shall be \nappropriate to the relevant circumstances and the risks.\nThe technical solutions to address AI specific vulnerabilities shall include, where \nappropriate, measures to prevent, detect, respond to, resolve and control for attacks trying \nto manipulate the training data set (data poisoning), or pre-trained components used in \ntraining (model poisoning), inputs designed to cause the AI model to make a mistake \n(adversarial examples or model evasion), confidentiality attacks or model flaws.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What must high-risk AI systems be resilient against?", "answer": "High-risk AI systems shall be resilient against attempts by unauthorized third parties to alter their use, outputs or performance by exploiting system vulnerabilities."}, {"question": "What types of attacks should technical solutions for high-risk AI systems address?", "answer": "The technical solutions to address AI specific vulnerabilities shall include measures to prevent, detect, respond to, resolve and control for attacks like data poisoning, model poisoning, adversarial examples, confidentiality attacks, or model flaws."}], "seed_document_id": 228, "topic": "High-Risk AI Systems"}}
{"id": "0e46ce6b-1383-49ec-a03a-d7e781290e51", "question": "What is meant by the term 'publicly accessible space' in this Regulation, and are online spaces considered publicly accessible spaces?", "reference_answer": "The term 'publicly accessible space' refers to any physical space accessible to an undetermined number of people, regardless of ownership, while online spaces are not considered publicly accessible as they are not physical.", "reference_context": "Document 19: EN\nUnited in diversity\nEN\n(19)\nFor the purposes of this Regulation the notion of \u2018publicly accessible space\u2019 should be \nunderstood as referring to any physical space that is accessible to an undetermined \nnumber of natural persons, and irrespective of whether the space in question is privately \nor publicly owned, irrespective of the activity for which the space may be used, such as \nfor commerce, for example, shops, restaurants, caf\u00e9s; for services, for example, banks, \nprofessional activities, hospitality; for sport, for example, swimming pools, gyms, \nstadiums; for transport, for example, bus, metro and railway stations, airports, means of \ntransport; for entertainment, for example, cinemas, theatres, museums, concert and \nconference halls; or for leisure or otherwise, for example, public roads and squares, \nparks, forests, playgrounds. A space should also be classified as being publicly accessible \nif, regardless of potential capacity or security restrictions, access is subject to certain \npredetermined conditions which can be fulfilled by an undetermined number of persons, \nsuch as the purchase of a ticket or title of transport, prior registration or having a certain \nage. In contrast, a space should not be considered to be publicly accessible if access is \nlimited to specific and defined natural persons through either Union or national law \ndirectly related to public safety or security or through the clear manifestation of will by \nthe person having the relevant authority over the space. The factual possibility of access \nalone, such as an unlocked door or an open gate in a fence, does not imply that the space \nis publicly accessible in the presence of indications or circumstances suggesting the \ncontrary, such as. signs prohibiting or restricting access. Company and factory premises, \nas well as offices and workplaces that are intended to be accessed only by relevant \nemployees and service providers, are spaces that are not publicly accessible. Publicly \naccessible spaces should not include prisons or border control. Some other spaces may \ncomprise both publicly accessible and non-publicly accessible spaces, such as the \nhallway of a private residential building necessary to access a doctor's office or an \nairport. Online spaces are not covered, as they are not physical spaces.", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What is meant by the term 'publicly accessible space' in this Regulation?", "answer": "The notion of \u2018publicly accessible space\u2019 refers to any physical space that is accessible to an undetermined number of natural persons, regardless of whether it is privately or publicly owned."}, {"question": "Are online spaces considered publicly accessible spaces?", "answer": "No, online spaces are not covered, as they are not physical spaces."}], "seed_document_id": 19, "topic": "Biometric Data and AI"}}
{"id": "b187175e-8975-4506-8621-2856add39cb8", "question": "What role does the AI Office play in supporting national competent authorities and how does the Regulation aim to ensure cooperation with third countries?", "reference_answer": "The AI Office assists national competent authorities in developing AI regulatory sandboxes and facilitates cooperation among them, while the Regulation aims to promote effective cooperation with third countries and international organizations.", "reference_context": "Document 345: EN\nUnited in diversity\nEN\n(h)\ncooperate, as appropriate, with other Union institutions, bodies, offices and agencies, as \nwell as relevant Union expert groups and networks, in particular in the fields of product \nsafety, cybersecurity, competition, digital and media services, financial services, \nconsumer protection, data and fundamental rights protection;\n(i)\ncontribute to effective cooperation with the competent authorities of third countries and \nwith international organisations;\n(j)\nassist national competent authorities and the Commission in developing the \norganisational and technical expertise required for the implementation of this \nRegulation, including by contributing to the assessment of training needs for staff of \nMember States involved in implementing this Regulation;\n(k)\nassist the AI Office in supporting national competent authorities in the establishment \nand development of AI regulatory sandboxes, and facilitate cooperation and \ninformation-sharing among AI regulatory sandboxes;\n(l)\ncontribute to, and provide relevant advice on, the development of guidance documents;\n(m)\nadvise the Commission in relation to international matters on AI;\n(n)\nprovide opinions to the Commission on the qualified alerts regarding general-purpose \nAI models;", "conversation_history": [], "metadata": {"question_type": "double", "original_questions": [{"question": "What role does the AI Office play in supporting national competent authorities?", "answer": "The AI Office assists national competent authorities in the establishment and development of AI regulatory sandboxes, and facilitates cooperation and information-sharing among AI regulatory sandboxes."}, {"question": "How does the Regulation aim to ensure cooperation with third countries?", "answer": "The Regulation aims to contribute to effective cooperation with the competent authorities of third countries and with international organisations."}], "seed_document_id": 345, "topic": "AI Regulation in the EU"}}
{"id": "d09d055a-6397-4eb0-9b2c-03f545679c56", "question": "What can be done in such a situation?", "reference_answer": "The market surveillance authority shall require the relevant provider to take all necessary actions to bring the AI system into compliance with the requirements and obligations laid down in this Regulation, as well as take appropriate corrective action within a period the market surveillance authority may prescribe.", "reference_context": "Document 379: EN\nUnited in diversity\nEN\nArticle 80\nProcedure for dealing with AI systems classified by the provider as\nnon-high-risk in application of Annex III\n1.\nWhere a market surveillance authority has sufficient reason to consider that an AI \nsystem classified by the provider as non-high-risk pursuant to Article 6(3) is indeed \nhigh-risk, the market surveillance authority shall carry out an evaluation of the AI \nsystem concerned in respect of its classification as a high-risk AI system based on the \nconditions set out in Article 6(3) and the Commission guidelines.\n2.\nWhere, in the course of that evaluation, the market surveillance authority finds that the \nAI system concerned is high-risk, it shall without undue delay require the relevant \nprovider to take all necessary actions to bring the AI system into compliance with the \nrequirements and obligations laid down in this Regulation, as well as take appropriate \ncorrective action within a period the market surveillance authority may prescribe.\n3.\nWhere the market surveillance authority considers that the use of the AI system \nconcerned is not restricted to its national territory, it shall inform the Commission and \nthe other Member States without undue delay of the results of the evaluation and of the \nactions which it has required the provider to take.", "conversation_history": [{"role": "user", "content": "I am curious about the actions a market surveillance authority can take if it finds that a non-high-risk AI system is actually high-risk."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 379, "topic": "High-Risk AI Regulation"}}
{"id": "dd162568-0811-4aca-8446-06289c3160e2", "question": "What are the specific authorities involved?", "reference_answer": "Market surveillance authorities should have effective investigative and corrective powers, including at least the power to obtain access to all personal data that are being processed and to all information necessary for the performance of its tasks.", "reference_context": "Document 156: EN\nUnited in diversity\nEN\n(159)\nEach market surveillance authority for high-risk AI systems in the area of biometrics, as \nlisted in an annex to this Regulation insofar as those systems are used for the purposes \nof law enforcement, migration, asylum and border control management, or the \nadministration of justice and democratic processes, should have effective investigative \nand corrective powers, including at least the power to obtain access to all personal data \nthat are being processed and to all information necessary for the performance of its \ntasks. The market surveillance authorities should be able to exercise their powers by \nacting with complete independence. Any limitations of their access to sensitive \noperational data under this Regulation should be without prejudice to the powers \nconferred to them by Directive (EU) 2016/680. No exclusion on disclosing data to \nnational data protection authorities under this Regulation should affect the current or \nfuture powers of those authorities beyond the scope of this Regulation.\n(160)\nThe market surveillance authorities and the Commission should be able to propose joint \nactivities, including joint investigations, to be conducted by market surveillance \nauthorities or market surveillance authorities jointly with the Commission, that have the \naim of promoting compliance, identifying non-compliance, raising awareness and \nproviding guidance in relation to this Regulation with respect to specific categories of \nhigh-risk AI systems that are found to present a serious risk across two or more Member \nStates. Joint activities to promote compliance should be carried out in accordance with \nArticle 9 of Regulation (EU) 2019/1020. The AI Office should provide coordination \nsupport for joint investigations.\n\nDocument 366: EN\nUnited in diversity\nEN\n11.\nMarket surveillance authorities and the Commission shall be able to propose joint \nactivities, including joint investigations, to be conducted by either market surveillance \nauthorities or market surveillance authorities jointly with the Commission, that have the \naim of promoting compliance, identifying non-compliance, raising awareness or \nproviding guidance in relation to this Regulation with respect to specific categories of \nhigh-risk AI systems that are found to present a serious risk across two or more Member \nStates in accordance with Article 9 of Regulation (EU) 2019/1020. The AI Office shall \nprovide coordination support for joint investigations.\n12.\nWithout prejudice to the powers provided for under Regulation (EU) 2019/1020, and \nwhere relevant and limited to what is necessary to fulfil their tasks, the market \nsurveillance authorities shall be granted full access by providers to the documentation as \nwell as the training, validation and testing data sets used for the development of high-\nrisk AI systems, including, where appropriate and subject to security safeguards, \nthrough application programming interfaces (API) or other relevant technical means \nand tools enabling remote access.\n\nDocument 365: EN\nUnited in diversity\nEN\n8.\nFor high-risk AI systems listed in point 1 of Annex III to this Regulation, in so far as the \nsystems are used for law enforcement purposes, border management and justice and \ndemocracy, and for high-risk AI systems listed in points 6, 7 and 8 of Annex III to this \nRegulation, Member States shall designate as market surveillance authorities for the \npurposes of this Regulation either the competent data protection supervisory authorities \nunder Regulation (EU) 2016/679 or Directive (EU) 2016/680, or any other authority \ndesignated pursuant to the same conditions laid down in Articles 41 to 44 of Directive \n(EU) 2016/680. Market surveillance activities shall in no way affect the independence of \njudicial authorities, or otherwise interfere with their activities when acting in their \njudicial capacity.\n9.\nWhere Union institutions, bodies, offices or agencies fall within the scope of this \nRegulation, the European Data Protection Supervisor shall act as their market surveillance \nauthority, except in relation to the Court of Justice of the European Union acting in its \njudicial capacity.\n10.\nMember States shall facilitate coordination between market surveillance authorities \ndesignated under this Regulation and other relevant national authorities or bodies which \nsupervise the application of Union harmonisation legislation listed in Annex I, or in other \nUnion law, that might be relevant for the high-risk AI systems referred to in Annex III.\n\nDocument 364: EN\nUnited in diversity\nEN\n5.\nWithout prejudice to the powers of market surveillance authorities under Article 14 of \nRegulation (EU) 2019/1020, for the purpose of ensuring the effective enforcement of \nthis Regulation, market surveillance authorities may exercise the powers referred to in \nArticle 14(4), points (d) and (j), of that Regulation remotely, as appropriate.\n6.\nFor high-risk AI systems placed on the market, put into service, or used by financial \ninstitutions regulated by Union financial services law, the market surveillance authority for \nthe purposes of this Regulation shall be the relevant national authority responsible for the \nfinancial supervision of those institutions under that legislation in so far as the placing on \nthe market, putting into service, or the use of the AI system is in direct connection with \nthe provision of those financial services.\n7.\nBy way of derogation from paragraph 6, in appropriate circumstances, and provided that \ncoordination is ensured, another relevant authority may be identified by the Member \nState as market surveillance authority for the purposes of this Regulation.\nNational market surveillance authorities supervising regulated credit institutions \nregulated under Directive 2013/36/EU, which are participating in the Single Supervisory \nMechanism established by Regulation (EU) No 1024/2013, should report, without delay, \nto the European Central Bank any information identified in the course of their market \nsurveillance activities that may be of potential interest for the prudential supervisory \ntasks of the European Central Bank specified in that Regulation.", "conversation_history": [{"role": "user", "content": "I am interested in the powers granted to market surveillance authorities for high-risk AI systems used in law enforcement and other sensitive areas."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 156, "topic": "AI Regulation and Compliance"}}
{"id": "b566c36d-9afc-473b-ae49-87a15a82472f", "question": "What might those be?", "reference_answer": "Factors include financial benefits gained or losses avoided from the infringement, degree of cooperation with national authorities, degree of responsibility of the operator, how the infringement became known, intentional or negligent character of the infringement, and any actions taken by the operator to mitigate harm.", "reference_context": "Document 406: EN\nUnited in diversity\nEN\n(e)\nany other aggravating or mitigating factor applicable to the circumstances of the \ncase, such as financial benefits gained, or losses avoided, directly or indirectly, \nfrom the infringement;\n(f)\nthe degree of cooperation with the national competent authorities, in order to \nremedy the infringement and mitigate the possible adverse effects of the \ninfringement;\n(g)\nthe degree of responsibility of the operator taking into account the technical and \norganisational measures implemented by it;\n(h)\nthe manner in which the infringement became known to the national competent \nauthorities, in particular whether, and if so to what extent, the operator notified \nthe infringement;\n(i)\nthe intentional or negligent character of the infringement;\n(j)\nany action taken by the operator to mitigate the harm suffered by the affected \npersons.\n8.\nEach Member State shall lay down rules on \u258c to what extent administrative fines may be \nimposed on public authorities and bodies established in that Member State.", "conversation_history": [{"role": "user", "content": "I am looking to understand the factors that may be considered in determining administrative fines for infringements."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 406, "topic": "Others"}}
{"id": "de46408d-7aa8-4aa4-b940-314760c5e279", "question": "What are the requirements?", "reference_answer": "High-risk AI systems should only be placed on the Union market, put into service or used if they comply with certain mandatory requirements that ensure they do not pose unacceptable risks to important Union public interests as recognized and protected by Union law.", "reference_context": "Document 73: EN\nUnited in diversity\nEN\n(64)\nTo mitigate the risks from high-risk AI systems placed on the market or put into service \nand to ensure a high level of trustworthiness, certain mandatory requirements should \napply to high-risk AI systems, taking into account the intended purpose and the context of \nuse of the AI system and according to the risk-management system to be established by the \nprovider. The measures adopted by the providers to comply with the mandatory \nrequirements of this Regulation should take into account the generally acknowledged \nstate of the art on AI, be proportionate and effective to meet the objectives of this \nRegulation. Based on the New Legislative Framework, as clarified in Commission notice \n\u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules 2022\u201d, the general rule is \nthat more than one legal act of Union harmonisation legislation may be applicable to \none product, since the making available or putting into service can take place only when \nthe product complies with all applicable Union harmonisation legislation. The hazards \nof AI systems covered by the requirements of this Regulation concern different aspects \nthan the existing Union harmonisation legislation and therefore the requirements of this \nRegulation would complement the existing body of the Union harmonisation legislation. \nFor example, machinery or medical devices products incorporating an AI system might \npresent risks not addressed by the essential health and safety requirements set out in the \nrelevant Union harmonised legislation, as that sectoral law does not deal with risks \nspecific to AI systems.\n\nDocument 46: EN\nUnited in diversity\nEN\n(46)\nHigh-risk AI systems should only be placed on the Union market, put into service or used \nif they comply with certain mandatory requirements. Those requirements should ensure \nthat high-risk AI systems available in the Union or whose output is otherwise used in the \nUnion do not pose unacceptable risks to important Union public interests as recognised and \nprotected by Union law. On the basis of the New Legislative Framework, as clarified in \nthe Commission notice \u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules \n2022\u201d20, the general rule is that more than one legal act of Union harmonisation \nlegislation, such as Regulations (EU) 2017/74521 and (EU) 2017/74622 of the European \nParliament and of the Council or Directive 2006/42/EC of the European Parliament and \nof the Council23, may be applicable to one product, since the making available or putting \ninto service can take place only when the product complies with all applicable Union \nharmonisation legislation. To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.\n\nDocument 210: EN\nUnited in diversity\nEN\n2.\nWhere a product contains an AI system, to which the requirements of this Regulation as \nwell as requirements of the Union harmonisation legislation listed in Section A of Annex \nI apply, providers shall be responsible for ensuring that their product is fully compliant \nwith all applicable requirements under applicable Union harmonisation legislation. In \nensuring the compliance of high-risk AI systems referred to in paragraph 1 with the \nrequirements set out in this Section, and in order to ensure consistency, avoid \nduplication and minimise additional burdens, providers shall have a choice of \nintegrating, as appropriate, the necessary testing and reporting processes, information \nand documentation they provide with regard to their product into documentation and \nprocedures that already exist and are required under the Union harmonisation \nlegislation listed in Section A of Annex I.\nArticle 9\nRisk management system\n1.\nA risk management system shall be established, implemented, documented and maintained \nin relation to high-risk AI systems.\n\nDocument 74: EN\nUnited in diversity\nEN\nThis calls for a simultaneous and complementary application of the various legislative \nacts. To ensure consistency and to avoid an unnecessary administrative burden and \nunnecessary costs, providers of a product that contains one or more high-risk AI system, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nbased on the New Legislative Framework and listed in an annex to this Regulation \napply, should have flexibility with regard to operational decisions on how to ensure \ncompliance of a product that contains one or more AI systems with all the applicable \nrequirements of that Union harmonised legislation in an optimal manner. That \nflexibility could mean, for example a decision by the provider to integrate a part of the \nnecessary testing and reporting processes, information and documentation required \nunder this Regulation into already existing documentation and procedures required \nunder existing Union harmonisation legislation based on the New Legislative \nFramework and listed in an annex to this Regulation. This should not, in any way, \nundermine the obligation of the provider to comply with all the applicable requirements.", "conversation_history": [{"role": "user", "content": "I need to know what high-risk AI systems must comply with before being placed on the Union market."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 73, "topic": "Others"}}
{"id": "f399e111-6c2d-4c7d-9894-1e493dbe5eaa", "question": "What details are required?", "reference_answer": "The EU declaration of conformity shall contain the AI system name and type, the name and address of the provider, a statement of sole responsibility, a statement of conformity with this Regulation and relevant Union law, a statement of compliance with data processing regulations if applicable, references to relevant harmonised standards, information about the notified body and conformity assessment procedure if applicable, and the place and date of issue, along with the name and function of the signer.", "reference_context": "Document 449: EN\nUnited in diversity\nEN\nANNEX V\nEU declaration of conformity\nThe EU declaration of conformity referred to in Article 47, shall contain all of the following \ninformation:\n1.\nAI system name and type and any additional unambiguous reference allowing the \nidentification and traceability of the AI system;\n2.\nThe name and address of the provider or, where applicable, of their authorised \nrepresentative;\n3.\nA statement that the EU declaration of conformity referred to in Article 47 is issued under \nthe sole responsibility of the provider;\n4.\nA statement that the AI system is in conformity with this Regulation and, if applicable, \nwith any other relevant Union law that provides for the issuing of the EU declaration of \nconformity referred to in Article 47;\n5.\nWhere an AI system involves the processing of personal data, a statement that that AI \nsystem complies with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive \n(EU) 2016/680;\n6.\nReferences to any relevant harmonised standards used or any other common specification \nin relation to which conformity is declared;\n7.\nWhere applicable, the name and identification number of the notified body, a description \nof the conformity assessment procedure performed, and identification of the certificate \nissued;\n8.\nThe place and date of issue of the declaration, the name and function of the person who \nsigned it, as well as an indication for, or on behalf of whom, that person signed, a \nsignature.", "conversation_history": [{"role": "user", "content": "I need to know what information must be included in the EU declaration of conformity according to Article 47."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 449, "topic": "Others"}}
{"id": "ae9f7eab-8c5c-4421-bf20-77a3f4340577", "question": "What is needed for compliance?", "reference_answer": "High-risk AI systems should demonstrate compliance by fulfilling the essential cybersecurity requirements set out in a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements.", "reference_context": "Document 86: EN\nUnited in diversity\nEN\n(78)\nThe conformity assessment procedure provided by this Regulation should apply in \nrelation to the essential cybersecurity requirements of a product with digital elements \ncovered by a regulation of the European Parliament and of the Council on horizontal \ncybersecurity requirements for products with digital elements and classified as a high-\nrisk AI system under this Regulation. However, this rule should not result in reducing \nthe necessary level of assurance for critical products with digital elements covered by a \nregulation of the European Parliament and of the Council on horizontal cybersecurity \nrequirements for products with digital elements. Therefore, by way of derogation from \nthis rule, high-risk AI systems that fall within the scope of this Regulation and are also \nqualified as important and critical products with digital elements pursuant to a \nregulation of the European Parliament and of the Council on horizontal cybersecurity \nrequirements for products with digital elements and to which the conformity assessment \nprocedure based on internal control set out in an annex to this Regulation applies, are \nsubject to the conformity assessment provisions of a regulation of the European \nParliament and of the Council on horizontal cybersecurity requirements for products \nwith digital elements insofar as the essential cybersecurity requirements of that \nregulation are concerned. In this case, for all the other aspects covered by this \nRegulation the respective provisions on conformity assessment based on internal control \nset out in an annex to this Regulation should apply. Building on the knowledge and \nexpertise of ENISA on the cybersecurity policy and tasks assigned to ENISA under the \nRegulation (EU) 2019/881 of the European Parliament and of the Council37, the \nCommission should cooperate with ENISA on issues related to cybersecurity of AI \nsystems.\n37\nRegulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 \non ENISA (the European Union Agency for Cybersecurity) and on information and \ncommunications technology cybersecurity certification and repealing Regulation (EU) No \n526/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 15).\n\nDocument 85: EN\nUnited in diversity\nEN\n(77)\nWithout prejudice to the requirements related to robustness and accuracy set out in this \nRegulation, high-risk AI systems which fall within the scope of a regulation of the \nEuropean Parliament and of the Council on horizontal cybersecurity requirements for \nproducts with digital elements, in accordance with that regulation may demonstrate \ncompliance with the cybersecurity requirements of this Regulation by fulfilling the \nessential cybersecurity requirements set out in that regulation. When high-risk AI \nsystems fulfil the essential requirements of a regulation of the European Parliament and \nof the Council on horizontal cybersecurity requirements for products with digital \nelements, they should be deemed compliant with the cybersecurity requirements set out \nin this Regulation in so far as the achievement of those requirements is demonstrated in \nthe EU declaration of conformity or parts thereof issued under that regulation. To that \nend, the assessment of the cybersecurity risks, associated to a product with digital \nelements classified as high-risk AI system according to this Regulation, carried out \nunder a regulation of the European Parliament and of the Council on horizontal \ncybersecurity requirements for products with digital elements, should consider risks to \nthe cyber resilience of an AI system as regards attempts by unauthorised third parties to \nalter its use, behaviour or performance, including AI specific vulnerabilities such as \ndata poisoning or adversarial attacks, as well as, as relevant, risks to fundamental rights \nas required by this Regulation.", "conversation_history": [{"role": "user", "content": "I am looking for what high-risk AI systems should demonstrate to comply with the cybersecurity requirements of this Regulation."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 86, "topic": "High-Risk AI Regulation"}}
{"id": "a5496d31-72d2-467f-a7dc-466236efc3c5", "question": "What actions are necessary?", "reference_answer": "Providers of general-purpose AI models must put in place a policy to comply with Union law on copyright and related rights, in particular to identify and comply with the reservation of rights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790.", "reference_context": "Document 303: EN\nUnited in diversity\nEN\n(ii)\ncontain, at a minimum, the elements set out in Annex XII;\n(c)\nput in place a policy to comply with Union law on copyright and related rights, and \nin particular to identify and comply with, including through state-of-the-art \ntechnologies, a reservation of rights expressed pursuant to Article 4(3) of Directive \n(EU) 2019/790;\n(d)\ndraw up and make publicly available a sufficiently detailed summary about the \ncontent used for training of the general-purpose AI model, according to a template \nprovided by the AI Office.\n2.\nThe obligations set out in paragraph 1, points (a) and (b), shall not apply to providers of \nAI models that are released under a free and open-source licence that allows for the \naccess, usage, modification, and distribution of the model, and whose parameters, \nincluding the weights, the information on the model architecture, and the information \non model usage, are made publicly available. This exception shall not apply to general-\npurpose AI models with systemic risks.\n3.\nProviders of general-purpose AI models shall cooperate as necessary with the \nCommission and the national competent authorities in the exercise of their competences \nand powers pursuant to this Regulation.\n\nDocument 105: EN\nUnited in diversity\nEN\n(104)\nThe providers of general-purpose AI models that are released under a free and open-\nsource licence, and whose parameters, including the weights, the information on the \nmodel architecture, and the information on model usage, are made publicly available \nshould be subject to exceptions as regards the transparency-related requirements \nimposed on general-purpose AI models, unless they can be considered to present a \nsystemic risk, in which case the circumstance that the model is transparent and \naccompanied by an open-source license should not be considered to be a sufficient \nreason to exclude compliance with the obligations under this Regulation. In any case, \ngiven that the release of general-purpose AI models under free and open-source licence \ndoes not necessarily reveal substantial information on the data set used for the training \nor fine-tuning of the model and on how compliance of copyright law was thereby \nensured, the exception provided for general-purpose AI models from compliance with \nthe transparency-related requirements should not concern the obligation to produce a \nsummary about the content used for model training and the obligation to put in place a \npolicy to comply with Union copyright law, in particular to identify and comply with the \nreservation of rights pursuant to Article 4(3) of Directive (EU) 2019/790 of the \nEuropean Parliament and of the Council40.\n40\nDirective (EU) 2019/790 of the European Parliament and of the Council of 17 April 2019 on \ncopyright and related rights in the Digital Single Market and amending Directives 96/9/EC \nand 2001/29/EC (OJ L 130, 17.5.2019, p. 92).\n\nDocument 107: EN\nUnited in diversity\nEN\n(106)\nProviders that place general-purpose AI models on the Union market should ensure \ncompliance with the relevant obligations in this Regulation. To that end, providers of \ngeneral-purpose AI models should put in place a policy to comply with Union law on \ncopyright and related rights, in particular to identify and comply with the reservation of \nrights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790. \nAny provider placing a general-purpose AI model on the Union market should comply \nwith this obligation, regardless of the jurisdiction in which the copyright-relevant acts \nunderpinning the training of those general-purpose AI models take place. This is \nnecessary to ensure a level playing field among providers of general-purpose AI models \nwhere no provider should be able to gain a competitive advantage in the Union market \nby applying lower copyright standards than those provided in the Union.", "conversation_history": [{"role": "user", "content": "I am interested in what providers of general-purpose AI models must do to comply with Union law on copyright and related rights."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 303, "topic": "Others"}}
{"id": "b57ccd11-b0dc-449d-aeb9-a5387a959719", "question": "What are the specific factors involved?", "reference_answer": "An AI system is classified as high-risk if it is intended to be used as a safety component of a product or is itself a product covered by Union harmonisation legislation, and if the product is required to undergo a third-party conformity assessment.", "reference_context": "Document 57: EN\nUnited in diversity\nEN\n(52)\nAs regards stand-alone AI systems, namely high-risk AI systems other than those that are \nsafety components of products, or that are themselves products, it is appropriate to classify \nthem as high-risk if, in light of their intended purpose, they pose a high risk of harm to the \nhealth and safety or the fundamental rights of persons, taking into account both the severity \nof the possible harm and its probability of occurrence and they are used in a number of \nspecifically pre-defined areas specified in this Regulation. The identification of those \nsystems is based on the same methodology and criteria envisaged also for any future \namendments of the list of high-risk AI systems that the Commission should be \nempowered to adopt, via delegated acts, to take into account the rapid pace of \ntechnological development, as well as the potential changes in the use of AI systems.\n\nDocument 56: EN\nUnited in diversity\nEN\n(50)\nAs regards AI systems that are safety components of products, or which are themselves \nproducts, falling within the scope of certain Union harmonisation legislation listed in an \nannex to this Regulation, it is appropriate to classify them as high-risk under this \nRegulation if the product concerned undergoes the conformity assessment procedure with a \nthird-party conformity assessment body pursuant to that relevant Union harmonisation \nlegislation. In particular, such products are machinery, toys, lifts, equipment and protective \nsystems intended for use in potentially explosive atmospheres, radio equipment, pressure \nequipment, recreational craft equipment, cableway installations, appliances burning \ngaseous fuels, medical devices, in vitro diagnostic medical devices, automotive and \naviation.\n(51)\nThe classification of an AI system as high-risk pursuant to this Regulation should not \nnecessarily mean that the product whose safety component is the AI system, or the AI \nsystem itself as a product, is considered to be high-risk under the criteria established in the \nrelevant Union harmonisation legislation that applies to the product. This is, in particular, \nthe case for Regulations (EU) 2017/745 and (EU) 2017/746, where a third-party \nconformity assessment is provided for medium-risk and high-risk products.\n\nDocument 202: EN\nUnited in diversity\nEN\nCHAPTER III\nHIGH-RISK AI SYSTEMS\nSection 1\nClassification of AI systems as high-risk\nArticle 6\nClassification rules for high-risk AI systems\n1.\nIrrespective of whether an AI system is placed on the market or put into service \nindependently of the products referred to in points (a) and (b), that AI system shall be \nconsidered to be high-risk where both of the following conditions are fulfilled:\n(a)\nthe AI system is intended to be used as a safety component of a product, or the AI \nsystem is itself a product, covered by the Union harmonisation legislation listed in \nAnnex I;\n(b)\nthe product whose safety component pursuant to point (a) is the AI system, or the \nAI system itself as a product, is required to undergo a third-party conformity \nassessment, with a view to the placing on the market or the putting into service of \nthat product pursuant to the Union harmonisation legislation listed in Annex I.", "conversation_history": [{"role": "user", "content": "I am interested in the criteria that classify an AI system as high-risk under the Regulation."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 57, "topic": "High-Risk AI Systems"}}
{"id": "d23feb94-fc22-403e-8653-cd1973e07bd2", "question": "Who holds those exclusive powers?", "reference_answer": "The Commission shall have exclusive powers to supervise and enforce Chapter V.", "reference_context": "Document 388: EN\nUnited in diversity\nEN\nSection 5\nSupervision, investigation, enforcement and monitoring in respect of \nproviders of general-purpose AI models\nArticle 88\nEnforcement of the obligations of providers of general-purpose AI models\n1.\nThe Commission shall have exclusive powers to supervise and enforce Chapter V, taking \ninto account the procedural guarantees under Article 94. The Commission shall entrust \nthe implementation of these tasks to the AI Office, without prejudice to the powers of \norganisation of the Commission and the division of competences between Member States \nand the Union based on the Treaties.\n2.\nWithout prejudice to Article 75(3), market surveillance authorities may request the \nCommission to exercise the powers laid down in this Section, where that is necessary \nand proportionate to assist with the fulfilment of their tasks under this Regulation.", "conversation_history": [{"role": "user", "content": "I am interested in knowing who has exclusive powers to supervise and enforce Chapter V concerning providers of general-purpose AI models."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 388, "topic": "AI Regulation and Governance"}}
{"id": "f76cb6d1-7c1c-43f4-98f5-108d40e55565", "question": "What options are available?", "reference_answer": "The Commission, through the AI Office and after having informed the Board, may exercise the powers laid down in this Section for the purpose of assessing the matter.", "reference_context": "Document 390: EN\nUnited in diversity\nEN\nArticle 90\nAlerts of systemic risks by the scientific panel\n1.\nThe scientific panel may provide a qualified alert to the AI Office where it has reason to \nsuspect that:\n(a)\na general-purpose AI model poses concrete identifiable risk at Union level; or,\n(b)\na general-purpose AI model meets the conditions referred to in Article 51.\n2.\nUpon such qualified alert, the Commission, through the AI Office and after having \ninformed the Board, may exercise the powers laid down in this Section for the purpose of \nassessing the matter. The AI Office shall inform the Board of any measure according to \nArticles 91 to 94.\n3.\nA qualified alert shall be duly reasoned and indicate at least:\n(a)\nthe point of contact of the provider of the general-purpose AI model with systemic \nrisk concerned;", "conversation_history": [{"role": "user", "content": "I'm curious about the actions the Commission can take upon receiving a qualified alert from the scientific panel."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 390, "topic": "Systemic Risks in AI"}}
{"id": "4a2b9588-ca53-4d29-8c32-3495b3fff5f4", "question": "What do they need to adhere to?", "reference_answer": "High-risk AI systems should only be placed on the Union market, put into service or used if they comply with certain mandatory requirements that ensure they do not pose unacceptable risks to important Union public interests as recognized and protected by Union law.", "reference_context": "Document 174: EN\nUnited in diversity\nEN\n(d)\nimporters and distributors of AI systems;\n(e)\nproduct manufacturers placing on the market or putting into service an AI system \ntogether with their product and under their own name or trademark;\n(f)\nauthorised representatives of providers, which are not established in the Union;\n(g)\naffected persons that are located in the Union.\n2.\nFor \u258c AI systems classified as high-risk AI systems in accordance with Article 6(1) \nrelated to products covered by the Union harmonisation legislation listed in Section B of \nAnnex I, only Article 6(1), Articles 102 to 109 and Article 112 apply. Article 57 applies \nonly in so far as the requirements for high-risk AI systems under this Regulation have \nbeen integrated in that Union harmonisation legislation.\n\u258c \n3.\nThis Regulation does not apply to areas outside the scope of Union law, and shall not, in \nany event, affect the competences of the Member States concerning national security, \nregardless of the type of entity entrusted by the Member States with carrying out tasks in \nrelation to those competences.\n\nDocument 46: EN\nUnited in diversity\nEN\n(46)\nHigh-risk AI systems should only be placed on the Union market, put into service or used \nif they comply with certain mandatory requirements. Those requirements should ensure \nthat high-risk AI systems available in the Union or whose output is otherwise used in the \nUnion do not pose unacceptable risks to important Union public interests as recognised and \nprotected by Union law. On the basis of the New Legislative Framework, as clarified in \nthe Commission notice \u201cThe \u2018Blue Guide\u2019 on the implementation of EU product rules \n2022\u201d20, the general rule is that more than one legal act of Union harmonisation \nlegislation, such as Regulations (EU) 2017/74521 and (EU) 2017/74622 of the European \nParliament and of the Council or Directive 2006/42/EC of the European Parliament and \nof the Council23, may be applicable to one product, since the making available or putting \ninto service can take place only when the product complies with all applicable Union \nharmonisation legislation. To ensure consistency and avoid unnecessary administrative \nburdens or costs, providers of a product that contains one or more high-risk AI systems, \nto which the requirements of this Regulation and of the Union harmonisation legislation \nlisted in an annex to this Regulation apply, should have flexibility with regard to \noperational decisions on how to ensure compliance of a product that contains one or \nmore AI systems with all applicable requirements of the Union harmonisation legislation \nin an optimal manner. AI systems identified as high-risk should be limited to those that \nhave a significant harmful impact on the health, safety and fundamental rights of persons \nin the Union and such limitation should minimise any potential restriction to international \ntrade.\n20 \nOJ C 247, 29.6.2022, p. 1.", "conversation_history": [{"role": "user", "content": "I am interested in high-risk AI systems and their compliance requirements for placement on the Union market."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 174, "topic": "Others"}}
{"id": "487cf54e-9cda-4352-85cf-4a804ab36c4c", "question": "What are they suggesting?", "reference_answer": "The Commission may encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content.", "reference_context": "Document 131: EN\nUnited in diversity\nEN\n(135)\nWithout prejudice to the mandatory nature and full applicability of the transparency \nobligations, the Commission may also encourage and facilitate the drawing up of codes \nof practice at Union level to facilitate the effective implementation of the obligations \nregarding the detection and labelling of artificially generated or manipulated content, \nincluding to support practical arrangements for making, as appropriate, the detection \nmechanisms accessible and facilitating cooperation with other actors along the value \nchain, disseminating content or checking its authenticity and provenance to enable the \npublic to effectively distinguish AI-generated content.", "conversation_history": [{"role": "user", "content": "I am interested in what the Commission encourages to facilitate the implementation of obligations regarding AI-generated content."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 131, "topic": "AI Regulation in the EU"}}
{"id": "9f075dae-8123-48ad-8e5d-641fa5a963b4", "question": "What should it convey?", "reference_answer": "A qualified alert shall be duly reasoned and indicate at least the point of contact of the provider of the general-purpose AI model with systemic risk concerned.", "reference_context": "Document 390: EN\nUnited in diversity\nEN\nArticle 90\nAlerts of systemic risks by the scientific panel\n1.\nThe scientific panel may provide a qualified alert to the AI Office where it has reason to \nsuspect that:\n(a)\na general-purpose AI model poses concrete identifiable risk at Union level; or,\n(b)\na general-purpose AI model meets the conditions referred to in Article 51.\n2.\nUpon such qualified alert, the Commission, through the AI Office and after having \ninformed the Board, may exercise the powers laid down in this Section for the purpose of \nassessing the matter. The AI Office shall inform the Board of any measure according to \nArticles 91 to 94.\n3.\nA qualified alert shall be duly reasoned and indicate at least:\n(a)\nthe point of contact of the provider of the general-purpose AI model with systemic \nrisk concerned;", "conversation_history": [{"role": "user", "content": "I am curious about what a qualified alert from the scientific panel must indicate."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 390, "topic": "Systemic Risks in AI"}}
{"id": "81c1208e-74ed-407f-abed-210989eaa566", "question": "What are the differences?", "reference_answer": "Remote biometric identification systems are intended for the identification of natural persons without their active involvement, while biometric verification systems are intended to confirm that a specific natural person is who they claim to be for access to services or security.", "reference_context": "Document 17: EN\nUnited in diversity\nEN\n(17)\nThe notion of \u2018remote biometric identification system\u2019 referred to in this Regulation should \nbe defined functionally, as an AI system intended for the identification of natural persons \nwithout their active involvement, typically at a distance, through the comparison of a \nperson\u2019s biometric data with the biometric data contained in a reference database, \nirrespectively of the particular technology, processes or types of biometric data used. \nSuch remote biometric identification systems are typically used to perceive multiple \npersons or their behaviour simultaneously in order to facilitate significantly the \nidentification of natural persons without their active involvement. This excludes AI \nsystems intended to be used for biometric verification, which includes authentication, the \nsole purpose of which is to confirm that a specific natural person is the person he or she \nclaims to be and to confirm the identity of a natural person for the sole purpose of \nhaving access to a service, unlocking a device or having security access to premises. \nThat exclusion is justified by the fact that such systems are likely to have a minor impact \non fundamental rights of natural persons compared to the remote biometric \nidentification systems which may be used for the processing of the biometric data of a \nlarge number of persons without their active involvement. In the case of \u2018real-time\u2019 \nsystems, the capturing of the biometric data, the comparison and the identification occur all \ninstantaneously, near-instantaneously or in any event without a significant delay. In this \nregard, there should be no scope for circumventing the rules of this Regulation on the \u2018real-\ntime\u2019 use of the AI systems concerned by providing for minor delays. \u2018Real-time\u2019 systems \ninvolve the use of \u2018live\u2019 or \u2018near-live\u2019 material, such as video footage, generated by a \ncamera or other device with similar functionality. In the case of \u2018post\u2019 systems, in contrast, \nthe biometric data has already been captured and the comparison and identification occur \nonly after a significant delay. This involves material, such as pictures or video footage \ngenerated by closed circuit television cameras or private devices, which has been generated \nbefore the use of the system in respect of the natural persons concerned.", "conversation_history": [{"role": "user", "content": "I am looking to understand what distinguishes remote biometric identification systems from biometric verification systems according to the Regulation."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 17, "topic": "Biometric Data and AI"}}
{"id": "03c41c5c-a552-45b4-9158-9a9d39c96620", "question": "What should be considered?", "reference_answer": "The requirements set out in Chapter III, Section 2, of Regulation (EU) 2024/\u2026 shall be taken into account.", "reference_context": "Document 420: EN\nUnited in diversity\nEN\n(6)\nin Article 58, the following paragraph is added:\n\u20183.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019.\nArticle 109\nAmendment to Regulation (EU) 2019/2144\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\u20183.\nWhen adopting the implementing acts pursuant to paragraph 2, concerning artificial \nintelligence systems which are safety components within the meaning of Regulation (EU) \n2024/\u2026 of the European Parliament and of the Council*++, the requirements set out in \nChapter III, Section 2, of that Regulation shall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n++\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n\nDocument 419: EN\nUnited in diversity\nEN\n(3)\nin Article 43, the following paragraph is added:\n\u20184.\nWhen adopting implementing acts pursuant to paragraph 1 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n(4)\nin Article 47, the following paragraph is added:\n\u20183.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n(5)\nin Article 57, the following subparagraph is added:\n\u2018When adopting those implementing acts concerning Artificial Intelligence systems which \nare safety components within the meaning of Regulation (EU) 2024/\u2026+, the requirements \nset out in Chapter III, Section 2, of that Regulation shall be taken into account.\u2019;\n+\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n\nDocument 418: EN\nUnited in diversity\nEN\nArticle 108\nAmendments to Regulation (EU) 2018/1139\nRegulation (EU) 2018/1139 is amended as follows:\n(1)\nin Article 17, the following paragraph is added:\n\u20183.\nWithout prejudice to paragraph 2, when adopting implementing acts pursuant to \nparagraph 1 concerning Artificial Intelligence systems which are safety components \nwithin the meaning of Regulation (EU) 2024/\u2026 of the European Parliament and of \nthe Council*+, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying \ndown harmonised rules on artificial intelligence and amending Regulations (EC) No \n300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 \nand (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) \n2020/1828 (Artificial Intelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019;\n(2)\nin Article 19, the following paragraph is added:\n\u20184.\nWhen adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial \nIntelligence systems which are safety components within the meaning of Regulation \n(EU) 2024/\u2026++, the requirements set out in Chapter III, Section 2, of that Regulation \nshall be taken into account.\u2019;\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.\n++\nOJ: Please insert the number of this Regulation (2021/0106(COD)).\n\nDocument 416: EN\nUnited in diversity\nEN\nArticle 106\nAmendment to Directive (EU) 2016/797\nIn Article 5 of Directive (EU) 2016/797, the following paragraph is added:\n\u201812.\nWhen adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to \nparagraph 11 concerning Artificial Intelligence systems which are safety components \nwithin the meaning of Regulation (EU) 2024/\u2026 of the European Parliament and of the \nCouncil*+, the requirements set out in Chapter III, Section 2, of that Regulation shall be \ntaken into account.\n________________\n*\nRegulation (EU) 2024/\u2026 of the European Parliament and of the Council of \u2026 laying down \nharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, \n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) \n2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial \nIntelligence Act) (OJ L, \u2026, ELI: \u2026).\u2019\n+\nOJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete \nthe corresponding footnote.", "conversation_history": [{"role": "user", "content": "I need to know what must be taken into account when adopting delegated acts concerning safety components of Artificial Intelligence systems."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 420, "topic": "Artificial Intelligence Regulation"}}
{"id": "3a3a705d-89c6-468f-8463-11724f625c9e", "question": "What does it say about this?", "reference_answer": "The Commission shall develop Union expertise and capabilities in the field of AI through the AI Office.", "reference_context": "Document 339: EN\nUnited in diversity\nEN\n2.\nParagraph 1 of this Article shall not be interpreted as exempting those operators from \nfulfilling any other requirements or obligations laid down in this Regulation, including \nthose established in Articles 9, 10, 11, 12, 13, 14, 15, 72 and 73.\nCHAPTER VII\nGOVERNANCE\nSection 1\nGovernance at Union level\nArticle 64\nAI Office\n1.\nThe Commission shall develop Union expertise and capabilities in the field of AI \nthrough the AI Office.\n2.\nMember States shall facilitate the tasks entrusted to the AI Office, as reflected in this \nRegulation.", "conversation_history": [{"role": "user", "content": "I am looking for the role of the AI Office as stated in Article 64."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 339, "topic": "Others"}}
{"id": "e33f12ab-0936-409b-9acb-9486e82d78fb", "question": "What is that?", "reference_answer": "AI-enabled manipulative techniques that can be used to persuade persons to engage in unwanted behaviours or to deceive them by nudging them into decisions that subvert and impair their autonomy, decision-making, and free choices should be prohibited.", "reference_context": "Document 29: EN\nUnited in diversity\nEN\n(28)\nAside from the many beneficial uses of AI, it can also be misused and provide novel and \npowerful tools for manipulative, exploitative and social control practices. Such practices \nare particularly harmful and abusive and should be prohibited because they contradict \nUnion values of respect for human dignity, freedom, equality, democracy and the rule of \nlaw and fundamental rights enshrined in the Charter, including the right to non-\ndiscrimination, to data protection and to privacy and the rights of the child.\n(29)\nAI-enabled manipulative techniques can be used to persuade persons to engage in \nunwanted behaviours, or to deceive them by nudging them into decisions in a way that \nsubverts and impairs their autonomy, decision-making and free choices. The placing on \nthe market, the putting into service or the use of certain AI systems with the objective to or \nthe effect of materially distorting human behaviour, whereby significant harms, in \nparticular having sufficiently important adverse impacts on physical, psychological \nhealth or financial interests are likely to occur, are particularly dangerous and should \ntherefore be prohibited. Such AI systems deploy subliminal components such as audio, \nimage, video stimuli that persons cannot perceive, as those stimuli are beyond human \nperception, or other manipulative or deceptive techniques that subvert or impair person\u2019s \nautonomy, decision-making or free choice in ways that people are not consciously aware \nof those techniques or, where they are aware of them, can still be deceived or are not \nable to control or resist them. This could be facilitated, for example, by machine-brain \ninterfaces or virtual reality as they allow for a higher degree of control of what stimuli \nare presented to persons, insofar as they may materially distort their behaviour in a \nsignificantly harmful manner. In addition, AI systems may also otherwise exploit the \nvulnerabilities of a person or a specific group of persons due to their age, disability within \nthe meaning of Directive (EU) 2019/882 of the European Parliament and of the \nCouncil16, or a specific social or economic situation that is likely to make those persons \nmore vulnerable to exploitation such as persons living in extreme poverty, ethnic or \nreligious minorities.", "conversation_history": [{"role": "user", "content": "I am looking for something that should be prohibited due to its harmful and abusive nature according to the text."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 29, "topic": "AI Regulation and Ethics"}}
{"id": "c1bee9d4-4361-4cd0-a3a5-6914dc4e0a07", "question": "What does it mean?", "reference_answer": "'AI system' means a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.", "reference_context": "Document 178: EN\nUnited in diversity\nEN\nArticle 3\nDefinitions\nFor the purposes of this Regulation, the following definitions apply:\n(1)\n\u2018AI system\u2019 means a machine-based system that is designed to operate with varying \nlevels of autonomy and that may exhibit adaptiveness after deployment, and that, for \nexplicit or implicit objectives, infers, from the input it receives, how to generate outputs \nsuch as predictions, content, recommendations, or decisions that can influence physical \nor virtual environments;\n(2)\n\u2018risk\u2019 means the combination of the probability of an occurrence of harm and the \nseverity of that harm;\n(3)\n\u2018provider\u2019 means a natural or legal person, public authority, agency or other body that \ndevelops an AI system or a general-purpose AI model or that has an AI system or a \ngeneral-purpose AI model developed and places it on the market or puts the AI system \ninto service under its own name or trademark, whether for payment or free of charge;", "conversation_history": [{"role": "user", "content": "I am looking to understand what the term 'AI system' refers to in this Regulation."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 178, "topic": "AI Regulation and Ethics"}}
{"id": "0fd9138d-74e5-4d82-af44-c6b47fc2d434", "question": "What is its objective?", "reference_answer": "The purpose of the information requirement is to ensure information of workers and their representatives on the planned deployment of high-risk AI systems at the workplace, which is necessary to protect fundamental rights.", "reference_context": "Document 96: EN\nUnited in diversity\nEN\n(92)\nThis Regulation is without prejudice to obligations for employers to inform or to inform \nand consult workers or their representatives under Union or national law and practice, \nincluding Directive 2002/14/EC of the European Parliament and of the Council39, on \ndecisions to put into service or use AI systems. It remains necessary to ensure \ninformation of workers and their representatives on the planned deployment of high-risk \nAI systems at the workplace where the conditions for those information or information \nand consultation obligations in other legal instruments are not fulfilled. Moreover, such \ninformation right is ancillary and necessary to the objective of protecting fundamental \nrights that underlies this Regulation. Therefore, an information requirement to that \neffect should be laid down in this Regulation, without affecting any existing rights of \nworkers.\n39\nDirective 2002/14/EC of the European Parliament and of the Council of 11 March 2002 \nestablishing a general framework for informing and consulting employees in the European \nCommunity (OJ L 80, 23.3.2002, p. 29).", "conversation_history": [{"role": "user", "content": "I'm looking to understand the purpose of the information requirement laid down in this Regulation concerning AI systems."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 96, "topic": "AI Regulation and Compliance"}}
{"id": "31dc1e08-9da2-4c4c-937a-a8395a67d5a2", "question": "What is their function?", "reference_answer": "The AI regulatory sandboxes shall be designed and implemented to facilitate cross-border cooperation between national competent authorities and to ensure compliance with the Regulation.", "reference_context": "Document 319: EN\nUnited in diversity\nEN\n13.\nThe AI regulatory sandboxes shall be designed and implemented in such a way that, \nwhere relevant, they facilitate cross-border cooperation between national competent \nauthorities.\n14.\nNational competent authorities \u258c shall coordinate their activities and cooperate within the \nframework of the \u258c Board. \u258c\n15.\nNational competent authorities shall inform the AI Office and the Board of the \nestablishment of a sandbox, and may ask them for support and guidance. The AI Office \nshall make publicly available a list of planned and existing sandboxes and keep it up to \ndate in order to encourage more interaction in the AI regulatory sandboxes and cross-\nborder cooperation.\n\nDocument 135: EN\nUnited in diversity\nEN\nWhere appropriate, national competent authorities establishing AI regulatory sandboxes \nshould cooperate with other relevant authorities, including those supervising the \nprotection of fundamental rights, and could allow for the involvement of other actors \nwithin the AI ecosystem such as national or European standardisation organisations, \nnotified bodies, testing and experimentation facilities, research and experimentation \nlabs, European Digital Innovation Hubs and relevant stakeholder and civil society \norganisations. To ensure uniform implementation across the Union and economies of \nscale, it is appropriate to establish common rules for the AI regulatory sandboxes\u2019 \nimplementation and a framework for cooperation between the relevant authorities \ninvolved in the supervision of the sandboxes. AI regulatory sandboxes established under \nthis Regulation should be without prejudice to other law allowing for the establishment \nof other sandboxes aiming to ensure compliance with law other than this Regulation. \nWhere appropriate, relevant competent authorities in charge of those other regulatory \nsandboxes should consider the benefits of using those sandboxes also for the purpose of \nensuring compliance of AI systems with this Regulation. Upon agreement between the \nnational competent authorities and the participants in the AI regulatory sandbox, testing \nin real world conditions may also be operated and supervised in the framework of the AI \nregulatory sandbox.\n\nDocument 314: EN\nUnited in diversity\nEN\n2.\nAdditional AI regulatory sandboxes at regional or local level, or established jointly with \nthe competent authorities of other Member States may also be established.\n3.\nThe European Data Protection Supervisor may also establish an AI regulatory sandbox \nfor Union institutions, bodies, offices and agencies, and may exercise the roles and the \ntasks of national competent authorities in accordance with this Chapter.\n4.\nMember States shall ensure that the competent authorities referred to in paragraphs 1 \nand 2 allocate sufficient resources to comply with this Article effectively and in a timely \nmanner. Where appropriate, national competent authorities shall cooperate with other \nrelevant authorities, and may allow for the involvement of other actors within the AI \necosystem. This Article shall not affect other regulatory sandboxes established under \nUnion or national law. Member States shall ensure an appropriate level of cooperation \nbetween the authorities supervising those other sandboxes and the national competent \nauthorities.", "conversation_history": [{"role": "user", "content": "I am interested in the purpose of the AI regulatory sandboxes based on the provided context."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 319, "topic": "AI Regulatory Sandboxes"}}
{"id": "231a233b-c5e6-4552-a0ee-98be94f60ad3", "question": "What is required?", "reference_answer": "The application for notification shall be accompanied by a description of the conformity assessment activities, the conformity assessment module or modules and the types of AI systems for which the conformity assessment body claims to be competent, as well as by an accreditation certificate, where one exists, issued by a national accreditation body attesting that the conformity assessment body fulfils the requirements laid down in Article 31.", "reference_context": "Document 259: EN\nUnited in diversity\nEN\nArticle 29\nApplication of a conformity assessment body for notification \n1.\nConformity assessment bodies shall submit an application for notification to the notifying \nauthority of the Member State in which they are established.\n2.\nThe application for notification shall be accompanied by a description of the conformity \nassessment activities, the conformity assessment module or modules and the types of AI \nsystems for which the conformity assessment body claims to be competent, as well as by \nan accreditation certificate, where one exists, issued by a national accreditation body \nattesting that the conformity assessment body fulfils the requirements laid down in \nArticle 31. \nAny valid document related to existing designations of the applicant notified body under \nany other Union harmonisation legislation shall be added.\n3.\nWhere the conformity assessment body concerned cannot provide an accreditation \ncertificate, it shall provide the notifying authority with all the documentary evidence \nnecessary for the verification, recognition and regular monitoring of its compliance with \nthe requirements laid down in Article 31. \n4.\nFor notified bodies which are designated under any other Union harmonisation legislation, \nall documents and certificates linked to those designations may be used to support their \ndesignation procedure under this Regulation, as appropriate. The notified body shall \nupdate the documentation referred to in paragraphs 2 and 3 of this Article whenever \nrelevant changes occur, in order to enable the authority responsible for notified bodies to \nmonitor and verify continuous compliance with all the requirements laid down in Article \n31.\n\nDocument 260: EN\nUnited in diversity\nEN\nArticle 30\nNotification procedure\n1.\nNotifying authorities may \u258c notify only conformity assessment bodies which have \nsatisfied the requirements laid down in Article 31.\n2.\nNotifying authorities shall notify the Commission and the other Member States, using the \nelectronic notification tool developed and managed by the Commission, of each \nconformity assessment body referred to in paragraph 1.\n3.\nThe notification referred to in paragraph 2 of this Article shall include full details of the \nconformity assessment activities, the conformity assessment module or modules, the types \nof AI systems concerned, and the relevant attestation of competence. Where a \nnotification is not based on an accreditation certificate as referred to in Article 29(2), the \nnotifying authority shall provide the Commission and the other Member States with \ndocumentary evidence which attests to the competence of the conformity assessment \nbody and to the arrangements in place to ensure that that body will be monitored \nregularly and will continue to satisfy the requirements laid down in Article 31.\n4.\nThe conformity assessment body concerned may perform the activities of a notified body \nonly where no objections are raised by the Commission or the other Member States within \ntwo weeks of a notification by a notifying authority where it includes an accreditation \ncertificate referred to in Article 29(2), or within two months of a notification by the \nnotifying authority where it includes documentary evidence referred to in Article 29(3).", "conversation_history": [{"role": "user", "content": "I need to know what conformity assessment bodies must submit with their application for notification according to Article 29."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 259, "topic": "Notified Bodies Regulations"}}
{"id": "1ce8b41b-6616-44c9-94fb-9034a9f4571a", "question": "What must they adhere to?", "reference_answer": "Notified bodies should comply with a set of requirements, in particular on independence, competence, absence of conflicts of interests and suitable cybersecurity requirements.", "reference_context": "Document 124: EN\nUnited in diversity\nEN\n(126)\nIn order to carry out third-party conformity assessments when so required, notified bodies \nshould be notified under this Regulation by the national competent authorities, provided \nthat they comply with a set of requirements, in particular on independence, competence, \nabsence of conflicts of interests and suitable cybersecurity requirements. Notification of \nthose bodies should be sent by national competent authorities to the Commission and the \nother Member States by means of the electronic notification tool developed and managed \nby the Commission pursuant to Article R23 of Annex I to Decision No 768/2008/EC.\n(127)\nIn line with Union commitments under the World Trade Organization Agreement on \nTechnical Barriers to Trade, it is adequate to facilitate the mutual recognition of \nconformity assessment results produced by competent conformity assessment bodies, \nindependent of the territory in which they are established, provided that those conformity \nassessment bodies established under the law of a third country meet the applicable \nrequirements of this Regulation and the Union has concluded an agreement to that \nextent. In this context, the Commission should actively explore possible international \ninstruments for that purpose and in particular pursue the conclusion of mutual \nrecognition agreements with third countries.", "conversation_history": [{"role": "user", "content": "I need to know the requirements that notified bodies must comply with under this Regulation."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 124, "topic": "Notified Bodies Regulations"}}
{"id": "78fa395e-0ca4-4a62-9d75-a4ecf6b23b3e", "question": "What actions are required?", "reference_answer": "Each Member State shall identify the public authorities or bodies referred to in paragraph 1 and make a list of them publicly available.", "reference_context": "Document 371: EN\nUnited in diversity\nEN\n2.\nBy \u2026 [three months after the entry into force of this Regulation], each Member State shall \nidentify the public authorities or bodies referred to in paragraph 1 and make a list of them \npublicly available \u258c . Member States shall notify the list to the Commission and to the \nother Member States, and shall keep the list up to date.\n3.\nWhere the documentation referred to in paragraph 1 is insufficient to ascertain whether an \ninfringement of obligations under Union law protecting fundamental rights has occurred, \nthe public authority or body referred to in paragraph 1 may make a reasoned request to the \nmarket surveillance authority, to organise testing of the high-risk AI system through \ntechnical means. The market surveillance authority shall organise the testing with the close \ninvolvement of the requesting public authority or body within a reasonable time following \nthe request.\n4.\nAny information or documentation obtained by the national public authorities or bodies \nreferred to in paragraph 1 of this Article pursuant to this Article shall be treated in \naccordance with the confidentiality obligations set out in Article 78.", "conversation_history": [{"role": "user", "content": "I am interested in what each Member State must do three months after the entry into force of the Regulation."}, {"role": "assistant", "content": "How can I help you with that?"}], "metadata": {"question_type": "conversational", "seed_document_id": 371, "topic": "AI Regulation and Compliance"}}
