{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from huggingface_hub import InferenceApi\n",
    "from transformers import pipeline\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "\n",
    "# LLamaIndex Imports\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.vector_stores.neo4jvector import Neo4jVectorStore\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext, get_response_synthesizer,PropertyGraphIndex, Document, KnowledgeGraphIndex\n",
    "from llama_index.core.evaluation import (DatasetGenerator,FaithfulnessEvaluator,RelevancyEvaluator)\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from llama_index.core.schema import IndexNode, NodeWithScore, Document, QueryBundle\n",
    "from llama_index.core.extractors import (SummaryExtractor,QuestionsAnsweredExtractor)\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine, KnowledgeGraphQueryEngine, CitationQueryEngine\n",
    "from llama_index.core.prompts.base import PromptTemplate, PromptType\n",
    "from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "# Giskard imports\n",
    "import giskard\n",
    "from giskard.rag import AgentAnswer, evaluate, RAGReport, KnowledgeBase, generate_testset, QATestset\n",
    "from giskard.rag.metrics.ragas_metrics import ragas_context_recall, ragas_faithfulness, ragas_answer_relevancy, ragas_context_precision\n",
    "from giskard.llm import set_llm_model, set_llm_api\n",
    "from giskard.llm.client import get_default_client\n",
    "from giskard.llm.embeddings import set_default_embedding, get_default_embedding\n",
    "\n",
    "def remove_openai_api_key():\n",
    "    if \"OPENAI_API_KEY\" in os.environ:\n",
    "        del os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Need to specify this here otherwise it doesn't work - Giskard Problem (?)\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"GSK_AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"GSK_AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_API_VERSION\"] = os.getenv(\"AZURE_API_VERSION\")\n",
    "os.environ[\"GSK_LLM_API\"] = \"azure\"\n",
    "os.environ[\"GSK_LLM_MODEL\"] = \"gpt-4o-mini\"\n",
    "set_llm_api(\"azure\")\n",
    "set_llm_model('gpt-4o-mini')\n",
    "\n",
    "AZURE_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Setup LLM\n",
    "llm_gpt4o = AzureOpenAI(\n",
    "    deployment_name=\"gpt-4o-mini\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "llm_gpt35 = AzureOpenAI(\n",
    "    deployment_name=\"gpt35\",\n",
    "    temperature=0, \n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "# Setup LLM\n",
    "llm_gpt4o_ = AzureOpenAI(\n",
    "    deployment_name=\"gpt4o\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"GPT4O_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"GPT4O_AZURE_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"GPT4O_API_VERSION\")\n",
    ")\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "# Verify LLM setup\n",
    "client = get_default_client()\n",
    "\n",
    "#print(\"Client base URL:\", client._client._base_url)\n",
    "#print(\"Client API key:\", client._client.api_key)\n",
    "#print(\"Client API version:\", client._client._api_version)\n",
    "#print(\"Client model:\", client.model)\n",
    "\n",
    "assert client._client._base_url == f'{os.environ[\"AZURE_OPENAI_ENDPOINT\"]}/openai/'\n",
    "assert client._client.api_key == os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "assert client._client._api_version == os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "url = os.getenv(\"NEO4J_URI\")\n",
    "username = os.getenv(\"NEO4J_USERNAME\")\n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "database = os.getenv(\"NEO4J_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "# Verify LLM setup\n",
    "client = get_default_client()\n",
    "\n",
    "assert client._client._base_url == f'{os.environ[\"AZURE_OPENAI_ENDPOINT\"]}/openai/'\n",
    "assert client._client.api_key == os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "assert client._client._api_version == os.environ[\"OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "def initialize_openai_creds():\n",
    "    \"\"\"Load environment variables and set API keys.\"\"\"\n",
    "    # Debug: Find the path of the .env file\n",
    "    dotenv_path = find_dotenv()\n",
    "    if dotenv_path == \"\":\n",
    "        print(\"No .env file found. Make sure the .env file is in the correct directory.\")\n",
    "    else:\n",
    "        print(f\".env file found at: {dotenv_path}\")\n",
    "\n",
    "    # Load environment variables from the .env file\n",
    "    load_dotenv(dotenv_path)\n",
    "\n",
    "    # General Azure OpenAI settings for gpt35 and gpt-4o-mini\n",
    "    general_creds = {\n",
    "        \"api_key\": os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "        \"api_version\": os.getenv(\"AZURE_API_VERSION\"),\n",
    "        \"endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        \"temperature\": 0  # Default temperature for models\n",
    "    }\n",
    "\n",
    "    # For GPT-4o specific settings\n",
    "    gpt4o_creds = {\n",
    "        \"api_key\": os.getenv('GPT4O_API_KEY'),\n",
    "        \"api_version\": os.getenv(\"GPT4O_API_VERSION\"),\n",
    "        \"endpoint\": os.getenv(\"GPT4O_AZURE_ENDPOINT\"),\n",
    "        \"deployment_name\": os.getenv(\"GPT4O_DEPLOYMENT_NAME\"),\n",
    "        \"temperature\": os.getenv(\"GPT4O_TEMPERATURE\", 0)  # Default temperature for GPT-4o\n",
    "    }\n",
    "\n",
    "    return general_creds, gpt4o_creds\n",
    "\n",
    "\n",
    "if not os.getenv('GPT4O_API_KEY'):\n",
    "    print(\"Environment variables not loaded. Check your .env file.\")\n",
    "load_dotenv\n",
    "general_creds, gpt4o_creds = initialize_openai_creds()\n",
    "print(general_creds)\n",
    "\n",
    "print(gpt4o_creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Giskard\n",
    "loader = PyMuPDFReader()\n",
    "#file_extractor = {\".pdf\": loader}\n",
    "documents1 = loader.load(file_path=\"../../legal_data/LL144/LL144.pdf\")\n",
    "documents2 = loader.load(file_path=\"../../legal_data/LL144/LL144_Definitions.pdf\")\n",
    "documents = documents1 + documents2\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"../../legal_data/EU_AI_ACT/EUAIACT.pdf\")\n",
    "splitter = SentenceSplitter(chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = Neo4jGraphStore(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    url=url,\n",
    "    database=database,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "Settings.llm = llm_gpt4o_\n",
    "Settings.embed_model=embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=5,\n",
    "    llm = llm_gpt4o_,\n",
    "    embed_model=embed_model,\n",
    "    include_embeddings=True,\n",
    "    transformations=[splitter]\n",
    ")\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    transformations=[splitter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm_gpt35\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    transformations=[splitter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = vector_index.as_chat_engine(chat_mode=\"context\")\n",
    "\n",
    "question = \"what is the ai act all about\"\n",
    "response = chat_engine.chat(question)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = graph_index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "nodes = retriever.retrieve(question)\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = graph_index.as_chat_engine(chat_mode=\"context\")\n",
    "\n",
    "question = \"what is the ai act all about\"\n",
    "response = chat_engine.chat(question)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the text classification pipeline with the specified model\n",
    "pipe = pipeline(\"text-classification\", model=\"wu981526092/bias_classifier_roberta\")\n",
    "\n",
    "# Define a mapping for the output labels\n",
    "label_mapping = {0: \"right\", 1: \"left\", 2: \"center\"}\n",
    "\n",
    "# Function to classify input text\n",
    "def classify_text(input_text):\n",
    "    # Use the pipeline to classify the input text\n",
    "    result = pipe(input_text)\n",
    "    \n",
    "    # Extract the label index\n",
    "    label_index = result[0]['label'].split('_')[-1]  # Extract numeric part from label like 'LABEL_0'\n",
    "    label_index = int(label_index)  # Convert label index to integer\n",
    "\n",
    "    # Map the label index to the human-readable label\n",
    "    human_readable_label = label_mapping.get(label_index, \"Unknown\")\n",
    "    \n",
    "    return human_readable_label\n",
    "\n",
    "# Get input text from the user\n",
    "input_text = input(\"Enter the text to classify: \")\n",
    "\n",
    "# Classify the text and print the result\n",
    "classification_result = classify_text(input_text)\n",
    "print(f\"The text is classified as: {classification_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input text from the user\n",
    "input_text = \"NYC Local Law 144 is a law that ensures employment candidates are not discriminated against. This is bad for businesses\"\n",
    "\n",
    "# Classify the text and print the result\n",
    "classification_result = classify_text(input_text)\n",
    "print(f\"The text is classified as: {classification_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n",
    "from llama_index.core.indices.property_graph import DynamicLLMPathExtractor\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "\n",
    "\n",
    "# Evaluate with Giskard\n",
    "loader = PyMuPDFReader()\n",
    "#file_extractor = {\".pdf\": loader}\n",
    "documents1 = loader.load(file_path=\"../../legal_data/LL144/LL144.pdf\")\n",
    "documents2 = loader.load(file_path=\"../../legal_data/LL144/LL144_Definitions.pdf\")\n",
    "documents = documents1 + documents2\n",
    "\n",
    "\n",
    "graph_store = Neo4jPropertyGraphStore(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    url=url\n",
    ")\n",
    "\n",
    "#kg_extractor = SimpleLLMPathExtractor(llm=llm_gpt4o_, max_paths_per_chunk=20,num_workers=4,show_progress=True)\n",
    "kg_extractor = DynamicLLMPathExtractor(\n",
    "    llm = llm_gpt35,\n",
    "    max_triplets_per_chunk=10,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "graph_index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    #property_graph_store=graph_store,\n",
    "    embed_model=embed_model,\n",
    "    embed_kg_nodes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "query_engine = graph_index.as_query_engine(\n",
    "    include_text=True,  # include source chunk with matching paths\n",
    "    similarity_top_k=10, \n",
    "     # top k for vector kg node retrieval\n",
    ")\n",
    "response = query_engine.query(\"what is definition for an ai system\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = graph_index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "retrieved_nodes = retriever.retrieve(\"what is definition of an ai system\")\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    print(node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle, NodeWithScore, TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from transformers import pipeline\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.indices.property_graph import LLMSynonymRetriever\n",
    "from llama_index.core.indices.property_graph import VectorContextRetriever, PGRetriever\n",
    "\n",
    "class CustomRetrieverWithQueryRewriting(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs query rewriting, Vector search, BM25 search, and Knowledge Graph search.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,  # LLM for query generation\n",
    "        vector_retriever: Optional[VectorIndexRetriever] = None,\n",
    "        bm25_retriever: Optional[BaseRetriever] = None,\n",
    "        kg_index=None,  # Pass the graph index to create KGTableRetriever on the fly\n",
    "        mode: str = \"OR\",\n",
    "        rewriter: bool = True,\n",
    "        classifier_model: Optional[str] = None,  # Optional classifier model\n",
    "        device: str = 'mps',  # Set to 'mps' as the default device\n",
    "        reranker_model_name: Optional[str] = None,  # Model name for SentenceTransformerRerank\n",
    "        verbose: bool = False,  # Verbose flag\n",
    "        property_index = True\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._bm25_retriever = bm25_retriever\n",
    "        self._kg_index = kg_index  # Store the KG index instead of the retriever\n",
    "        self._llm = llm\n",
    "        self._rewriter = rewriter\n",
    "        self._mode = mode\n",
    "        self._reranker_model_name = reranker_model_name  # Store the model name for the reranker\n",
    "        self._reranker = None  # Initialize reranker as None initially\n",
    "        self.verbose = verbose  # Set verbose flag\n",
    "        self.property_index= property_index\n",
    "\n",
    "        # Initialize the classifier if provided\n",
    "        self.classifier = None\n",
    "        if classifier_model:\n",
    "            self.classifier = pipeline(\"text-classification\", model=classifier_model, device=device)\n",
    "\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "\n",
    "    def classify_query_and_get_params(self, query: str) -> dict:\n",
    "        \"\"\"Classify the query and determine adaptive parameters for KG retriever.\"\"\"\n",
    "        params = {\n",
    "            \"top_k\": 5,  # Default top-k\n",
    "            \"max_keywords_per_query\": 4,  # Default max keywords\n",
    "            \"max_knowledge_sequence\": 2  # Default max knowledge sequence\n",
    "        }\n",
    "        classification_result = None\n",
    "        \n",
    "        if self.classifier:\n",
    "            classification = self.classifier(query)[0]\n",
    "            label = int(classification['label'].split('_')[-1])\n",
    "            if self.verbose:\n",
    "                print(f\"Query Classification: {classification['label']} with score {classification['score']}\")\n",
    "            \n",
    "            if label == 0:\n",
    "                params[\"top_k\"] = 5\n",
    "                params[\"max_keywords_per_query\"] = 3\n",
    "                params[\"max_knowledge_sequence\"] = 1\n",
    "            elif label == 1:\n",
    "                params[\"top_k\"] = 7\n",
    "                params[\"max_keywords_per_query\"] = 4\n",
    "                params[\"max_knowledge_sequence\"] = 2\n",
    "            elif label == 2:\n",
    "                params[\"top_k\"] = 7\n",
    "                params[\"max_keywords_per_query\"] = 5\n",
    "                params[\"max_knowledge_sequence\"] = 3\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Selected parameters for the query: {params}\")\n",
    "        return classification_result, params\n",
    "\n",
    "    def classify_query(self, query_str: str) -> str:\n",
    "        \"\"\"Classify the query into one of the predefined categories using LLM.\"\"\"\n",
    "        classification_prompt = (\n",
    "            f\"Classify the following query into one of the following categories: '5-300. Definitions', \"\n",
    "            f\"'5-301 Bias Audit', '5-302 Data Requirements', '§ 5-303 Published Results', '§ 5-304 Notice to Candidates and Employees'. \"\n",
    "            f\"If it doesn't fit into any category, respond with 'None'. Return the classification, do not output absolutely anything else. Query: '{query_str}'\"\n",
    "        )\n",
    "        response = self._llm.complete(classification_prompt)\n",
    "        category = response.text.strip()\n",
    "        return category if category in [\n",
    "            '5-300. Definitions', '5-301 Bias Audit', \n",
    "            '5-302 Data Requirements', '§ 5-303 Published Results', \n",
    "            '§ 5-304 Notice to Candidates and Employees'\n",
    "        ] else None\n",
    "\n",
    "    def generate_queries(self, query_str: str, category: str, num_queries: int = 3) -> List[str]:\n",
    "        \"\"\"Generate query variations using the LLM, taking into account the category if applicable.\"\"\"\n",
    "        \n",
    "        query_gen_prompt_str = (\n",
    "            f\"You are an expert at distilling a user question into sub-questions that can be used to fully answer the original query. \"\n",
    "            f\"First, identify the key words from the original question below: \\n\"\n",
    "            f\"{query_str}\"\n",
    "            f\"Generate {num_queries} sub-queries that cover the different aspects needed to fully address the user's query.\\n\\n\"\n",
    "            f\"Here is an example: \\n\"\n",
    "            f\"Original Question: What does test data mean and what do I need to know about it?\"\n",
    "            f\"Output:\"\n",
    "            f\"definition of 'test data'\\n\"\n",
    "            f\"test data requirements and conditions for a bias audit\\n\"\n",
    "            f\"examples of the use of test data in a bias audit\\n\\n\"\n",
    "            f\"Output the rewritten sub-queries, one on each line, do not output absolutely anything else\"\n",
    "        )\n",
    "\n",
    "        query_gen_prompt = query_gen_prompt_str\n",
    "        response = self._llm.complete(query_gen_prompt)\n",
    "        queries = response.text.split(\"\\n\")\n",
    "\n",
    "        # Remove empty strings from the generated queries\n",
    "        queries = [query.strip() for query in queries if query.strip()]\n",
    "        \n",
    "        # Add the category-specific query if the category is available\n",
    "        if category:\n",
    "            category_query = f\"{category}\"\n",
    "            queries.append(category_query)\n",
    "\n",
    "        return queries\n",
    "\n",
    "    \n",
    "    async def run_queries(self, queries: List[str], retrievers: List[BaseRetriever]) -> dict:\n",
    "        \"\"\"Run queries against retrievers.\"\"\"\n",
    "        tasks = []\n",
    "        for query in queries:\n",
    "            for i, retriever in enumerate(retrievers):\n",
    "                tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "        task_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        results_dict = {}\n",
    "        for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "            results_dict[(query, i)] = query_result\n",
    "        return results_dict\n",
    "\n",
    "    def fuse_vector_and_bm25_results(self, results_dict, similarity_top_k: int) -> List[NodeWithScore]:\n",
    "        \"\"\"Fuse results from Vector and BM25 retrievers.\"\"\"\n",
    "        k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "        fused_scores = {}\n",
    "        text_to_node = {}\n",
    "\n",
    "        # Compute reciprocal rank scores for BM25 and Vector retrievers\n",
    "        for nodes_with_scores in results_dict.values():\n",
    "            for rank, node_with_score in enumerate(\n",
    "                sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)\n",
    "            ):\n",
    "                text = node_with_score.node.get_content()\n",
    "                text_to_node[text] = node_with_score\n",
    "                if text not in fused_scores:\n",
    "                    fused_scores[text] = 0.0\n",
    "                fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "        # Sort results by combined scores\n",
    "        reranked_results = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Adjust node scores and prepare final results\n",
    "        reranked_nodes: List[NodeWithScore] = []\n",
    "        for text, score in reranked_results.items():\n",
    "            if text in text_to_node:\n",
    "                node = text_to_node[text]\n",
    "                node.score = score\n",
    "                reranked_nodes.append(node)\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(f\"Warning: Text not found in `text_to_node`: {text}\")\n",
    "\n",
    "        return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        # Classify the query to determine its category and retriever parameters\n",
    "        if self._rewriter:\n",
    "            category = self.classify_query(query_bundle.query_str)\n",
    "            if self.verbose:\n",
    "                print(f\"Classified Category: {category}\")\n",
    "\n",
    "        # Get adaptive parameters based on classification\n",
    "        classification_result, params = self.classify_query_and_get_params(query_bundle.query_str)\n",
    "        self._classification_result = classification_result\n",
    "\n",
    "        top_k = params[\"top_k\"]\n",
    "\n",
    "        # Initialize the reranker with the correct top_k value\n",
    "        if self._reranker_model_name:\n",
    "            self._reranker = SentenceTransformerRerank(model=self._reranker_model_name, top_n=top_k)\n",
    "            if self.verbose:\n",
    "                print(f\"Initialized reranker with top_n: {top_k}\")\n",
    "\n",
    "        # Determine the number of query rewrites based on classification\n",
    "        num_queries = 3 if top_k == 5 else 5 if top_k == 7 else 7\n",
    "        if self.verbose:\n",
    "            print(f\"Number of Query Rewrites: {num_queries}\")\n",
    "\n",
    "        # Generate query variations if rewriter is True\n",
    "        if self._rewriter:\n",
    "            queries = self.generate_queries(query_bundle.query_str, category, num_queries=num_queries)\n",
    "            if self.verbose:\n",
    "                print(f\"Generated Queries: {queries}\")\n",
    "        else:\n",
    "            queries = [query_bundle.query_str]\n",
    "\n",
    "        # Prepare the list of active retrievers\n",
    "        active_retrievers = []\n",
    "        if self._vector_retriever:\n",
    "            active_retrievers.append(self._vector_retriever)\n",
    "        if self._bm25_retriever:\n",
    "            active_retrievers.append(self._bm25_retriever)\n",
    "\n",
    "        # Instantiate the KG retriever with the adaptive parameters\n",
    "        if self._kg_index and self.property_index==False:\n",
    "            kg_retriever = KGTableRetriever(\n",
    "                index=self._kg_index,\n",
    "                retriever_mode='hybrid',\n",
    "                include_text=False,\n",
    "                max_keywords_per_query=params[\"max_keywords_per_query\"],\n",
    "                max_knowledge_sequence=params[\"max_knowledge_sequence\"]\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"Instantiated KG Retriever: max_keywords_per_query={params['max_keywords_per_query']}, \"\n",
    "                      f\"max_knowledge_sequence={params['max_knowledge_sequence']}\")\n",
    "            active_retrievers.append(kg_retriever)\n",
    "\n",
    "        elif self._kg_index and self.property_index==True:\n",
    "\n",
    "            synonym_retriever = LLMSynonymRetriever(\n",
    "                                    graph_index.property_graph_store,\n",
    "                                    llm=llm_gpt35,\n",
    "                                    # include source chunk text with retrieved paths\n",
    "                                    include_text=False,\n",
    "                                    #synonym_prompt=prompt,\n",
    "                                    #output_parsing_fn=parse_fn,\n",
    "                                    max_keywords=params[\"max_keywords_per_query\"],\n",
    "                                    # the depth of relations to follow after node retrieval\n",
    "                                    path_depth=params[\"max_knowledge_sequence\"],\n",
    "                                )\n",
    "            \n",
    "            vector_retriever = VectorContextRetriever(\n",
    "                                    graph_index.property_graph_store,\n",
    "                                    # only needed when the graph store doesn't support vector queries\n",
    "                                    # vector_store=index.vector_store,\n",
    "                                    embed_model=embed_model,\n",
    "                                    # include source chunk text with retrieved paths\n",
    "                                    include_text=False,\n",
    "                                    # the number of nodes to fetch\n",
    "                                    similarity_top_k=params[\"top_k\"],\n",
    "                                    # the depth of relations to follow after node retrieval\n",
    "                                    path_depth=params[\"max_knowledge_sequence\"],\n",
    "                                    # can provide any other kwargs for the VectorStoreQuery class\n",
    "                                )\n",
    "            \n",
    "            sub_retrievers=[synonym_retriever, vector_retriever]\n",
    "            kg_retriever = PGRetriever(sub_retrievers=sub_retrievers)\n",
    "           \n",
    "            \n",
    "\n",
    "        # If no active retrievers (BM25, Vector, or KG), raise an error\n",
    "        if not active_retrievers:\n",
    "            raise ValueError(\"No active retriever provided!\")\n",
    "\n",
    "        results = {}\n",
    "        # Run the queries asynchronously for active retrievers\n",
    "        if active_retrievers:\n",
    "            results = asyncio.run(self.run_queries(queries, active_retrievers))\n",
    "            if self.verbose:\n",
    "                print(f\"Fusion Results: {results}\")\n",
    "\n",
    "        # Fuse the results from active retrievers (BM25/Vector)\n",
    "        final_results = self.fuse_vector_and_bm25_results(results, similarity_top_k=top_k)\n",
    "\n",
    "        # Combine with KG nodes according to the mode (\"AND\" or \"OR\")\n",
    "        if self._kg_index:\n",
    "            kg_nodes = kg_retriever.retrieve(query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"KG Retrieved Nodes: {kg_nodes}\")\n",
    "\n",
    "            vector_ids = {n.node.id_ for n in final_results}\n",
    "            kg_ids = {n.node.id_ for n in kg_nodes}\n",
    "\n",
    "            combined_dict = {n.node.id_: n for n in final_results}\n",
    "            combined_dict.update({n.node.id_: n for n in kg_nodes})\n",
    "\n",
    "            if self._mode == \"AND\":\n",
    "                retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "            else:\n",
    "                retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "            final_results = [combined_dict[rid] for rid in retrieve_ids]\n",
    "\n",
    "        # Apply reranker if available\n",
    "        if self._reranker:\n",
    "            final_results = self._reranker.postprocess_nodes(final_results, query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"Reranked Results: {final_results}\")\n",
    "        else:\n",
    "            final_results = final_results[:top_k]\n",
    "\n",
    "        # Remove duplicates if rewriter is used\n",
    "        if self._rewriter:\n",
    "            unique_nodes = {}\n",
    "            for node in final_results:\n",
    "                content = node.node.get_content()\n",
    "                if content not in unique_nodes:\n",
    "                    unique_nodes[content] = node\n",
    "            final_results = list(unique_nodes.values())\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Final Results: {final_results}\")\n",
    "        return final_results\n",
    "    def get_classification_result(self) -> str:\n",
    "        return getattr(self, \"_classification_result\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n",
    "from llama_index.core.indices.property_graph import DynamicLLMPathExtractor\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from llama_index.core.schema import QueryBundle, NodeWithScore, TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from transformers import pipeline\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.indices.property_graph import LLMSynonymRetriever\n",
    "from llama_index.core.indices.property_graph import VectorContextRetriever, PGRetriever\n",
    "\n",
    "class CustomRetrieverWithQueryRewriting(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs query rewriting, Vector search, BM25 search, and Knowledge Graph search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,  # LLM for query generation\n",
    "        vector_retriever: Optional[VectorIndexRetriever] = None,\n",
    "        bm25_retriever: Optional[BaseRetriever] = None,\n",
    "        kg_index=None,  # Pass the graph index to create KGTableRetriever on the fly\n",
    "        mode: str = \"OR\",\n",
    "        rewriter: bool = True,\n",
    "        classifier_model: Optional[str] = None,  # Optional classifier model\n",
    "        device: str = 'mps',  # Set to 'mps' as the default device\n",
    "        reranker_model_name: Optional[str] = None,  # Model name for SentenceTransformerRerank\n",
    "        verbose: bool = False,  # Verbose flag\n",
    "        property_index=True\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._bm25_retriever = bm25_retriever\n",
    "        self._kg_index = kg_index  # Store the KG index instead of the retriever\n",
    "        self._llm = llm\n",
    "        self._rewriter = rewriter\n",
    "        self._mode = mode\n",
    "        self._reranker_model_name = reranker_model_name  # Store the model name for the reranker\n",
    "        self._reranker = None  # Initialize reranker as None initially\n",
    "        self.verbose = verbose  # Set verbose flag\n",
    "        self.property_index = property_index\n",
    "\n",
    "        # Initialize the classifier if provided\n",
    "        self.classifier = None\n",
    "        if classifier_model:\n",
    "            self.classifier = pipeline(\"text-classification\", model=classifier_model, device=device)\n",
    "\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "\n",
    "    def classify_query_and_get_params(self, query: str) -> dict:\n",
    "        \"\"\"Classify the query and determine adaptive parameters for KG retriever.\"\"\"\n",
    "        params = {\n",
    "            \"top_k\": 5,  # Default top-k\n",
    "            \"max_keywords_per_query\": 4,  # Default max keywords\n",
    "            \"max_knowledge_sequence\": 2  # Default max knowledge sequence\n",
    "        }\n",
    "        classification_result = None\n",
    "        \n",
    "        if self.classifier:\n",
    "            classification = self.classifier(query)[0]\n",
    "            label = int(classification['label'].split('_')[-1])\n",
    "            if self.verbose:\n",
    "                print(f\"Query Classification: {classification['label']} with score {classification['score']}\")\n",
    "            \n",
    "            if label == 0:\n",
    "                params[\"top_k\"] = 5\n",
    "                params[\"max_keywords_per_query\"] = 3\n",
    "                params[\"max_knowledge_sequence\"] = 1\n",
    "            elif label == 1:\n",
    "                params[\"top_k\"] = 7\n",
    "                params[\"max_keywords_per_query\"] = 4\n",
    "                params[\"max_knowledge_sequence\"] = 2\n",
    "            elif label == 2:\n",
    "                params[\"top_k\"] = 7\n",
    "                params[\"max_keywords_per_query\"] = 5\n",
    "                params[\"max_knowledge_sequence\"] = 3\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Selected parameters for the query: {params}\")\n",
    "        return classification_result, params\n",
    "\n",
    "    def generate_queries(self, query_str: str, category: str, num_queries: int = 3) -> List[str]:\n",
    "        \"\"\"Generate query variations using the LLM, taking into account the category if applicable.\"\"\"\n",
    "        \n",
    "        query_gen_prompt_str = (\n",
    "            f\"You are an expert at distilling a user question into sub-questions that can be used to fully answer the original query. \"\n",
    "            f\"First, identify the key words from the original question below: \\n\"\n",
    "            f\"{query_str}\"\n",
    "            f\"Generate {num_queries} sub-queries that cover the different aspects needed to fully address the user's query.\\n\\n\"\n",
    "            f\"Here is an example: \\n\"\n",
    "            f\"Original Question: What does test data mean and what do I need to know about it?\"\n",
    "            f\"Output:\"\n",
    "            f\"definition of 'test data'\\n\"\n",
    "            f\"test data requirements and conditions for a bias audit\\n\"\n",
    "            f\"examples of the use of test data in a bias audit\\n\\n\"\n",
    "            f\"Output the rewritten sub-queries, one on each line, do not output absolutely anything else\"\n",
    "        )\n",
    "\n",
    "        query_gen_prompt = query_gen_prompt_str\n",
    "        response = self._llm.complete(query_gen_prompt)\n",
    "        queries = response.text.split(\"\\n\")\n",
    "\n",
    "        # Remove empty strings from the generated queries\n",
    "        queries = [query.strip() for query in queries if query.strip()]\n",
    "        \n",
    "        # Add the category-specific query if the category is available\n",
    "        if category:\n",
    "            category_query = f\"{category}\"\n",
    "            queries.append(category_query)\n",
    "\n",
    "        return queries\n",
    "\n",
    "    async def run_queries(self, queries: List[str], retrievers: List[BaseRetriever]) -> dict:\n",
    "        \"\"\"Run queries against retrievers.\"\"\"\n",
    "        tasks = []\n",
    "        for query in queries:\n",
    "            for i, retriever in enumerate(retrievers):\n",
    "                tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "        task_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        results_dict = {}\n",
    "        for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "            results_dict[(query, i)] = query_result\n",
    "        return results_dict\n",
    "\n",
    "    def fuse_vector_bm25_pg_results(self, results_dict, pg_results, similarity_top_k: int) -> List[NodeWithScore]:\n",
    "        \"\"\"Fuse results from Vector, BM25, and Property Graph retrievers.\"\"\"\n",
    "        k = 60.0  # Parameter to control impact of outlier rankings.\n",
    "        fused_scores = {}\n",
    "        text_to_node = {}\n",
    "\n",
    "        # Process vector/BM25 results\n",
    "        for nodes_with_scores in results_dict.values():\n",
    "            for rank, node_with_score in enumerate(\n",
    "                sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)\n",
    "            ):\n",
    "                text = node_with_score.node.get_content()\n",
    "                text_to_node[text] = node_with_score\n",
    "                if text not in fused_scores:\n",
    "                    fused_scores[text] = 0.0\n",
    "                fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "        # Process PG results\n",
    "        for rank, node_with_score in enumerate(sorted(pg_results, key=lambda x: x.score or 0.0, reverse=True)):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "        # Sort results by fused scores\n",
    "        reranked_results = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Prepare final reranked nodes\n",
    "        reranked_nodes: List[NodeWithScore] = []\n",
    "        for text, score in reranked_results.items():\n",
    "            if text in text_to_node:\n",
    "                node = text_to_node[text]\n",
    "                node.score = score\n",
    "                reranked_nodes.append(node)\n",
    "\n",
    "        return reranked_nodes[:similarity_top_k]\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        # Classify the query to determine its category and retriever parameters\n",
    "        if self._rewriter:\n",
    "            category = self.classify_query(query_bundle.query_str)\n",
    "            if self.verbose:\n",
    "                print(f\"Classified Category: {category}\")\n",
    "\n",
    "        # Get adaptive parameters based on classification\n",
    "        classification_result, params = self.classify_query_and_get_params(query_bundle.query_str)\n",
    "        self._classification_result = classification_result\n",
    "\n",
    "        top_k = params[\"top_k\"]\n",
    "\n",
    "        # Initialize the reranker with the correct top_k value\n",
    "        if self._reranker_model_name:\n",
    "            self._reranker = SentenceTransformerRerank(model=self._reranker_model_name, top_n=top_k)\n",
    "            if self.verbose:\n",
    "                print(f\"Initialized reranker with top_n: {top_k}\")\n",
    "\n",
    "        # Determine the number of query rewrites based on classification\n",
    "        num_queries = 3 if top_k == 5 else 5 if top_k == 7 else 7\n",
    "        if self.verbose:\n",
    "            print(f\"Number of Query Rewrites: {num_queries}\")\n",
    "\n",
    "        # Generate query variations if rewriter is True\n",
    "        if self._rewriter:\n",
    "            queries = self.generate_queries(query_bundle.query_str, category, num_queries=num_queries)\n",
    "            if self.verbose:\n",
    "                print(f\"Generated Queries: {queries}\")\n",
    "        else:\n",
    "            queries = [query_bundle.query_str]\n",
    "\n",
    "        # Prepare the list of active retrievers\n",
    "        active_retrievers = []\n",
    "        if self._vector_retriever:\n",
    "            active_retrievers.append(self._vector_retriever)\n",
    "        if self._bm25_retriever:\n",
    "            active_retrievers.append(self._bm25_retriever)\n",
    "\n",
    "        # Instantiate the KG retriever with the adaptive parameters\n",
    "        if self._kg_index and not self.property_index:\n",
    "            kg_retriever = KGTableRetriever(\n",
    "                index=self._kg_index,\n",
    "                retriever_mode='hybrid',\n",
    "                include_text=False,\n",
    "                max_keywords_per_query=params[\"max_keywords_per_query\"],\n",
    "                max_knowledge_sequence=params[\"max_knowledge_sequence\"]\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"Instantiated KG Retriever: max_keywords_per_query={params['max_keywords_per_query']}, \"\n",
    "                      f\"max_knowledge_sequence={params['max_knowledge_sequence']}\")\n",
    "            active_retrievers.append(kg_retriever)\n",
    "\n",
    "        elif self._kg_index and self.property_index:\n",
    "            synonym_retriever = LLMSynonymRetriever(\n",
    "                graph_index.property_graph_store,\n",
    "                llm=self._llm,\n",
    "                include_text=True,\n",
    "                max_keywords=params[\"max_keywords_per_query\"],\n",
    "                path_depth=params[\"max_knowledge_sequence\"]\n",
    "            )\n",
    "            vector_retriever = VectorContextRetriever(\n",
    "                graph_index.property_graph_store,\n",
    "                #embed_model=self._vector_retriever.embed_model,\n",
    "                include_text=True,\n",
    "                similarity_top_k=params[\"top_k\"],\n",
    "                path_depth=params[\"max_knowledge_sequence\"]\n",
    "            )\n",
    "            sub_retrievers = [synonym_retriever, vector_retriever] #graph_index.as_retriever(similarity_top_k=params['top_k'], max_keywords=params[\"max_keywords_per_query\"], path_depth=params[\"max_knowledge_sequence\"])#P\n",
    "            kg_retriever = graph_index.as_retriever(sub_retrievers=sub_retrievers)\n",
    "            #kg_retriever = PGRetriever(sub_retrievers=sub_retrievers)\n",
    "\n",
    "        # If no active retrievers (BM25, Vector, or KG), raise an error\n",
    "        if not active_retrievers:\n",
    "            raise ValueError(\"No active retriever provided!\")\n",
    "\n",
    "        # Run the queries asynchronously for active retrievers\n",
    "        results = {}\n",
    "        if active_retrievers:\n",
    "            results = asyncio.run(self.run_queries(queries, active_retrievers))\n",
    "            if self.verbose:\n",
    "                print(f\"Fusion Results: {results}\")\n",
    "\n",
    "        # If using property index, retrieve from PGRetriever and fuse results\n",
    "        if self.property_index:\n",
    "            pg_results = kg_retriever.retrieve(query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"PG Retrieved Nodes: {pg_results}\")\n",
    "\n",
    "            final_results = self.fuse_vector_bm25_pg_results(results, pg_results, similarity_top_k=top_k)\n",
    "\n",
    "        else:\n",
    "            # Fuse the results from active retrievers (BM25/Vector)\n",
    "            final_results = self.fuse_vector_and_bm25_results(results, similarity_top_k=top_k)\n",
    "\n",
    "        # Apply reranker if available\n",
    "        if self._reranker:\n",
    "            final_results = self._reranker.postprocess_nodes(final_results, query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"Reranked Results: {final_results}\")\n",
    "        else:\n",
    "            final_results = final_results[:top_k]\n",
    "\n",
    "        # Remove duplicates if rewriter is used\n",
    "        if self._rewriter:\n",
    "            unique_nodes = {}\n",
    "            for node in final_results:\n",
    "                content = node.node.get_content()\n",
    "                if content not in unique_nodes:\n",
    "                    unique_nodes[content] = node\n",
    "            final_results = list(unique_nodes.values())\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Final Results: {final_results}\")\n",
    "        return final_results\n",
    "\n",
    "    def get_classification_result(self) -> str:\n",
    "        return getattr(self, \"_classification_result\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm_gpt35\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    transformations=[splitter]\n",
    ")\n",
    "\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10)\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=vector_index.docstore, similarity_top_k=10\n",
    ")\n",
    "\n",
    "# Define the custom retriever with query rewriting\n",
    "retriever = CustomRetrieverWithQueryRewriting(\n",
    "    llm=llm_gpt35,\n",
    "    vector_retriever=vector_retriever,\n",
    "    kg_index=graph_index,\n",
    "    bm25_retriever=bm25_retriever,\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\",\n",
    "    mode=\"OR\",\n",
    "    rewriter=False,\n",
    "    reranker_model_name=None,\n",
    "    verbose=True,\n",
    "    property_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(\"what is a ai system?\")\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    print(node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine, ContextChatEngine\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "def run_evaluation(\n",
    "    results_base_path: str,\n",
    "    test_set_path: str = \"../giskard_test_sets/LL144_275_New.jsonl\",\n",
    "    rewriter: bool = False,\n",
    "    reranker_model_name: str = None,\n",
    "    classifier_model: str = \"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False,\n",
    "    property_index=False,\n",
    "    kg_index=True\n",
    "):\n",
    "    \n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10)\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        index=vector_index, similarity_top_k=10\n",
    "    )\n",
    "\n",
    "    # Define the custom retriever with query rewriting\n",
    "    if kg_index==False:\n",
    "\n",
    "        retriever = CustomRetrieverWithQueryRewriting(\n",
    "            llm=llm_gpt35,\n",
    "            vector_retriever=vector_retriever,\n",
    "            kg_index=None,\n",
    "            bm25_retriever=bm25_retriever,\n",
    "            classifier_model=classifier_model,\n",
    "            mode=\"OR\",\n",
    "            rewriter=rewriter,\n",
    "            reranker_model_name=reranker_model_name,\n",
    "            verbose=verbose,\n",
    "            property_index=property_index\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        retriever = CustomRetrieverWithQueryRewriting(\n",
    "            llm=llm_gpt35,\n",
    "            vector_retriever=vector_retriever,\n",
    "            kg_index=graph_index,\n",
    "            bm25_retriever=bm25_retriever,\n",
    "            classifier_model=classifier_model,\n",
    "            mode=\"OR\",\n",
    "            rewriter=rewriter,\n",
    "            reranker_model_name=reranker_model_name,\n",
    "            verbose=verbose,\n",
    "            property_index=property_index\n",
    "        )\n",
    "\n",
    "\n",
    "    memory = ChatMemoryBuffer.from_defaults(token_limit=8192)\n",
    "    #chat_engine.reset()\n",
    "    chat_engine = ContextChatEngine.from_defaults(\n",
    "        retriever=retriever,\n",
    "        verbose=False,\n",
    "        chat_mode=\"context\",\n",
    "        memory_cls=memory,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    Settings.llm = llm_gpt35\n",
    "    splitter = SentenceSplitter(chunk_size=512)\n",
    "    text_nodes = splitter(graph_index.docstore.docs.values())\n",
    "    knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=['text'])\n",
    "    knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "\n",
    "    def answer_fn(question, history=None):\n",
    "        chat_history = [ChatMessage(role=MessageRole.USER if msg['role'] == 'user' else MessageRole.ASSISTANT, content=msg['content']) for msg in history] if history else []\n",
    "        \n",
    "        # Debug: Print chat history and token count\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        total_token_count = 0\n",
    "        for msg in chat_history:\n",
    "            tokens = tokenizer.encode(msg.content)\n",
    "            token_count = len(tokens)\n",
    "            total_token_count += token_count\n",
    "            if verbose:\n",
    "                print(f\"Message: {msg.content}\\nToken count: {token_count}\")\n",
    "        \n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Total token count in chat history: {total_token_count}\")\n",
    "        \n",
    "        return str(chat_engine.chat(question, chat_history=chat_history))\n",
    "\n",
    "    def get_answer_fn(question: str, history=None) -> str:\n",
    "        if verbose:\n",
    "            print(f\"Question: {question}\")\n",
    "        messages = history if history else []\n",
    "        messages.append({'role': 'user', 'content': question})\n",
    "        if verbose:\n",
    "            print(f\"Messages: {messages}\")\n",
    "        answer = answer_fn(question, history)\n",
    "        if verbose:\n",
    "            print(f\"Answer: {answer}\")\n",
    "        retrieved_nodes = retriever.retrieve(question)\n",
    "        \n",
    "        # Debug: Print retrieved nodes and their token counts\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        total_token_count = 0\n",
    "        for node in retrieved_nodes:\n",
    "            tokens = tokenizer.encode(node.node.text)\n",
    "            token_count = len(tokens)\n",
    "            total_token_count += token_count\n",
    "            if verbose:\n",
    "                print(f\"Node token count: {token_count}\")\n",
    "                print(f\"Node Snippet: {node.text[:200]}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Total token count for all nodes: {total_token_count}\")\n",
    "        \n",
    "        documents = [node.node.text for node in retrieved_nodes]\n",
    "        return AgentAnswer(message=answer, documents=documents)\n",
    "\n",
    "    # Load test set\n",
    "    testset = QATestset.load(test_set_path)\n",
    "\n",
    "    results_path = f'{results_base_path}'\n",
    "    report = evaluate(get_answer_fn, testset=testset, knowledge_base=knowledge_base, metrics=[ragas_faithfulness, ragas_answer_relevancy])\n",
    "    results = report.to_pandas()\n",
    "\n",
    "    csv_path = results_path + '.csv'\n",
    "    html_path = results_path + '.html'\n",
    "    results.to_csv(csv_path, index=False)\n",
    "\n",
    "# Example of how to call the function:\n",
    "# run_evaluation(\"path/to/results\", \"../giskard_test_sets/LL144_275_New.jsonl\", rewriter=True, reranker_model=\"mixedbread-ai/mxbai-rerank-base-v1\", classifier_model=\"your_classifier_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate, PromptTemplate\n",
    "from typing import Dict\n",
    "\n",
    "CORRECTNESS_SYS_TMPL = \"\"\"\n",
    "You are an expert evaluation system for a question answering chatbot.\n",
    "\n",
    "You are given the following information:\n",
    "- a user query,\n",
    "- a reference answer, and\n",
    "- a generated answer.\n",
    "\n",
    "Your job is to judge the correctness of the generated answer.\n",
    "Output a single score that represents a holistic evaluation.\n",
    "You must return your response in a line with only the score.\n",
    "Do not return answers in any other format.\n",
    "On a separate line provide your reasoning for the score as well.\n",
    "The reasoning MUST NOT UNDER ANY CIRCUMSTANCES BE LONGER THAN 1 SENTENCE.\n",
    "\n",
    "Follow these guidelines for scoring:\n",
    "- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\n",
    "- Use the following criteria for scoring correctness:\n",
    "\n",
    "1. Score of 1:\n",
    "    - The generated answer is completely incorrect.\n",
    "    - Contains major factual errors or misconceptions.\n",
    "    - Does not address any components of the user query correctly.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France?\"\n",
    "      - Generated Answer: \"The capital of France is Berlin.\"\n",
    "\n",
    "2. Score of 2:\n",
    "    - Significant mistakes are present.\n",
    "    - Addresses at least one component of the user query correctly but has major errors in other parts.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France and its population?\"\n",
    "      - Generated Answer: \"The capital of France is Paris, and its population is 100 million.\"\n",
    "\n",
    "3. Score of 3:\n",
    "    - Partially correct with some incorrect information.\n",
    "    - Addresses multiple components of the user query correctly.\n",
    "    - Minor factual errors are present.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France and its population?\"\n",
    "      - Generated Answer: \"The capital of France is Paris, and its population is around 3 million.\"\n",
    "\n",
    "4. Score of 4:\n",
    "    - Mostly correct with minimal errors.\n",
    "    - Correctly addresses all components of the user query.\n",
    "    - Errors do not substantially affect the overall correctness.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France and its population?\"\n",
    "      - Generated Answer: \"The capital of France is Paris, and its population is approximately 2.1 million.\"\n",
    "\n",
    "5. Score of 5:\n",
    "    - Completely correct.\n",
    "    - Addresses all components of the user query correctly without any errors.\n",
    "    - Providing more information than necessary should not be penalized as long as all provided information is correct.\n",
    "    - Example:\n",
    "      - Query: \"What is the capital of France and its population?\"\n",
    "      - Generated Answer: \"The capital of France is Paris, and its population is approximately 2.1 million. Paris is known for its rich history and iconic landmarks such as the Eiffel Tower and Notre-Dame Cathedral.\"\n",
    "\n",
    "Checklist for Evaluation:\n",
    "  - Component Coverage: Does the answer cover all parts of the query?\n",
    "  - Factual Accuracy: Are the facts presented in the answer correct?\n",
    "  - Error Severity: How severe are any errors present in the answer?\n",
    "  - Comparison to Reference: How closely does the answer align with the reference answer?\n",
    "\n",
    "Edge Cases:\n",
    "  - If the answer includes both correct and completely irrelevant information, focus only on the relevant portions for scoring.\n",
    "  - If the answer is correct but incomplete, score based on the completeness criteria within the relevant score range.\n",
    "  - If the answer provides more information than necessary, it should not be penalized as long as all information is correct.\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_USER_TMPL = \"\"\"\n",
    "## User Query\n",
    "{query}\n",
    "\n",
    "## Reference Answer\n",
    "{reference_answer}\n",
    "\n",
    "## Generated Answer\n",
    "{generated_answer}\n",
    "\"\"\"\n",
    "\n",
    "eval_chat_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=CORRECTNESS_SYS_TMPL),\n",
    "        ChatMessage(role=MessageRole.USER, content=CORRECTNESS_USER_TMPL),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def run_correctness_eval(\n",
    "    query_str: str,\n",
    "    reference_answer: str,\n",
    "    generated_answer: str,\n",
    "    llm: AzureOpenAI,\n",
    "    threshold: float = 4.0,\n",
    ") -> Dict:\n",
    "    \"\"\"Run correctness eval.\"\"\"\n",
    "    fmt_messages = eval_chat_template.format_messages(\n",
    "        llm=llm,\n",
    "        query=query_str,\n",
    "        reference_answer=reference_answer,\n",
    "        generated_answer=generated_answer,\n",
    "    )\n",
    "    chat_response = llm.chat(fmt_messages)\n",
    "    raw_output = chat_response.message.content\n",
    "\n",
    "    # Extract from response\n",
    "    score_str, reasoning_str = raw_output.split(\"\\n\", 1)\n",
    "    score = float(score_str)\n",
    "    reasoning = reasoning_str.lstrip(\"\\n\")\n",
    "\n",
    "    return {\"passing\": score >= threshold, \"score\": score, \"reason\": reasoning}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def process_correctness_scores(file_path, llm, threshold=4.0, num_rows=None):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract the 'question', 'reference answer', and 'agent answer' columns into lists\n",
    "    questions = df['question'].tolist()\n",
    "    reference_answers = df['reference_answer'].tolist()\n",
    "    agent_answers = df['agent_answer'].tolist()\n",
    "\n",
    "    # If num_rows is None, process all rows\n",
    "    if num_rows is None:\n",
    "        num_rows = len(df)\n",
    "\n",
    "    # Initialize the results list\n",
    "    results_method2 = []\n",
    "\n",
    "    # Use tqdm for the loading bar\n",
    "    for question, ref_answer, agent_answer in tqdm(zip(questions[:num_rows], reference_answers[:num_rows], agent_answers[:num_rows]), total=num_rows, desc=\"Processing\"):\n",
    "        result = run_correctness_eval(question, ref_answer, agent_answer, llm=llm, threshold=threshold)\n",
    "        results_method2.append(result)\n",
    "\n",
    "    # Extract the scores from results_method2\n",
    "    correctness_scores2 = [result['score'] for result in results_method2]\n",
    "    correctness_reasons2 = [result['reason'] for result in results_method2]\n",
    "\n",
    "    # Add correctness scores and reasons to the DataFrame\n",
    "    df.loc[:num_rows-1, 'correctness_method2'] = correctness_scores2\n",
    "    df.loc[:num_rows-1, 'reason_method2'] = correctness_reasons2\n",
    "\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    # Calculate the average score\n",
    "    total_score = sum(result['score'] for result in results_method2)\n",
    "    average_score = total_score / len(results_method2)\n",
    "    print(f\"Average Score: {average_score}\")\n",
    "\n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle, NodeWithScore, TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from transformers import pipeline\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.indices.property_graph import LLMSynonymRetriever\n",
    "from llama_index.core.indices.property_graph import VectorContextRetriever, PGRetriever\n",
    "import pandas as pd\n",
    "import json\n",
    "import tiktoken\n",
    "from llama_index.core.chat_engine import ContextChatEngine\n",
    "from llama_index.core.memory.chat_memory_buffer import ChatMemoryBuffer, ChatMessage\n",
    "\n",
    "\n",
    "class CustomRetrieverWithQueryRewriting(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs query rewriting, Vector search, BM25 search, and Knowledge Graph search.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,  # LLM for query generation\n",
    "        vector_retriever: Optional[VectorIndexRetriever] = None,\n",
    "        bm25_retriever: Optional[BaseRetriever] = None,\n",
    "        kg_index=None,  \n",
    "        mode: str = \"OR\",\n",
    "        rewriter: bool = True,\n",
    "        classifier_model: Optional[str] = None,  # Optional classifier model\n",
    "        device: str = 'mps',  # Set to 'mps' as the default device\n",
    "        reranker_model_name: Optional[str] = None,  # Model name for SentenceTransformerRerank\n",
    "        verbose: bool = False,  # Verbose flag\n",
    "        property_index=True,\n",
    "        use_fixed_params: bool = False  # New parameter to control fixed parameter usage\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._bm25_retriever = bm25_retriever\n",
    "        self._kg_index = kg_index  \n",
    "        self._llm = llm\n",
    "        self._rewriter = rewriter\n",
    "        self._mode = mode\n",
    "        self._reranker_model_name = reranker_model_name  # Store the model name for the reranker\n",
    "        self._reranker = None  # Initialize reranker as None initially\n",
    "        self.verbose = verbose  # Set verbose flag\n",
    "        self.property_index = property_index\n",
    "        self.use_fixed_params = use_fixed_params  # Store the use_fixed_params setting\n",
    "        self._classification_result = None  # To store the classification result\n",
    "        \n",
    "        # Initialize the classifier if provided\n",
    "        self.classifier = None\n",
    "        if classifier_model:\n",
    "            self.classifier = pipeline(\"text-classification\", model=classifier_model, device=device)\n",
    "\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "\n",
    "    def classify_query_and_get_params(self, query: str) -> (str, dict):\n",
    "        \"\"\"Classify the query and determine adaptive parameters for KG retriever or use fixed parameters.\"\"\"\n",
    "        # Set default parameters to the highest available if use_fixed_params is True\n",
    "        if self.use_fixed_params:\n",
    "            params = {\n",
    "                \"top_k\": 7,  # Highest top-k\n",
    "                \"max_keywords_per_query\": 5,  # Highest max keywords\n",
    "                \"max_knowledge_sequence\": 3  # Highest max knowledge sequence\n",
    "            }\n",
    "            classification_result = \"Fixed\"\n",
    "            if self.verbose:\n",
    "                print(f\"Using fixed parameters: {params}\")\n",
    "        else:\n",
    "            params = {\n",
    "                \"top_k\": 5,  # Default top-k\n",
    "                \"max_keywords_per_query\": 4,  # Default max keywords\n",
    "                \"max_knowledge_sequence\": 2  # Default max knowledge sequence\n",
    "            }\n",
    "            classification_result = None\n",
    "\n",
    "            if self.classifier:\n",
    "                classification = self.classifier(query)[0]\n",
    "                label = int(classification['label'].split('_')[-1])\n",
    "                classification_result = classification['label']  # Store the classification result\n",
    "                if self.verbose:\n",
    "                    print(f\"Query Classification: {classification['label']} with score {classification['score']}\")\n",
    "                \n",
    "                if label == 0:\n",
    "                    params[\"top_k\"] = 5\n",
    "                    params[\"max_keywords_per_query\"] = 3\n",
    "                    params[\"max_knowledge_sequence\"] = 1\n",
    "                elif label == 1:\n",
    "                    params[\"top_k\"] = 7\n",
    "                    params[\"max_keywords_per_query\"] = 4\n",
    "                    params[\"max_knowledge_sequence\"] = 2\n",
    "                elif label == 2:\n",
    "                    params[\"top_k\"] = 7\n",
    "                    params[\"max_keywords_per_query\"] = 5\n",
    "                    params[\"max_knowledge_sequence\"] = 3\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"Selected parameters for the query: {params}\")\n",
    "\n",
    "        self._classification_result = classification_result\n",
    "        return classification_result, params\n",
    "\n",
    "    def classify_query(self, query_str: str) -> str:\n",
    "        \"\"\"Classify the query into one of the predefined categories using LLM.\"\"\"\n",
    "        classification_prompt = (\n",
    "            f\"Classify the following query into one of the following categories: '5-300. Definitions', \"\n",
    "            f\"'5-301 Bias Audit', '5-302 Data Requirements', '§ 5-303 Published Results', '§ 5-304 Notice to Candidates and Employees'. \"\n",
    "            f\"If it doesn't fit into any category, respond with 'None'. Return the classification, do not output absolutely anything else. Query: '{query_str}'\"\n",
    "        )\n",
    "        response = self._llm.complete(classification_prompt)\n",
    "        category = response.text.strip()\n",
    "        return category if category in [\n",
    "            '5-300. Definitions', '5-301 Bias Audit', \n",
    "            '5-302 Data Requirements', '§ 5-303 Published Results', \n",
    "            '§ 5-304 Notice to Candidates and Employees'\n",
    "        ] else None\n",
    "\n",
    "    def generate_queries(self, query_str: str, category: str, num_queries: int = 3) -> List[str]:\n",
    "        \"\"\"Generate query variations using the LLM, taking into account the category if applicable.\"\"\"\n",
    "        \n",
    "        query_gen_prompt_str = (\n",
    "            f\"You are an expert at distilling a user question into sub-questions that can be used to fully answer the original query. \"\n",
    "            f\"First, identify the key words from the original question below: \\n\"\n",
    "            f\"{query_str}\"\n",
    "            f\"Generate {num_queries} sub-queries that cover the different aspects needed to fully address the user's query.\\n\\n\"\n",
    "            f\"Here is an example: \\n\"\n",
    "            f\"Original Question: What does test data mean and what do I need to know about it?\"\n",
    "            f\"Output:\"\n",
    "            f\"definition of 'test data'\\n\"\n",
    "            f\"test data requirements and conditions for a bias audit\\n\"\n",
    "            f\"examples of the use of test data in a bias audit\\n\\n\"\n",
    "            f\"Output the rewritten sub-queries, one on each line, do not output absolutely anything else\"\n",
    "        )\n",
    "\n",
    "        query_gen_prompt = query_gen_prompt_str\n",
    "        response = self._llm.complete(query_gen_prompt)\n",
    "        queries = response.text.split(\"\\n\")\n",
    "\n",
    "        # Remove empty strings from the generated queries\n",
    "        queries = [query.strip() for query in queries if query.strip()]\n",
    "        \n",
    "        # Add the category-specific query if the category is available\n",
    "        if category:\n",
    "            category_query = f\"{category}\"\n",
    "            queries.append(category_query)\n",
    "\n",
    "        return queries\n",
    "\n",
    "    \n",
    "    async def run_queries(self, queries: List[str], retrievers: List[BaseRetriever]) -> dict:\n",
    "        \"\"\"Run queries against retrievers.\"\"\"\n",
    "        tasks = []\n",
    "        for query in queries:\n",
    "            for i, retriever in enumerate(retrievers):\n",
    "                tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "        task_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        results_dict = {}\n",
    "        for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "            results_dict[(query, i)] = query_result\n",
    "        return results_dict\n",
    "\n",
    "    def fuse_vector_and_bm25_results(self, results_dict, similarity_top_k: int) -> List[NodeWithScore]:\n",
    "        \"\"\"Fuse results from Vector and BM25 retrievers.\"\"\"\n",
    "        k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "        fused_scores = {}\n",
    "        text_to_node = {}\n",
    "\n",
    "        # Compute reciprocal rank scores for BM25 and Vector retrievers\n",
    "        for nodes_with_scores in results_dict.values():\n",
    "            for rank, node_with_score in enumerate(\n",
    "                sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)\n",
    "            ):\n",
    "                text = node_with_score.node.get_content()\n",
    "                text_to_node[text] = node_with_score\n",
    "                if text not in fused_scores:\n",
    "                    fused_scores[text] = 0.0\n",
    "                fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "        # Sort results by combined scores\n",
    "        reranked_results = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Adjust node scores and prepare final results\n",
    "        reranked_nodes: List[NodeWithScore] = []\n",
    "        for text, score in reranked_results.items():\n",
    "            if text in text_to_node:\n",
    "                node = text_to_node[text]\n",
    "                node.score = score\n",
    "                reranked_nodes.append(node)\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(f\"Warning: Text not found in `text_to_node`: {text}\")\n",
    "\n",
    "        return reranked_nodes[:similarity_top_k]\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        # Classify the query to determine its category and retriever parameters\n",
    "        if self._rewriter:\n",
    "            category = self.classify_query(query_bundle.query_str)\n",
    "            if self.verbose:\n",
    "                print(f\"Classified Category: {category}\")\n",
    "\n",
    "        # Correctly unpack both classification_result and params\n",
    "        classification_result, params = self.classify_query_and_get_params(query_bundle.query_str)\n",
    "        self._classification_result = classification_result\n",
    "\n",
    "        top_k = params[\"top_k\"]\n",
    "\n",
    "        # Initialize the reranker with the correct top_k value\n",
    "        if self._reranker_model_name:\n",
    "            self._reranker = SentenceTransformerRerank(model=self._reranker_model_name, top_n=top_k)\n",
    "            if self.verbose:\n",
    "                print(f\"Initialized reranker with top_n: {top_k}\")\n",
    "\n",
    "        # Determine the number of query rewrites based on classification\n",
    "        num_queries = 3 if top_k == 5 else 5 if top_k == 7 else 7\n",
    "        if self.verbose:\n",
    "            print(f\"Number of Query Rewrites: {num_queries}\")\n",
    "\n",
    "        # Generate query variations if rewriter is True\n",
    "        if self._rewriter:\n",
    "            queries = self.generate_queries(query_bundle.query_str, category, num_queries=num_queries)\n",
    "            if self.verbose:\n",
    "                print(f\"Generated Queries: {queries}\")\n",
    "        else:\n",
    "            queries = [query_bundle.query_str]\n",
    "\n",
    "        # Prepare the list of active retrievers\n",
    "        active_retrievers = []\n",
    "        if self._vector_retriever:\n",
    "            active_retrievers.append(self._vector_retriever)\n",
    "        if self._bm25_retriever:\n",
    "            active_retrievers.append(self._bm25_retriever)\n",
    "\n",
    "        # Instantiate the KG retriever with the adaptive parameters\n",
    "        if self._kg_index and not self.property_index:\n",
    "            kg_retriever = KGTableRetriever(\n",
    "                index=self._kg_index,\n",
    "                retriever_mode='hybrid',\n",
    "                include_text=False,\n",
    "                max_keywords_per_query=params[\"max_keywords_per_query\"],\n",
    "                max_knowledge_sequence=params[\"max_knowledge_sequence\"]\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"Instantiated KG Retriever: max_keywords_per_query={params['max_keywords_per_query']}, \"\n",
    "                    f\"max_knowledge_sequence={params['max_knowledge_sequence']}\")\n",
    "            active_retrievers.append(kg_retriever)\n",
    "\n",
    "        elif self._kg_index and self.property_index:\n",
    "            synonym_retriever = LLMSynonymRetriever(\n",
    "                graph_index.property_graph_store,\n",
    "                llm=self._llm,\n",
    "                include_text=False,\n",
    "                max_keywords=params[\"max_keywords_per_query\"],\n",
    "                path_depth=params[\"max_knowledge_sequence\"],\n",
    "            )\n",
    "            \n",
    "            vector_retriever = VectorContextRetriever(\n",
    "                graph_index.property_graph_store,\n",
    "                embed_model=embed_model,\n",
    "                include_text=False,\n",
    "                similarity_top_k=params[\"top_k\"],\n",
    "                path_depth=params[\"max_knowledge_sequence\"],\n",
    "            )\n",
    "            \n",
    "            sub_retrievers = [synonym_retriever, vector_retriever]\n",
    "            kg_retriever = PGRetriever(sub_retrievers=sub_retrievers)\n",
    "\n",
    "        # If no active retrievers (BM25, Vector, or KG), raise an error\n",
    "        if not active_retrievers:\n",
    "            raise ValueError(\"No active retriever provided!\")\n",
    "\n",
    "        results = {}\n",
    "        # Run the queries asynchronously for active retrievers\n",
    "        if active_retrievers:\n",
    "            results = asyncio.run(self.run_queries(queries, active_retrievers))\n",
    "            if self.verbose:\n",
    "                print(f\"Fusion Results: {results}\")\n",
    "\n",
    "        # Fuse the results from active retrievers (BM25/Vector)\n",
    "        final_results = self.fuse_vector_and_bm25_results(results, similarity_top_k=top_k)\n",
    "\n",
    "        # Combine with KG nodes according to the mode (\"AND\" or \"OR\")\n",
    "        if self._kg_index:\n",
    "            kg_nodes = kg_retriever.retrieve(query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"KG Retrieved Nodes: {kg_nodes}\")\n",
    "\n",
    "            vector_ids = {n.node.id_ for n in final_results}\n",
    "            kg_ids = {n.node.id_ for n in kg_nodes}\n",
    "\n",
    "            combined_dict = {n.node.id_: n for n in final_results}\n",
    "            combined_dict.update({n.node.id_: n for n in kg_nodes})\n",
    "\n",
    "            if self._mode == \"AND\":\n",
    "                retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "            else:\n",
    "                retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "            final_results = [combined_dict[rid] for rid in retrieve_ids]\n",
    "\n",
    "        # Apply reranker if available\n",
    "        if self._reranker:\n",
    "            final_results = self._reranker.postprocess_nodes(final_results, query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"Reranked Results: {final_results}\")\n",
    "        else:\n",
    "            final_results = final_results[:top_k]\n",
    "\n",
    "        # Remove duplicates if rewriter is used\n",
    "        if self._rewriter:\n",
    "            unique_nodes = {}\n",
    "            for node in final_results:\n",
    "                content = node.node.get_content()\n",
    "                if content not in unique_nodes:\n",
    "                    unique_nodes[content] = node\n",
    "            final_results = list(unique_nodes.values())\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Final Results: {final_results}\")\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def get_classification_result(self) -> str:\n",
    "        return getattr(self, \"_classification_result\", None)\n",
    "    \n",
    "\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine, ContextChatEngine\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "\n",
    "def run_evaluation(\n",
    "    results_base_path: str,\n",
    "    test_set_path: str = \"../giskard_test_sets/LL144_275_New.jsonl\",\n",
    "    rewriter: bool = False,\n",
    "    reranker_model_name: str = None,\n",
    "    classifier_model: str = \"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False,\n",
    "    property_index=False,\n",
    "    kg_index=True, ## IS THIS MEANT TO BE TRUE/FALSE OR ACTUAL GRAPH INDEX\n",
    "    use_fixed_params: bool = False  ## TO DO: Add ability to override any parameters from here\n",
    "    # \n",
    "):\n",
    "    \n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10)\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        index=vector_index, similarity_top_k=10\n",
    "    )\n",
    "\n",
    "    # Define the custom retriever with query rewriting\n",
    "    retriever = CustomRetrieverWithQueryRewriting(\n",
    "        llm=llm_gpt35,\n",
    "        vector_retriever=vector_retriever,\n",
    "        kg_index=graph_index if kg_index else None,\n",
    "        bm25_retriever=bm25_retriever,\n",
    "        classifier_model=classifier_model,\n",
    "        mode=\"OR\",\n",
    "        rewriter=rewriter,\n",
    "        reranker_model_name=reranker_model_name,\n",
    "        verbose=verbose,\n",
    "        property_index=property_index,\n",
    "        use_fixed_params=use_fixed_params  # Pass the new parameter\n",
    "    )\n",
    "\n",
    "    memory = ChatMemoryBuffer.from_defaults(token_limit=8192)\n",
    "    chat_engine = ContextChatEngine.from_defaults(\n",
    "        retriever=retriever,\n",
    "        verbose=False,\n",
    "        chat_mode=\"context\",\n",
    "        memory_cls=memory,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    Settings.llm = llm_gpt35\n",
    "    splitter = SentenceSplitter(chunk_size=512)\n",
    "    text_nodes = splitter(graph_index.docstore.docs.values())\n",
    "    knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=['text'])\n",
    "    knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "\n",
    "    def answer_fn(question, history=None):\n",
    "        chat_history = [ChatMessage(role=MessageRole.USER if msg['role'] == 'user' else MessageRole.ASSISTANT, content=msg['content']) for msg in history] if history else []\n",
    "        \n",
    "        # Debug: Print chat history and token count\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        total_token_count = 0\n",
    "        for msg in chat_history:\n",
    "            tokens = tokenizer.encode(msg.content)\n",
    "            token_count = len(tokens)\n",
    "            total_token_count += token_count\n",
    "            if verbose:\n",
    "                print(f\"Message: {msg.content}\\nToken count: {token_count}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Total token count in chat history: {total_token_count}\")\n",
    "        \n",
    "        return str(chat_engine.chat(question, chat_history=chat_history))\n",
    "\n",
    "    def get_answer_fn(question: str, history=None) -> str:\n",
    "        if verbose:\n",
    "            print(f\"Question: {question}\")\n",
    "        messages = history if history else []\n",
    "        messages.append({'role': 'user', 'content': question})\n",
    "        if verbose:\n",
    "            print(f\"Messages: {messages}\")\n",
    "        answer = answer_fn(question, history)\n",
    "        if verbose:\n",
    "            print(f\"Answer: {answer}\")\n",
    "        retrieved_nodes = retriever.retrieve(question)\n",
    "        \n",
    "        # Debug: Print retrieved nodes and their token counts\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        total_token_count = 0\n",
    "        for node in retrieved_nodes:\n",
    "            tokens = tokenizer.encode(node.node.text)\n",
    "            token_count = len(tokens)\n",
    "            total_token_count += token_count\n",
    "            if verbose:\n",
    "                print(f\"Node token count: {token_count}\")\n",
    "                print(f\"Node Snippet: {node.text[:200]}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Total token count for all nodes: {total_token_count}\")\n",
    "        \n",
    "        documents = [node.node.text for node in retrieved_nodes]\n",
    "        return AgentAnswer(message=answer, documents=documents)\n",
    "\n",
    "    # Load test set with error handling\n",
    "    try:\n",
    "        testset = QATestset.load(test_set_path)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error loading test set: {e}\")\n",
    "        return\n",
    "\n",
    "    results_path = f'{results_base_path}'\n",
    "    report = evaluate(get_answer_fn, testset=testset, knowledge_base=knowledge_base, metrics=[ragas_faithfulness, ragas_answer_relevancy])\n",
    "    results = report.to_pandas()\n",
    "\n",
    "    csv_path = results_path + '.csv'\n",
    "    html_path = results_path + '.html'\n",
    "    results.to_csv(csv_path, index=False)\n",
    "\n",
    "    score = process_correctness_scores(file_path=csv_path, llm=llm_gpt4o, threshold=4.0, num_rows=None)\n",
    "\n",
    "\n",
    "# Example of how to call the function:\n",
    "run_evaluation(\n",
    "    results_base_path=\"fixed_upper_params_ll144\",\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",\n",
    "    rewriter=True,\n",
    "    reranker_model_name=None,\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False,\n",
    "    property_index=False,\n",
    "    kg_index=True,\n",
    "    use_fixed_params=True  # Use fixed parameters instead of adaptive ones\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='class3_HyPA2_k_Q_eu', \n",
    "    test_set_path=\"../eval/eu_ai_act_test_300_new.jsonl\",              #'../../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\", \n",
    "    rewriter=True, \n",
    "    reranker_model_name=None,\n",
    "    verbose=False,\n",
    "    property_index=False,\n",
    "    kg_index=graph_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='class2_HyPA2_k_Q_eu', \n",
    "    test_set_path=\"../eval/eu_ai_act_test_300_new.jsonl\",              #'../../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\", \n",
    "    rewriter=True, \n",
    "    reranker_model_name=None,\n",
    "    verbose=False,\n",
    "    property_index=False,\n",
    "    kg_index=graph_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='fixed_k_7_eu', \n",
    "    test_set_path=\"../eval/eu_ai_act_test_300_new.jsonl\",              #'../../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\", \n",
    "    rewriter=False, \n",
    "    reranker_model_name=None,\n",
    "    verbose=False,\n",
    "    property_index=False,\n",
    "    kg_index=False,\n",
    "    use_fixed_params=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_fixed(\n",
    "    results_base_path: str,\n",
    "    test_set_path: str = \"../giskard_test_sets/LL144_275_New.jsonl\",\n",
    "    rewriter: bool = False,\n",
    "    reranker_model_name: str = None,\n",
    "    classifier_model: str = \"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False,\n",
    "    property_index=False,\n",
    "    kg_index=True,\n",
    "    use_fixed_params: bool = False  # New parameter to control the use of fixed parameters\n",
    "):\n",
    "    \n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10)\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        index=vector_index, similarity_top_k=10\n",
    "    )\n",
    "\n",
    "    # Define the custom retriever with query rewriting\n",
    "    retriever = CustomRetrieverWithQueryRewriting(\n",
    "        llm=llm_gpt35,\n",
    "        vector_retriever=vector_retriever,\n",
    "        kg_index=graph_index if kg_index else None,\n",
    "        bm25_retriever=bm25_retriever,\n",
    "        classifier_model=classifier_model,\n",
    "        mode=\"OR\",\n",
    "        rewriter=rewriter,\n",
    "        reranker_model_name=reranker_model_name,\n",
    "        verbose=verbose,\n",
    "        property_index=property_index,\n",
    "        use_fixed_params=use_fixed_params  # Pass the new parameter\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    memory = ChatMemoryBuffer.from_defaults(token_limit=8192)\n",
    "    chat_engine = ContextChatEngine.from_defaults(\n",
    "        retriever=retriever,\n",
    "        verbose=False,\n",
    "        chat_mode=\"context\",\n",
    "        memory_cls=memory,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    Settings.llm = llm_gpt35\n",
    "    splitter = SentenceSplitter(chunk_size=512)\n",
    "    text_nodes = splitter(graph_index.docstore.docs.values())\n",
    "    knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=['text'])\n",
    "    knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "\n",
    "    def answer_fn(question, history=None):\n",
    "        chat_history = [ChatMessage(role=MessageRole.USER if msg['role'] == 'user' else MessageRole.ASSISTANT, content=msg['content']) for msg in history] if history else []\n",
    "        \n",
    "        # Debug: Print chat history and token count\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        total_token_count = 0\n",
    "        for msg in chat_history:\n",
    "            tokens = tokenizer.encode(msg.content)\n",
    "            token_count = len(tokens)\n",
    "            total_token_count += token_count\n",
    "            if verbose:\n",
    "                print(f\"Message: {msg.content}\\nToken count: {token_count}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Total token count in chat history: {total_token_count}\")\n",
    "        \n",
    "        return str(chat_engine.chat(question, chat_history=chat_history))\n",
    "\n",
    "    def get_answer_fn(question: str, history=None) -> str:\n",
    "        if verbose:\n",
    "            print(f\"Question: {question}\")\n",
    "        messages = history if history else []\n",
    "        messages.append({'role': 'user', 'content': question})\n",
    "        if verbose:\n",
    "            print(f\"Messages: {messages}\")\n",
    "        answer = answer_fn(question, history)\n",
    "        if verbose:\n",
    "            print(f\"Answer: {answer}\")\n",
    "        retrieved_nodes = retriever.retrieve(question)\n",
    "        \n",
    "        # Debug: Print retrieved nodes and their token counts\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        total_token_count = 0\n",
    "        for node in retrieved_nodes:\n",
    "            tokens = tokenizer.encode(node.node.text)\n",
    "            token_count = len(tokens)\n",
    "            total_token_count += token_count\n",
    "            if verbose:\n",
    "                print(f\"Node token count: {token_count}\")\n",
    "                print(f\"Node Snippet: {node.text[:200]}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Total token count for all nodes: {total_token_count}\")\n",
    "        \n",
    "        documents = [node.node.text for node in retrieved_nodes]\n",
    "        return AgentAnswer(message=answer, documents=documents)\n",
    "\n",
    "    # Load test set with error handling\n",
    "    try:\n",
    "        testset = QATestset.load(test_set_path)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error loading test set: {e}\")\n",
    "        return\n",
    "\n",
    "    results_path = f'{results_base_path}'\n",
    "    report = evaluate(get_answer_fn, testset=testset, knowledge_base=knowledge_base, metrics=[ragas_faithfulness, ragas_answer_relevancy])\n",
    "    results = report.to_pandas()\n",
    "\n",
    "    csv_path = results_path + '.csv'\n",
    "    html_path = results_path + '.html'\n",
    "    results.to_csv(csv_path, index=False)\n",
    "\n",
    "# Example of how to call the function:\n",
    "run_evaluation(\n",
    "    results_base_path=\"fixed_upper_params_ll144\",\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",\n",
    "    rewriter=True,\n",
    "    reranker_model_name=None,\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False,\n",
    "    property_index=False,\n",
    "    kg_index=True,\n",
    "    use_fixed_params=True  # Use fixed parameters instead of adaptive ones\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='2class_PA_k_Q_eu', \n",
    "    test_set_path=\"../eval/eu_ai_act_test_300_new.jsonl\",              #'../../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\", \n",
    "    rewriter=True, \n",
    "    reranker_model_name=None,\n",
    "    verbose=False,\n",
    "    property_index=False,\n",
    "    kg_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='HGRAG_3class_adaptive_v2', \n",
    "    test_set_path='../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\", \n",
    "    rewriter=False, \n",
    "    reranker_model_name=None,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='HGRAG_2class_adaptive_v2', \n",
    "    test_set_path='../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\", \n",
    "    rewriter=False, \n",
    "    reranker_model_name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='HGRAG_3class_adaptive_v2_rewriter', \n",
    "    test_set_path='../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\", \n",
    "    rewriter=True, \n",
    "    reranker_model_name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='HGRAG_2class_adaptive_v2_rewriter', \n",
    "    test_set_path='../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\", \n",
    "    rewriter=True, \n",
    "    reranker_model_name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='HGRAG_2class_adaptive_v2_rewriter_reranker', \n",
    "    test_set_path='../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\", \n",
    "    rewriter=True, \n",
    "    reranker_model_name=\"BAAI/bge-reranker-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='HGRAG_3class_adaptive_v2_rewriter_reranker', \n",
    "    test_set_path='../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\", \n",
    "    rewriter=True, \n",
    "    reranker_model_name=\"BAAI/bge-reranker-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path='HGRAG_2class_adaptive_v2_reranker', \n",
    "    test_set_path='../giskard_test_sets/LL144_275_New.jsonl', \n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\", \n",
    "    rewriter=False, \n",
    "    reranker_model_name=\"BAAI/bge-reranker-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path=\"PA_k_Q_3_class\",  # specify the desired output path for results\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",  # path to the test set\n",
    "    rewriter=True,  # enable rewriter\n",
    "    reranker_model_name=None,  # specify reranker model if needed\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\",  # use the 3-class classifier model\n",
    "    verbose=False,  # enable verbose output for debugging\n",
    "    property_index=False,  # disable property index if not needed\n",
    "    kg_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path=\"PA_k_Q_2_class\",  # specify the desired output path for results\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",  # path to the test set\n",
    "    rewriter=True,  # enable rewriter\n",
    "    reranker_model_name=None,  # specify reranker model if needed\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\",  # use the 3-class classifier model\n",
    "    verbose=False,  # enable verbose output for debugging\n",
    "    property_index=False,  # disable property index if not needed\n",
    "    kg_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path=\"PA_k_Q_3_class_reranker\",  # specify the desired output path for results\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",  # path to the test set\n",
    "    rewriter=True,  # enable rewriter\n",
    "    reranker_model_name=\"BAAI/bge-reranker-large\",  # specify reranker model if needed\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\",  # use the 3-class classifier model\n",
    "    verbose=False,  # enable verbose output for debugging\n",
    "    property_index=False,  # disable property index if not needed\n",
    "    kg_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path=\"PA_k_Q_2_class_reranker\",  # specify the desired output path for results\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",  # path to the test set\n",
    "    rewriter=True,  # enable rewriter\n",
    "    reranker_model_name=\"BAAI/bge-reranker-large\",  # specify reranker model if needed\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\",  # use the 3-class classifier model\n",
    "    verbose=False,  # enable verbose output for debugging\n",
    "    property_index=False,  # disable property index if not needed\n",
    "    kg_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_chat_engine = vector_index.as_chat_engine(chat_mode=\"simple\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = vector_chat_engine.chat(\"what is a abias audit\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path=\"../eval/eu_ai_act_test_300_new.jsonl\"\n",
    "results_base_path=\"llm_only_gpt35_eu\"\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "text_nodes = splitter(vector_index.docstore.docs.values())\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=['text'])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "\n",
    "chat_engine = vector_index.as_chat_engine(chat_mode=\"simple\", verbose=False)\n",
    "retriever=vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    chat_history = [ChatMessage(role=MessageRole.USER if msg['role'] == 'user' else MessageRole.ASSISTANT, content=msg['content']) for msg in history] if history else []\n",
    "    return str(chat_engine.chat(question, chat_history=chat_history))\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    messages = history if history else []\n",
    "    messages.append({'role': 'user', 'content': question})\n",
    "    answer = answer_fn(question, history)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    return AgentAnswer(message=answer, documents=documents)\n",
    "\n",
    "# Load test set\n",
    "testset = QATestset.load(test_set_path)\n",
    "\n",
    "results_path = f'{results_base_path}'\n",
    "report = evaluate(get_answer_fn, testset=testset, knowledge_base=knowledge_base, metrics=[ragas_faithfulness, ragas_answer_relevancy])\n",
    "results = report.to_pandas()\n",
    "\n",
    "csv_path = results_path + '.csv'\n",
    "html_path = results_path + '.html'\n",
    "results.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path=\"../eval/eu_ai_act_test_300_new.jsonl\"\n",
    "results_base_path=\"llm_only_eu_4o_mini\"\n",
    "\n",
    "Settings.llm = llm_gpt4o\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "text_nodes = splitter(vector_index.docstore.docs.values())\n",
    "knowledge_base_df = pd.DataFrame([node.text for node in text_nodes], columns=['text'])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)\n",
    "\n",
    "chat_engine = vector_index.as_chat_engine(chat_mode=\"simple\", verbose=False)\n",
    "retriever=vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "def answer_fn(question, history=None):\n",
    "    chat_history = [ChatMessage(role=MessageRole.USER if msg['role'] == 'user' else MessageRole.ASSISTANT, content=msg['content']) for msg in history] if history else []\n",
    "    return str(chat_engine.chat(question, chat_history=chat_history))\n",
    "\n",
    "def get_answer_fn(question: str, history=None) -> str:\n",
    "    messages = history if history else []\n",
    "    messages.append({'role': 'user', 'content': question})\n",
    "    answer = answer_fn(question, history)\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    documents = [node.node.text for node in retrieved_nodes]\n",
    "    return AgentAnswer(message=answer, documents=documents)\n",
    "\n",
    "# Load test set\n",
    "testset = QATestset.load(test_set_path)\n",
    "\n",
    "results_path = f'{results_base_path}'\n",
    "report = evaluate(get_answer_fn, testset=testset, knowledge_base=knowledge_base, metrics=[ragas_faithfulness, ragas_answer_relevancy])\n",
    "results = report.to_pandas()\n",
    "\n",
    "csv_path = results_path + '.csv'\n",
    "html_path = results_path + '.html'\n",
    "results.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with Giskard\n",
    "loader = PyMuPDFReader()\n",
    "#file_extractor = {\".pdf\": loader}\n",
    "documents1 = loader.load(file_path=\"../../legal_data/EU_AI_ACT/EUAIACT.pdf\")\n",
    "documents = documents1\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "graph_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=5,\n",
    "    llm = llm_gpt4o_,\n",
    "    embed_model=embed_model,\n",
    "    include_embeddings=True,\n",
    "    transformations=[splitter]\n",
    ")\n",
    "\n",
    "Settings.llm = llm_gpt35\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    transformations=[splitter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    results_base_path=\"PA_k_Q_2_class\",  # specify the desired output path for results\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",  # path to the test set\n",
    "    rewriter=True,  # enable rewriter\n",
    "    reranker_model_name=\"BAAI/bge-reranker-large\",  # specify reranker model if needed\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-2\",  # use the 3-class classifier model\n",
    "    verbose=False,  # enable verbose output for debugging\n",
    "    property_index=False,  # disable property index if not needed\n",
    "    kg_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle, NodeWithScore, TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from transformers import pipeline\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.indices.property_graph import LLMSynonymRetriever\n",
    "from llama_index.core.indices.property_graph import VectorContextRetriever, PGRetriever\n",
    "\n",
    "class CustomRetrieverWithQueryRewriting(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs query rewriting, Vector search, BM25 search, and Knowledge Graph search.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,  # LLM for query generation\n",
    "        vector_retriever: Optional[VectorIndexRetriever] = None,\n",
    "        bm25_retriever: Optional[BaseRetriever] = None,\n",
    "        kg_index=None,  # Pass the graph index to create KGTableRetriever on the fly\n",
    "        mode: str = \"OR\",\n",
    "        rewriter: bool = True,\n",
    "        classifier_model: Optional[str] = None,  # Optional classifier model\n",
    "        device: str = 'mps',  # Set to 'mps' as the default device\n",
    "        reranker_model_name: Optional[str] = None,  # Model name for SentenceTransformerRerank\n",
    "        verbose: bool = False,  # Verbose flag\n",
    "        property_index = True\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._bm25_retriever = bm25_retriever\n",
    "        self._kg_index = kg_index  # Store the KG index instead of the retriever\n",
    "        self._llm = llm\n",
    "        self._rewriter = rewriter\n",
    "        self._mode = mode\n",
    "        self._reranker_model_name = reranker_model_name  # Store the model name for the reranker\n",
    "        self._reranker = None  # Initialize reranker as None initially\n",
    "        self.verbose = verbose  # Set verbose flag\n",
    "        self.property_index = property_index\n",
    "        self._classification_result = None  # To store the classification result\n",
    "        \n",
    "        # Initialize the classifier if provided\n",
    "        self.classifier = None\n",
    "        if classifier_model:\n",
    "            self.classifier = pipeline(\"text-classification\", model=classifier_model, device=device)\n",
    "\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "\n",
    "    def classify_query_and_get_params(self, query: str) -> (str, dict):\n",
    "        \"\"\"Classify the query and determine adaptive parameters for KG retriever.\"\"\"\n",
    "        params = {\n",
    "            \"top_k\": 5,  # Default top-k\n",
    "            \"max_keywords_per_query\": 4,  # Default max keywords\n",
    "            \"max_knowledge_sequence\": 2  # Default max knowledge sequence\n",
    "        }\n",
    "        classification_result = None\n",
    "        \n",
    "        if self.classifier:\n",
    "            classification = self.classifier(query)[0]\n",
    "            label = int(classification['label'].split('_')[-1])\n",
    "            classification_result = classification['label']  # Store the classification result\n",
    "            if self.verbose:\n",
    "                print(f\"Query Classification: {classification['label']} with score {classification['score']}\")\n",
    "            \n",
    "            if label == 0:\n",
    "                params[\"top_k\"] = 5\n",
    "                params[\"max_keywords_per_query\"] = 3\n",
    "                params[\"max_knowledge_sequence\"] = 1\n",
    "            elif label == 1:\n",
    "                params[\"top_k\"] = 7\n",
    "                params[\"max_keywords_per_query\"] = 4\n",
    "                params[\"max_knowledge_sequence\"] = 2\n",
    "            elif label == 2:\n",
    "                params[\"top_k\"] = 7\n",
    "                params[\"max_keywords_per_query\"] = 5\n",
    "                params[\"max_knowledge_sequence\"] = 3\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Selected parameters for the query: {params}\")\n",
    "\n",
    "        self._classification_result = classification_result\n",
    "        return classification_result, params  # Ensure both values are returned correctly\n",
    "\n",
    "\n",
    "\n",
    "    def classify_query(self, query_str: str) -> str:\n",
    "        \"\"\"Classify the query into one of the predefined categories using LLM.\"\"\"\n",
    "        classification_prompt = (\n",
    "            f\"Classify the following query into one of the following categories: '5-300. Definitions', \"\n",
    "            f\"'5-301 Bias Audit', '5-302 Data Requirements', '§ 5-303 Published Results', '§ 5-304 Notice to Candidates and Employees'. \"\n",
    "            f\"If it doesn't fit into any category, respond with 'None'. Return the classification, do not output absolutely anything else. Query: '{query_str}'\"\n",
    "        )\n",
    "        response = self._llm.complete(classification_prompt)\n",
    "        category = response.text.strip()\n",
    "        return category if category in [\n",
    "            '5-300. Definitions', '5-301 Bias Audit', \n",
    "            '5-302 Data Requirements', '§ 5-303 Published Results', \n",
    "            '§ 5-304 Notice to Candidates and Employees'\n",
    "        ] else None\n",
    "\n",
    "    def generate_queries(self, query_str: str, category: str, num_queries: int = 3) -> List[str]:\n",
    "        \"\"\"Generate query variations using the LLM, taking into account the category if applicable.\"\"\"\n",
    "        \n",
    "        query_gen_prompt_str = (\n",
    "            f\"You are an expert at distilling a user question into sub-questions that can be used to fully answer the original query. \"\n",
    "            f\"First, identify the key words from the original question below: \\n\"\n",
    "            f\"{query_str}\"\n",
    "            f\"Generate {num_queries} sub-queries that cover the different aspects needed to fully address the user's query.\\n\\n\"\n",
    "            f\"Here is an example: \\n\"\n",
    "            f\"Original Question: What does test data mean and what do I need to know about it?\"\n",
    "            f\"Output:\"\n",
    "            f\"definition of 'test data'\\n\"\n",
    "            f\"test data requirements and conditions for a bias audit\\n\"\n",
    "            f\"examples of the use of test data in a bias audit\\n\\n\"\n",
    "            f\"Output the rewritten sub-queries, one on each line, do not output absolutely anything else\"\n",
    "        )\n",
    "\n",
    "        query_gen_prompt = query_gen_prompt_str\n",
    "        response = self._llm.complete(query_gen_prompt)\n",
    "        queries = response.text.split(\"\\n\")\n",
    "\n",
    "        # Remove empty strings from the generated queries\n",
    "        queries = [query.strip() for query in queries if query.strip()]\n",
    "        \n",
    "        # Add the category-specific query if the category is available\n",
    "        if category:\n",
    "            category_query = f\"{category}\"\n",
    "            queries.append(category_query)\n",
    "\n",
    "        return queries\n",
    "\n",
    "    \n",
    "    async def run_queries(self, queries: List[str], retrievers: List[BaseRetriever]) -> dict:\n",
    "        \"\"\"Run queries against retrievers.\"\"\"\n",
    "        tasks = []\n",
    "        for query in queries:\n",
    "            for i, retriever in enumerate(retrievers):\n",
    "                tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "        task_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        results_dict = {}\n",
    "        for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "            results_dict[(query, i)] = query_result\n",
    "        return results_dict\n",
    "\n",
    "    def fuse_vector_and_bm25_results(self, results_dict, similarity_top_k: int) -> List[NodeWithScore]:\n",
    "        \"\"\"Fuse results from Vector and BM25 retrievers.\"\"\"\n",
    "        k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "        fused_scores = {}\n",
    "        text_to_node = {}\n",
    "\n",
    "        # Compute reciprocal rank scores for BM25 and Vector retrievers\n",
    "        for nodes_with_scores in results_dict.values():\n",
    "            for rank, node_with_score in enumerate(\n",
    "                sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)\n",
    "            ):\n",
    "                text = node_with_score.node.get_content()\n",
    "                text_to_node[text] = node_with_score\n",
    "                if text not in fused_scores:\n",
    "                    fused_scores[text] = 0.0\n",
    "                fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "        # Sort results by combined scores\n",
    "        reranked_results = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Adjust node scores and prepare final results\n",
    "        reranked_nodes: List[NodeWithScore] = []\n",
    "        for text, score in reranked_results.items():\n",
    "            if text in text_to_node:\n",
    "                node = text_to_node[text]\n",
    "                node.score = score\n",
    "                reranked_nodes.append(node)\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(f\"Warning: Text not found in `text_to_node`: {text}\")\n",
    "\n",
    "        return reranked_nodes[:similarity_top_k]\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> (List[NodeWithScore], dict, dict):\n",
    "        \"\"\"Retrieve nodes given query and compute token usage.\"\"\"\n",
    "\n",
    "        # Initialize dictionaries to track token counts\n",
    "        input_tokens = {\n",
    "            'question_tokens': 0,\n",
    "            'generated_queries_tokens': 0,\n",
    "            'retrieved_nodes_tokens': 0\n",
    "        }\n",
    "        output_tokens = {\n",
    "            'final_output_tokens': 0\n",
    "        }\n",
    "\n",
    "        # Classify the query to determine its category and retriever parameters\n",
    "        if self._rewriter:\n",
    "            category = self.classify_query(query_bundle.query_str)\n",
    "            if self.verbose:\n",
    "                print(f\"Classified Category: {category}\")\n",
    "\n",
    "        # Correctly unpack both classification_result and params\n",
    "        classification_result, params = self.classify_query_and_get_params(query_bundle.query_str)\n",
    "        self._classification_result = classification_result\n",
    "\n",
    "        top_k = params[\"top_k\"]\n",
    "\n",
    "        # Initialize the reranker with the correct top_k value\n",
    "        if self._reranker_model_name:\n",
    "            self._reranker = SentenceTransformerRerank(model=self._reranker_model_name, top_n=top_k)\n",
    "            if self.verbose:\n",
    "                print(f\"Initialized reranker with top_n: {top_k}\")\n",
    "\n",
    "        # Determine the number of query rewrites based on classification\n",
    "        num_queries = 3 if top_k == 5 else 5 if top_k == 7 else 7\n",
    "        if self.verbose:\n",
    "            print(f\"Number of Query Rewrites: {num_queries}\")\n",
    "\n",
    "        # Generate query variations if rewriter is True\n",
    "        if self._rewriter:\n",
    "            queries = self.generate_queries(query_bundle.query_str, category, num_queries=num_queries)\n",
    "            input_tokens['generated_queries_tokens'] = sum(len(q.split()) for q in queries)\n",
    "            if self.verbose:\n",
    "                print(f\"Generated Queries: {queries}\")\n",
    "        else:\n",
    "            queries = [query_bundle.query_str]\n",
    "\n",
    "        # Prepare the list of active retrievers\n",
    "        active_retrievers = []\n",
    "        if self._vector_retriever:\n",
    "            active_retrievers.append(self._vector_retriever)\n",
    "        if self._bm25_retriever:\n",
    "            active_retrievers.append(self._bm25_retriever)\n",
    "\n",
    "        # Instantiate the KG retriever with the adaptive parameters\n",
    "        if self._kg_index and not self.property_index:\n",
    "            kg_retriever = KGTableRetriever(\n",
    "                index=self._kg_index,\n",
    "                retriever_mode='hybrid',\n",
    "                include_text=False,\n",
    "                max_keywords_per_query=params[\"max_keywords_per_query\"],\n",
    "                max_knowledge_sequence=params[\"max_knowledge_sequence\"]\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"Instantiated KG Retriever: max_keywords_per_query={params['max_keywords_per_query']}, \"\n",
    "                    f\"max_knowledge_sequence={params['max_knowledge_sequence']}\")\n",
    "            active_retrievers.append(kg_retriever)\n",
    "\n",
    "        elif self._kg_index and self.property_index:\n",
    "            synonym_retriever = LLMSynonymRetriever(\n",
    "                graph_index.property_graph_store,\n",
    "                llm=self._llm,\n",
    "                include_text=False,\n",
    "                max_keywords=params[\"max_keywords_per_query\"],\n",
    "                path_depth=params[\"max_knowledge_sequence\"],\n",
    "            )\n",
    "            \n",
    "            vector_retriever = VectorContextRetriever(\n",
    "                graph_index.property_graph_store,\n",
    "                embed_model=embed_model,\n",
    "                include_text=False,\n",
    "                similarity_top_k=params[\"top_k\"],\n",
    "                path_depth=params[\"max_knowledge_sequence\"],\n",
    "            )\n",
    "            \n",
    "            sub_retrievers = [synonym_retriever, vector_retriever]\n",
    "            kg_retriever = PGRetriever(sub_retrievers=sub_retrievers)\n",
    "\n",
    "        # If no active retrievers (BM25, Vector, or KG), raise an error\n",
    "        if not active_retrievers:\n",
    "            raise ValueError(\"No active retriever provided!\")\n",
    "\n",
    "        results = {}\n",
    "        # Run the queries asynchronously for active retrievers\n",
    "        if active_retrievers:\n",
    "            results = asyncio.run(self.run_queries(queries, active_retrievers))\n",
    "            input_tokens['retrieved_nodes_tokens'] = sum(len(node.node.get_content().split()) for result in results.values() for node in result)\n",
    "            if self.verbose:\n",
    "                print(f\"Fusion Results: {results}\")\n",
    "\n",
    "        # Fuse the results from active retrievers (BM25/Vector)\n",
    "        final_results = self.fuse_vector_and_bm25_results(results, similarity_top_k=top_k)\n",
    "\n",
    "        # Combine with KG nodes according to the mode (\"AND\" or \"OR\")\n",
    "        if self._kg_index:\n",
    "            kg_nodes = kg_retriever.retrieve(query_bundle)\n",
    "            input_tokens['retrieved_nodes_tokens'] += sum(len(node.node.get_content().split()) for node in kg_nodes)\n",
    "            if self.verbose:\n",
    "                print(f\"KG Retrieved Nodes: {kg_nodes}\")\n",
    "\n",
    "            vector_ids = {n.node.id_ for n in final_results}\n",
    "            kg_ids = {n.node.id_ for n in kg_nodes}\n",
    "\n",
    "            combined_dict = {n.node.id_: n for n in final_results}\n",
    "            combined_dict.update({n.node.id_: n for n in kg_nodes})\n",
    "\n",
    "            if self._mode == \"AND\":\n",
    "                retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "            else:\n",
    "                retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "            final_results = [combined_dict[rid] for rid in retrieve_ids]\n",
    "\n",
    "        # Apply reranker if available\n",
    "        if self._reranker:\n",
    "            final_results = self._reranker.postprocess_nodes(final_results, query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"Reranked Results: {final_results}\")\n",
    "        else:\n",
    "            final_results = final_results[:top_k]\n",
    "\n",
    "        # Remove duplicates if rewriter is used\n",
    "        if self._rewriter:\n",
    "            unique_nodes = {}\n",
    "            for node in final_results:\n",
    "                content = node.node.get_content()\n",
    "                if content not in unique_nodes:\n",
    "                    unique_nodes[content] = node\n",
    "            final_results = list(unique_nodes.values())\n",
    "\n",
    "        # Ensure to return a flat list of NodeWithScore objects\n",
    "        if any(isinstance(i, list) for i in final_results):\n",
    "            final_results = [item for sublist in final_results for item in sublist]\n",
    "\n",
    "        # Final output token count\n",
    "        output_tokens['final_output_tokens'] = sum(len(node.node.get_content().split()) for node in final_results)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Final Results: {final_results}\")\n",
    "            print(f\"Input Tokens: {input_tokens}\")\n",
    "            print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "        return final_results, input_tokens, output_tokens\n",
    "    \n",
    "        # Apply reranker if available\n",
    "        if self._reranker:\n",
    "            final_results = self._reranker.postprocess_nodes(final_results, query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"Reranked Results: {final_results}\")\n",
    "        else:\n",
    "            final_results = final_results[:top_k]\n",
    "\n",
    "        # Remove duplicates if rewriter is used\n",
    "        if self._rewriter:\n",
    "            unique_nodes = {}\n",
    "            for node in final_results:\n",
    "                content = node.node.get_content()\n",
    "                if content not in unique_nodes:\n",
    "                    unique_nodes[content] = node\n",
    "            final_results = list(unique_nodes.values())\n",
    "\n",
    "        # Ensure to return a flat list of NodeWithScore objects\n",
    "        if isinstance(final_results, list) and any(isinstance(i, list) for i in final_results):\n",
    "            final_results = [item for sublist in final_results for item in sublist]\n",
    "\n",
    "        # Final output token count\n",
    "        output_tokens['final_output_tokens'] = sum(len(node.node.get_content().split()) for node in final_results)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Final Results: {final_results}\")\n",
    "            print(f\"Input Tokens: {input_tokens}\")\n",
    "            print(f\"Output Tokens: {output_tokens}\")\n",
    "\n",
    "        return final_results, input_tokens, output_tokens\n",
    "\n",
    "\n",
    "    def get_classification_result(self) -> str:\n",
    "        return getattr(self, \"_classification_result\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_counts(\n",
    "    test_set_path: str,\n",
    "    results_path: str,\n",
    "    llm,\n",
    "    vector_index,\n",
    "    graph_index,\n",
    "    kg_index=True,\n",
    "    property_index=False,\n",
    "    rewriter: bool = False,\n",
    "    reranker_model_name: Optional[str] = None,\n",
    "    classifier_model: str = \"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False,\n",
    "    num_rows: Optional[int] = None\n",
    "):\n",
    "    \"\"\"Compute and save token usage for each question in the test set.\"\"\"\n",
    "    \n",
    "    # Initialize necessary components\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10)\n",
    "    bm25_retriever = BM25Retriever.from_defaults(index=vector_index, similarity_top_k=10)\n",
    "\n",
    "    # Define the custom retriever\n",
    "    retriever = CustomRetrieverWithQueryRewriting(\n",
    "        llm=llm,\n",
    "        vector_retriever=vector_retriever,\n",
    "        kg_index=graph_index if kg_index else None,\n",
    "        bm25_retriever=bm25_retriever,\n",
    "        classifier_model=classifier_model,\n",
    "        mode=\"OR\",\n",
    "        rewriter=rewriter,\n",
    "        reranker_model_name=reranker_model_name,\n",
    "        verbose=verbose,\n",
    "        property_index=property_index\n",
    "    )\n",
    "\n",
    "    # Create the chat engine\n",
    "    memory = ChatMemoryBuffer.from_defaults(token_limit=8192)\n",
    "    chat_engine = ContextChatEngine.from_defaults(\n",
    "        retriever=retriever,\n",
    "        verbose=False,\n",
    "        chat_mode=\"context\",\n",
    "        memory_cls=memory,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    # Function to get token count from a text\n",
    "    def count_tokens(text: str) -> int:\n",
    "        \"\"\"Returns the token count for a given text.\"\"\"\n",
    "        if isinstance(text, str):  # Check if text is a string\n",
    "            return len(tokenizer.encode(text))\n",
    "        else:\n",
    "            raise ValueError(\"Input to count_tokens must be a string.\")\n",
    "\n",
    "    # Function to get the answer, token counts, and classification\n",
    "    def get_token_usage_and_classification(question: str, history: List[dict] = None):\n",
    "        if not history:\n",
    "            history = []\n",
    "\n",
    "        # Initialize token counts\n",
    "        question_token_count = count_tokens(question)\n",
    "        history_token_count = sum(count_tokens(msg['content']) for msg in history)\n",
    "        retrieved_nodes_token_count = 0\n",
    "        generated_queries_token_count = 0\n",
    "        output_token_count = 0\n",
    "        \n",
    "        # Calculate input tokens for question and chat history\n",
    "        input_token_count = question_token_count + history_token_count\n",
    "        \n",
    "        # If query rewriter is enabled, generate queries and count tokens\n",
    "        if rewriter:\n",
    "            generated_queries = retriever.generate_queries(question, None)  # Assuming category is not needed here\n",
    "            generated_queries_token_count = sum(count_tokens(q) for q in generated_queries)\n",
    "            input_token_count += generated_queries_token_count\n",
    "\n",
    "        # Get the answer and retrieve nodes\n",
    "        answer = chat_engine.chat(question, chat_history=history)\n",
    "        retrieved_nodes, input_tokens, output_tokens = retriever.retrieve(QueryBundle(query_str=question))\n",
    "\n",
    "        # Add token count for retrieved chunks and triplets (KG retrievals)\n",
    "        retrieved_nodes_token_count = input_tokens['retrieved_nodes_tokens']\n",
    "        input_token_count += retrieved_nodes_token_count\n",
    "\n",
    "        # Calculate output tokens (e.g., generated keywords, rewritten queries, final answer)\n",
    "        if isinstance(answer, str):\n",
    "            output_token_count = count_tokens(answer)\n",
    "        else:\n",
    "            output_token_count = count_tokens(str(answer))\n",
    "\n",
    "        # Ensure to get the classification result after retrieval\n",
    "        classification_result = retriever.get_classification_result()\n",
    "        if verbose:\n",
    "            print(f\"Classification result for question '{question}': {classification_result}\")\n",
    "            print(f\"Token counts: Question = {question_token_count}, History = {history_token_count}, \"\n",
    "                  f\"Retrieved Nodes = {retrieved_nodes_token_count}, Generated Queries = {generated_queries_token_count}, \"\n",
    "                  f\"Output Tokens = {output_token_count}\")\n",
    "\n",
    "        return (question_token_count, history_token_count, retrieved_nodes_token_count, \n",
    "                generated_queries_token_count, output_token_count, classification_result)\n",
    "\n",
    "    # Load the test set\n",
    "    with open(test_set_path, 'r') as f:\n",
    "        test_set = [json.loads(line) for line in f]\n",
    "\n",
    "    # Apply row limit if specified\n",
    "    if num_rows is not None:\n",
    "        test_set = test_set[:num_rows]\n",
    "\n",
    "    # Store results\n",
    "    results = []\n",
    "\n",
    "    # Iterate through each question in the test set\n",
    "    for entry in test_set:\n",
    "        question = entry['question']\n",
    "        history = entry.get('conversation_history', [])\n",
    "        (question_tokens, history_tokens, retrieved_nodes_tokens, generated_queries_tokens,\n",
    "         output_tokens, classification_result) = get_token_usage_and_classification(question, history)\n",
    "\n",
    "        # Append results to the list\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'classification': classification_result,  \n",
    "            'question_tokens': question_tokens,\n",
    "            'history_tokens': history_tokens,\n",
    "            'retrieved_nodes_tokens': retrieved_nodes_tokens,\n",
    "            'generated_queries_tokens': generated_queries_tokens,\n",
    "            'output_tokens': output_tokens\n",
    "        })\n",
    "\n",
    "    # Convert results to a DataFrame and save as CSV\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(results_path, index=False)\n",
    "\n",
    "    print(f\"Results saved to {results_path}\")\n",
    "\n",
    "# Example of how to call the function:\n",
    "compute_token_counts(\n",
    "    test_set_path=\"../eval/eu_ai_act_test_300_new.jsonl\",\n",
    "    results_path=\"3class_PA_k_Q_eu_token_count.csv\",\n",
    "    llm=llm_gpt35,\n",
    "    vector_index=vector_index,\n",
    "    graph_index=graph_index,\n",
    "    kg_index=False,  # Set as required\n",
    "    property_index=False,\n",
    "    rewriter=True,  # Assuming we want to use query rewriting\n",
    "    reranker_model_name=None,\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=True,\n",
    "    num_rows=5  # Specify number of rows to process for testing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle, NodeWithScore, TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from transformers import pipeline\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.indices.property_graph import LLMSynonymRetriever\n",
    "from llama_index.core.indices.property_graph import VectorContextRetriever, PGRetriever\n",
    "import pandas as pd\n",
    "import json\n",
    "import tiktoken\n",
    "from llama_index.core.chat_engine import ContextChatEngine\n",
    "from llama_index.core.memory.chat_memory_buffer import ChatMemoryBuffer, ChatMessage\n",
    "\n",
    "\n",
    "class CustomRetrieverWithQueryRewriting(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs query rewriting, Vector search, BM25 search, and Knowledge Graph search.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,  # LLM for query generation\n",
    "        vector_retriever: Optional[VectorIndexRetriever] = None,\n",
    "        bm25_retriever: Optional[BaseRetriever] = None,\n",
    "        kg_index=None,  # Pass the graph index to create KGTableRetriever on the fly\n",
    "        mode: str = \"OR\",\n",
    "        rewriter: bool = True,\n",
    "        classifier_model: Optional[str] = None,  # Optional classifier model\n",
    "        device: str = 'mps',  # Set to 'mps' as the default device\n",
    "        reranker_model_name: Optional[str] = None,  # Model name for SentenceTransformerRerank\n",
    "        verbose: bool = False,  # Verbose flag\n",
    "        property_index=True,\n",
    "        use_fixed_params: bool = False  # New parameter to control whether to use fixed parameters\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._bm25_retriever = bm25_retriever\n",
    "        self._kg_index = kg_index  # Store the KG index instead of the retriever\n",
    "        self._llm = llm\n",
    "        self._rewriter = rewriter\n",
    "        self._mode = mode\n",
    "        self._reranker_model_name = reranker_model_name  # Store the model name for the reranker\n",
    "        self._reranker = None  # Initialize reranker as None initially\n",
    "        self.verbose = verbose  # Set verbose flag\n",
    "        self.property_index = property_index\n",
    "        self._classification_result = None  # To store the classification result\n",
    "        self.use_fixed_params = use_fixed_params  # Store whether to use fixed parameters\n",
    "        \n",
    "        # Initialize the classifier if provided\n",
    "        self.classifier = None\n",
    "        if classifier_model:\n",
    "            self.classifier = pipeline(\"text-classification\", model=classifier_model, device=device)\n",
    "\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "\n",
    "    def classify_query_and_get_params(self, query: str) -> (str, dict):\n",
    "        \"\"\"Classify the query and determine parameters for KG retriever.\"\"\"\n",
    "        # Define fixed parameters (highest available)\n",
    "        fixed_params = {\n",
    "            \"top_k\": 7,\n",
    "            \"max_keywords_per_query\": 5,\n",
    "            \"max_knowledge_sequence\": 3\n",
    "        }\n",
    "\n",
    "        if self.use_fixed_params:\n",
    "            # Use the fixed parameters directly if specified\n",
    "            if self.verbose:\n",
    "                print(f\"Using fixed parameters: {fixed_params}\")\n",
    "            return None, fixed_params\n",
    "        \n",
    "        # Default parameters for adaptive behavior\n",
    "        params = {\n",
    "            \"top_k\": 5,\n",
    "            \"max_keywords_per_query\": 4,\n",
    "            \"max_knowledge_sequence\": 2\n",
    "        }\n",
    "        classification_result = None\n",
    "        \n",
    "        if self.classifier:\n",
    "            classification = self.classifier(query)[0]\n",
    "            label = int(classification['label'].split('_')[-1])\n",
    "            classification_result = classification['label']  # Store the classification result\n",
    "            if self.verbose:\n",
    "                print(f\"Query Classification: {classification['label']} with score {classification['score']}\")\n",
    "            \n",
    "            if label == 0:\n",
    "                params[\"top_k\"] = 5\n",
    "                params[\"max_keywords_per_query\"] = 3\n",
    "                params[\"max_knowledge_sequence\"] = 1\n",
    "            elif label == 1:\n",
    "                params[\"top_k\"] = 7\n",
    "                params[\"max_keywords_per_query\"] = 4\n",
    "                params[\"max_knowledge_sequence\"] = 2\n",
    "            elif label == 2:\n",
    "                params[\"top_k\"] = 7\n",
    "                params[\"max_keywords_per_query\"] = 5\n",
    "                params[\"max_knowledge_sequence\"] = 3\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Selected parameters for the query: {params}\")\n",
    "\n",
    "        self._classification_result = classification_result\n",
    "        return classification_result, params  # Ensure both values are returned correctly\n",
    "\n",
    "    def classify_query(self, query_str: str) -> str:\n",
    "        \"\"\"Classify the query into one of the predefined categories using LLM.\"\"\"\n",
    "        classification_prompt = (\n",
    "            f\"Classify the following query into one of the following categories: '5-300. Definitions', \"\n",
    "            f\"'5-301 Bias Audit', '5-302 Data Requirements', '§ 5-303 Published Results', '§ 5-304 Notice to Candidates and Employees'. \"\n",
    "            f\"If it doesn't fit into any category, respond with 'None'. Return the classification, do not output absolutely anything else. Query: '{query_str}'\"\n",
    "        )\n",
    "        response = self._llm.complete(classification_prompt)\n",
    "        category = response.text.strip()\n",
    "        return category if category in [\n",
    "            '5-300. Definitions', '5-301 Bias Audit', \n",
    "            '5-302 Data Requirements', '§ 5-303 Published Results', \n",
    "            '§ 5-304 Notice to Candidates and Employees'\n",
    "        ] else None\n",
    "\n",
    "    def generate_queries(self, query_str: str, category: str, num_queries: int = 3) -> List[str]:\n",
    "        \"\"\"Generate query variations using the LLM, taking into account the category if applicable.\"\"\"\n",
    "        \n",
    "        query_gen_prompt_str = (\n",
    "            f\"You are an expert at distilling a user question into sub-questions that can be used to fully answer the original query. \"\n",
    "            f\"First, identify the key words from the original question below: \\n\"\n",
    "            f\"{query_str}\"\n",
    "            f\"Generate {num_queries} sub-queries that cover the different aspects needed to fully address the user's query.\\n\\n\"\n",
    "            f\"Here is an example: \\n\"\n",
    "            f\"Original Question: What does test data mean and what do I need to know about it?\"\n",
    "            f\"Output:\"\n",
    "            f\"definition of 'test data'\\n\"\n",
    "            f\"test data requirements and conditions for a bias audit\\n\"\n",
    "            f\"examples of the use of test data in a bias audit\\n\\n\"\n",
    "            f\"Output the rewritten sub-queries, one on each line, do not output absolutely anything else\"\n",
    "        )\n",
    "\n",
    "        query_gen_prompt = query_gen_prompt_str\n",
    "        response = self._llm.complete(query_gen_prompt)\n",
    "        queries = response.text.split(\"\\n\")\n",
    "\n",
    "        # Remove empty strings from the generated queries\n",
    "        queries = [query.strip() for query in queries if query.strip()]\n",
    "        \n",
    "        # Add the category-specific query if the category is available\n",
    "        if category:\n",
    "            category_query = f\"{category}\"\n",
    "            queries.append(category_query)\n",
    "\n",
    "        return queries\n",
    "\n",
    "    \n",
    "    async def run_queries(self, queries: List[str], retrievers: List[BaseRetriever]) -> dict:\n",
    "        \"\"\"Run queries against retrievers.\"\"\"\n",
    "        tasks = []\n",
    "        for query in queries:\n",
    "            for i, retriever in enumerate(retrievers):\n",
    "                tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "        task_results = await asyncio.gather(*tasks)\n",
    "\n",
    "        results_dict = {}\n",
    "        for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "            results_dict[(query, i)] = query_result\n",
    "        return results_dict\n",
    "\n",
    "    def fuse_vector_and_bm25_results(self, results_dict, similarity_top_k: int) -> List[NodeWithScore]:\n",
    "        \"\"\"Fuse results from Vector and BM25 retrievers.\"\"\"\n",
    "        k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "        fused_scores = {}\n",
    "        text_to_node = {}\n",
    "\n",
    "        # Compute reciprocal rank scores for BM25 and Vector retrievers\n",
    "        for nodes_with_scores in results_dict.values():\n",
    "            for rank, node_with_score in enumerate(\n",
    "                sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)\n",
    "            ):\n",
    "                text = node_with_score.node.get_content()\n",
    "                text_to_node[text] = node_with_score\n",
    "                if text not in fused_scores:\n",
    "                    fused_scores[text] = 0.0\n",
    "                fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "        # Sort results by combined scores\n",
    "        reranked_results = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Adjust node scores and prepare final results\n",
    "        reranked_nodes: List[NodeWithScore] = []\n",
    "        for text, score in reranked_results.items():\n",
    "            if text in text_to_node:\n",
    "                node = text_to_node[text]\n",
    "                node.score = score\n",
    "                reranked_nodes.append(node)\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(f\"Warning: Text not found in `text_to_node`: {text}\")\n",
    "\n",
    "        return reranked_nodes[:similarity_top_k]\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        # Classify the query to determine its category and retriever parameters\n",
    "        if self._rewriter:\n",
    "            category = self.classify_query(query_bundle.query_str)\n",
    "            if self.verbose:\n",
    "                print(f\"Classified Category: {category}\")\n",
    "\n",
    "        # Correctly unpack both classification_result and params\n",
    "        classification_result, params = self.classify_query_and_get_params(query_bundle.query_str)\n",
    "        self._classification_result = classification_result\n",
    "\n",
    "        top_k = params[\"top_k\"]\n",
    "\n",
    "        # Initialize the reranker with the correct top_k value\n",
    "        if self._reranker_model_name:\n",
    "            self._reranker = SentenceTransformerRerank(model=self._reranker_model_name, top_n=top_k)\n",
    "            if self.verbose:\n",
    "                print(f\"Initialized reranker with top_n: {top_k}\")\n",
    "\n",
    "        # Determine the number of query rewrites based on classification\n",
    "        num_queries = 3 if top_k == 5 else 5 if top_k == 7 else 7\n",
    "        if self.verbose:\n",
    "            print(f\"Number of Query Rewrites: {num_queries}\")\n",
    "\n",
    "        # Generate query variations if rewriter is True\n",
    "        if self._rewriter:\n",
    "            queries = self.generate_queries(query_bundle.query_str, category, num_queries=num_queries)\n",
    "            if self.verbose:\n",
    "                print(f\"Generated Queries: {queries}\")\n",
    "        else:\n",
    "            queries = [query_bundle.query_str]\n",
    "\n",
    "        # Prepare the list of active retrievers\n",
    "        active_retrievers = []\n",
    "        if self._vector_retriever:\n",
    "            active_retrievers.append(self._vector_retriever)\n",
    "        if self._bm25_retriever:\n",
    "            active_retrievers.append(self._bm25_retriever)\n",
    "\n",
    "        # Instantiate the KG retriever with the adaptive parameters\n",
    "        if self._kg_index and not self.property_index:\n",
    "            kg_retriever = KGTableRetriever(\n",
    "                index=self._kg_index,\n",
    "                retriever_mode='hybrid',\n",
    "                include_text=False,\n",
    "                max_keywords_per_query=params[\"max_keywords_per_query\"],\n",
    "                max_knowledge_sequence=params[\"max_knowledge_sequence\"]\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"Instantiated KG Retriever: max_keywords_per_query={params['max_keywords_per_query']}, \"\n",
    "                    f\"max_knowledge_sequence={params['max_knowledge_sequence']}\")\n",
    "            active_retrievers.append(kg_retriever)\n",
    "\n",
    "        elif self._kg_index and self.property_index:\n",
    "            synonym_retriever = LLMSynonymRetriever(\n",
    "                graph_index.property_graph_store,\n",
    "                llm=self._llm,\n",
    "                include_text=False,\n",
    "                max_keywords=params[\"max_keywords_per_query\"],\n",
    "                path_depth=params[\"max_knowledge_sequence\"],\n",
    "            )\n",
    "            \n",
    "            vector_retriever = VectorContextRetriever(\n",
    "                graph_index.property_graph_store,\n",
    "                embed_model=embed_model,\n",
    "                include_text=False,\n",
    "                similarity_top_k=params[\"top_k\"],\n",
    "                path_depth=params[\"max_knowledge_sequence\"],\n",
    "            )\n",
    "            \n",
    "            sub_retrievers = [synonym_retriever, vector_retriever]\n",
    "            kg_retriever = PGRetriever(sub_retrievers=sub_retrievers)\n",
    "\n",
    "        # If no active retrievers (BM25, Vector, or KG), raise an error\n",
    "        if not active_retrievers:\n",
    "            raise ValueError(\"No active retriever provided!\")\n",
    "\n",
    "        results = {}\n",
    "        # Run the queries asynchronously for active retrievers\n",
    "        if active_retrievers:\n",
    "            results = asyncio.run(self.run_queries(queries, active_retrievers))\n",
    "            if self.verbose:\n",
    "                print(f\"Fusion Results: {results}\")\n",
    "\n",
    "        # Fuse the results from active retrievers (BM25/Vector)\n",
    "        final_results = self.fuse_vector_and_bm25_results(results, similarity_top_k=top_k)\n",
    "\n",
    "        # Combine with KG nodes according to the mode (\"AND\" or \"OR\")\n",
    "        if self._kg_index:\n",
    "            kg_nodes = kg_retriever.retrieve(query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"KG Retrieved Nodes: {kg_nodes}\")\n",
    "\n",
    "            vector_ids = {n.node.id_ for n in final_results}\n",
    "            kg_ids = {n.node.id_ for n in kg_nodes}\n",
    "\n",
    "            combined_dict = {n.node.id_: n for n in final_results}\n",
    "            combined_dict.update({n.node.id_: n for n in kg_nodes})\n",
    "\n",
    "            if self._mode == \"AND\":\n",
    "                retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "            else:\n",
    "                retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "            final_results = [combined_dict[rid] for rid in retrieve_ids]\n",
    "\n",
    "        # Apply reranker if available\n",
    "        if self._reranker:\n",
    "            final_results = self._reranker.postprocess_nodes(final_results, query_bundle)\n",
    "            if self.verbose:\n",
    "                print(f\"Reranked Results: {final_results}\")\n",
    "        else:\n",
    "            final_results = final_results[:top_k]\n",
    "\n",
    "        # Remove duplicates if rewriter is used\n",
    "        if self._rewriter:\n",
    "            unique_nodes = {}\n",
    "            for node in final_results:\n",
    "                content = node.node.get_content()\n",
    "                if content not in unique_nodes:\n",
    "                    unique_nodes[content] = node\n",
    "            final_results = list(unique_nodes.values())\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Final Results: {final_results}\")\n",
    "\n",
    "        return final_results\n",
    "\n",
    "\n",
    "    def get_classification_result(self) -> str:\n",
    "        return getattr(self, \"_classification_result\", None)\n",
    "\n",
    "\n",
    "def compute_token_counts(\n",
    "    test_set_path: str,\n",
    "    results_path: str,\n",
    "    llm,\n",
    "    vector_index,\n",
    "    graph_index,\n",
    "    kg_index=True,\n",
    "    property_index=False,\n",
    "    rewriter: bool = False,\n",
    "    reranker_model_name: Optional[str] = None,\n",
    "    classifier_model: str = \"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False,\n",
    "    num_rows: Optional[int] = None,\n",
    "    use_fixed_params: bool = False  # New parameter to specify if fixed parameters should be used\n",
    "):\n",
    "    \"\"\"Compute and save token usage for each question in the test set.\"\"\"\n",
    "    \n",
    "    # Initialize necessary components\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10)\n",
    "    bm25_retriever = BM25Retriever.from_defaults(index=vector_index, similarity_top_k=10)\n",
    "\n",
    "    # Define the custom retriever\n",
    "    retriever = CustomRetrieverWithQueryRewriting(\n",
    "        llm=llm,\n",
    "        vector_retriever=vector_retriever,\n",
    "        kg_index=graph_index if kg_index else None,\n",
    "        bm25_retriever=bm25_retriever,\n",
    "        classifier_model=classifier_model,\n",
    "        mode=\"OR\",\n",
    "        rewriter=rewriter,\n",
    "        reranker_model_name=reranker_model_name,\n",
    "        verbose=verbose,\n",
    "        property_index=property_index,\n",
    "        use_fixed_params=use_fixed_params  # Pass the new parameter\n",
    "    )\n",
    "\n",
    "    # Create the chat engine\n",
    "    memory = ChatMemoryBuffer.from_defaults(token_limit=8192)\n",
    "    chat_engine = ContextChatEngine.from_defaults(\n",
    "        retriever=retriever,\n",
    "        verbose=False,\n",
    "        chat_mode=\"context\",\n",
    "        memory_cls=memory,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    # Function to get token count from a text\n",
    "    def count_tokens(text: str) -> int:\n",
    "        \"\"\"Returns the token count for a given text.\"\"\"\n",
    "        if isinstance(text, str):  # Check if text is a string\n",
    "            return len(tokenizer.encode(text))\n",
    "        else:\n",
    "            raise ValueError(\"Input to count_tokens must be a string.\")\n",
    "\n",
    "    # Function to get the answer, token counts, and classification\n",
    "    def get_token_usage_and_classification(question: str, history: List[dict] = None):\n",
    "        if not history:\n",
    "            history = []\n",
    "\n",
    "        # Initialize token counts\n",
    "        question_token_count = count_tokens(question)\n",
    "        history_token_count = sum(count_tokens(msg['content']) for msg in history)\n",
    "        retrieved_nodes_token_count = 0\n",
    "        generated_queries_token_count = 0\n",
    "        output_token_count = 0\n",
    "        \n",
    "        # Calculate input tokens for question and chat history\n",
    "        input_token_count = question_token_count + history_token_count\n",
    "        \n",
    "        # If query rewriter is enabled, generate queries and count tokens\n",
    "        if rewriter:\n",
    "            generated_queries = retriever.generate_queries(question, None)  # Assuming category is not needed here\n",
    "            generated_queries_token_count = sum(count_tokens(q) for q in generated_queries)\n",
    "            input_token_count += generated_queries_token_count\n",
    "\n",
    "        try:\n",
    "            # Get the answer and retrieve nodes\n",
    "            answer = chat_engine.chat(question, chat_history=history)\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error processing question '{question}': {e}\")\n",
    "            return None  # Skip this question if an error occurs\n",
    "\n",
    "        retrieved_nodes = retriever.retrieve(QueryBundle(query_str=question))\n",
    "\n",
    "        # Add token count for retrieved chunks and triplets (KG retrievals)\n",
    "        retrieved_nodes_token_count = sum(count_tokens(node.node.get_content()) for node in retrieved_nodes)\n",
    "        input_token_count += retrieved_nodes_token_count\n",
    "\n",
    "        # Calculate output tokens (e.g., generated keywords, rewritten queries, final answer)\n",
    "        if isinstance(answer, str):\n",
    "            output_token_count = count_tokens(answer)\n",
    "        else:\n",
    "            output_token_count = count_tokens(str(answer))\n",
    "\n",
    "        # Calculate total input and output tokens\n",
    "        total_input_tokens = question_token_count + history_token_count + retrieved_nodes_token_count\n",
    "        total_output_tokens = generated_queries_token_count + output_token_count\n",
    "\n",
    "        # Ensure to get the classification result after retrieval\n",
    "        classification_result = retriever.get_classification_result()\n",
    "        if verbose:\n",
    "            print(f\"Classification result for question '{question}': {classification_result}\")\n",
    "            print(f\"Token counts: Question = {question_token_count}, History = {history_token_count}, \"\n",
    "                  f\"Retrieved Nodes = {retrieved_nodes_token_count}, Generated Queries = {generated_queries_token_count}, \"\n",
    "                  f\"Output Tokens = {output_token_count}, Total Input Tokens = {total_input_tokens}, Total Output Tokens = {total_output_tokens}\")\n",
    "\n",
    "        return (question_token_count, history_token_count, retrieved_nodes_token_count, \n",
    "                generated_queries_token_count, output_token_count, classification_result,\n",
    "                total_input_tokens, total_output_tokens)\n",
    "\n",
    "    # Load the test set\n",
    "    with open(test_set_path, 'r') as f:\n",
    "        test_set = [json.loads(line) for line in f]\n",
    "\n",
    "    # Apply row limit if specified\n",
    "    if num_rows is not None:\n",
    "        test_set = test_set[:num_rows]\n",
    "\n",
    "    # Store results\n",
    "    results = []\n",
    "\n",
    "    # Iterate through each question in the test set\n",
    "    for entry in test_set:\n",
    "        question = entry['question']\n",
    "        history = entry.get('conversation_history', [])\n",
    "        \n",
    "        result = get_token_usage_and_classification(question, history)\n",
    "        if result is None:\n",
    "            continue  # Skip this entry if an error occurred\n",
    "        \n",
    "        (question_tokens, history_tokens, retrieved_nodes_tokens, generated_queries_tokens,\n",
    "         output_tokens, classification_result, total_input_tokens, total_output_tokens) = result\n",
    "\n",
    "        # Append results to the list\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'classification': classification_result,  \n",
    "            'question_tokens': question_tokens,\n",
    "            'history_tokens': history_tokens,\n",
    "            'retrieved_nodes_tokens': retrieved_nodes_tokens,\n",
    "            'generated_queries_tokens': generated_queries_tokens,\n",
    "            'output_tokens': output_tokens,\n",
    "            'total_input_tokens': total_input_tokens,\n",
    "            'total_output_tokens': total_output_tokens\n",
    "        })\n",
    "\n",
    "    # Convert results to a DataFrame and save as CSV\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(results_path, index=False)\n",
    "\n",
    "    print(f\"Results saved to {results_path}\")\n",
    "\n",
    "compute_token_counts(\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",\n",
    "    results_path=\"3class_fixed_params_upper_ll144_token_count.csv\",\n",
    "    llm=llm_gpt35,\n",
    "    vector_index=vector_index,\n",
    "    graph_index=graph_index,\n",
    "    kg_index=False,  # Set as required\n",
    "    property_index=False,\n",
    "    rewriter=True,  # Assuming we want to use query rewriting\n",
    "    reranker_model_name=None,\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False,\n",
    "    use_fixed_params=True  # Use fixed parameters instead of adaptive ones\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to call the function:\n",
    "compute_token_counts(\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",    #\"../eval/eu_ai_act_test_300_new.jsonl\",\n",
    "    results_path=\"3class_PA_k_K_S_ll144_token_count.csv\",\n",
    "    llm=llm_gpt35,\n",
    "    vector_index=vector_index,\n",
    "    graph_index=graph_index,\n",
    "    kg_index=True,  # Set as required\n",
    "    property_index=False,\n",
    "    rewriter=False,  # Assuming we want to use query rewriting\n",
    "    reranker_model_name=None,\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False#,\n",
    "    #num_rows=5  # Specify number of rows to process for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to call the function:\n",
    "compute_token_counts(\n",
    "    test_set_path=\"../../giskard_test_sets/LL144_275_New.jsonl\",    #\"../eval/eu_ai_act_test_300_new.jsonl\",\n",
    "    results_path=\"3class_hyPA_ll144_token_count.csv\",\n",
    "    llm=llm_gpt35,\n",
    "    vector_index=vector_index,\n",
    "    graph_index=graph_index,\n",
    "    kg_index=True,  # Set as required\n",
    "    property_index=False,\n",
    "    rewriter=True,  # Assuming we want to use query rewriting\n",
    "    reranker_model_name=None,\n",
    "    classifier_model=\"rk68/distilbert-q-classifier-3\",\n",
    "    verbose=False#,\n",
    "    #num_rows=5  # Specify number of rows to process for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
